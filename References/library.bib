Automatically generated by Mendeley Desktop 1.17.9
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Burger2014,
abstract = {Background: This article describes capture of biological information using a hybrid approach that combines natural language processing to extract biological entities and crowdsourcing with annotators recruited via Amazon Mechanical Turk to judge correctness of candidate biological relations. These techniques were applied to extract gene– mutation relations from biomedical abstracts with the goal of supporting production scale capture of gene–mutation–disease findings as an open source resource for personalized medicine. Results: The hybrid system could be configured to provide good performance for gene–mutation extraction (precision 82{\%}; recall 70{\%} against an expert-generated gold standard) at a cost of {\$}0.76 per abstract. This demonstrates that crowd labor platforms such as Amazon Mechanical Turk can be used to recruit quality annotators, even in an application requiring subject matter expertise; aggregated Turker judgments for gene–mutation relations exceeded 90{\%} accuracy. Over half of the precision errors were due to mismatches against the gold standard hidden from annotator view (e.g. incorrect EntrezGene identifier or incorrect mutation position extracted), or incomplete task instructions (e.g. the need to exclude nonhuman mutations). Conclusions: The hybrid curation model provides a readily scalable cost-effective approach to curation, particularly if coupled with expert human review to filter precision errors. We plan to generalize the framework and make it available as open source software.},
author = {Burger, John D and Doughty, Emily and Khare, Ritu and Wei, Chih Hsuan and Mishra, Rajashree and Aberdeen, John and Tresner-Kirsch, David and Wellner, Ben and Kann, Maricel G and Lu, Zhiyong and Hirschman, Lynette},
doi = {10.1093/database/bau094},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Burger et al. - 2014 - Hybrid curation of gene-mutation relations combining automated extraction and crowdsourcing.pdf:pdf},
isbn = {1758-0463 (Electronic)$\backslash$r1758-0463 (Linking)},
issn = {17580463},
journal = {Database (Oxford)},
pmid = {25246425},
publisher = {Oxford University Press},
title = {{Hybrid curation of gene-mutation relations combining automated extraction and crowdsourcing}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25246425 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4170591},
volume = {2014},
year = {2014}
}
@article{Cohen2005a,
abstract = {The volume of published biomedical research, and therefore the underlying biomedical knowledge base, is expanding at an increasing rate. Among the tools that can aid researchers in coping with this information overload are text mining and knowledge extraction. Significant progress has been made in applying text mining to named entity recognition, text classification, terminology extraction, relationship extraction and hypothesis generation. Several research groups are constructing integrated flexible text-mining systems intended for multiple uses. The major challenge of biomedical text mining over the next 5-10 years is to make these systems useful to biomedical researchers. This will require enhanced access to full text, better understanding of the feature space of biomedical literature, better methods for measuring the usefulness of systems to users, and continued cooperation with the biomedical research community to ensure that their needs are addressed.},
author = {Cohen, Aaron M and Hersh, William R},
doi = {10.1093/bib/6.1.57},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cohen, Hersh - 2005 - A survey of current work in biomedical text mining(2).pdf:pdf},
isbn = {1503494004},
issn = {14675463},
journal = {Briefings in Bioinformatics},
keywords = {Bioinformatics,Natural language processing,Text-mining},
month = {mar},
number = {1},
pages = {57--71},
pmid = {15826357},
title = {{A survey of current work in biomedical text mining}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15826357},
volume = {6},
year = {2005}
}
@article{Friedman2013,
abstract = {Natural language processing (NLP) is crucial for advancing healthcare because it is needed to transform relevant information locked in text into structured data that can be used by computer processes aimed at improving patient care and advancing medicine. In light of the importance of NLP to health, the National Library of Medicine (NLM) recently sponsored a workshop to review the state of the art in NLP focusing on text in English, both in biomedicine and in the general language domain. Specific goals of the NLM-sponsored workshop were to identify the current state of the art, grand challenges and specific roadblocks, and to identify effective use and best practices. This paper reports on the main outcomes of the workshop, including an overview of the state of the art, strategies for advancing the field, and obstacles that need to be addressed, resulting in recommendations for a research agenda intended to advance the field. {\textcopyright} 2013 The Authors.},
author = {Friedman, Carol and Rindflesch, Thomas C. and Corn, Milton},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Friedman, Rindflesch, Corn - 2013 - Natural language processing State of the art and prospects for significant progress, a workshop spon.pdf:pdf},
journal = {Journal of Biomedical Informatics},
keywords = {Biomedical language processing,Natural language processing},
month = {oct},
number = {5},
pages = {765--773},
title = {{Natural language processing: State of the art and prospects for significant progress, a workshop sponsored by the National Library of Medicine}},
volume = {46},
year = {2013}
}
@article{Opulencia2011,
abstract = {Ideally, an image should be reported and interpreted in the same way (e.g., the same perceived likelihood of malignancy) or similarly by any two radiologists; however, as much research has demonstrated, this is not often the case. Various efforts have made an attempt at tackling the problem of reducing the variability in radiologists' interpretations of images. The Lung Image Database Consortium (LIDC) has provided a database of lung nodule images and associated radiologist ratings in an effort to provide images to aid in the analysis of computer-aided tools. Likewise, the Radiological Society of North America has developed a radiological lexicon called RadLex. As such, the goal of this paper is to investigate the feasibility of associating LIDC characteristics and terminology with RadLex terminology. If matches between LIDC characteristics and RadLex terms are found, probabilistic models based on image features may be used as decision-based rules to predict if an image or lung nodule could be characterized or classified as an associated RadLex term. The results of this study were matches for 25 (74{\%}) out of 34 LIDC terms in RadLex. This suggests that LIDC characteristics and associated rating terminology may be better conceptualized or reduced to produce even more matches with RadLex. Ultimately, the goal is to identify and establish a more standardized rating system and terminology to reduce the subjective variability between radiologist annotations. A standardized rating system can then be utilized by future researchers to develop automatic annotation models and tools for computer-aided decision systems.},
author = {Opulencia, Pia and Channin, David S and Raicu, Daniela S and Furst, Jacob D},
doi = {10.1007/s10278-010-9285-6},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Opulencia et al. - 2011 - Mapping LIDC, radLex, and lung nodule image features.pdf:pdf},
isbn = {0897-1889},
issn = {08971889},
journal = {Journal of Digital Imaging},
keywords = {Chest CT,Computer-assisted,Digital imaging,Image data,Image interpretation,Imaging informatics,LIDC,Lung,RadLex,Radiographic image interpretation,Reporting,Semantic},
month = {apr},
number = {2},
pages = {256--270},
pmid = {20390436},
publisher = {Springer},
title = {{Mapping LIDC, radLex??, and lung nodule image features}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20390436 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3056962},
volume = {24},
year = {2011}
}
@inproceedings{Demner-Fushman2015,
abstract = {Annotation of MEDLINE citations with controlled vocabu-lary terms improves the quality of retrieval results. Due to variety in descriptions of similar clinical phenomena and abundance of negation and uncertainty, annotation of clinical radiology reports for subsequent indexing and retrieval with a search engine is even more important. Pro-vided with an opportunity to add about 4,000 radiology reports to col-lections indexed with NLM image retrieval engine Open-i, we needed to assure good retrieval quality. To accomplish this, we explored automatic and manual approaches to annotation, as well as developed a small con-trolled vocabulary of chest x-ray indexing terms and guidelines for man-ual annotation. Manual annotation captured the most salient findings in the reports and normalized the sparse distinct descriptions of similar findings to one controlled vocabulary term. This paper presents the vo-cabulary and the manual annotation process, as well as an evaluation of the automatic annotation of the reports.},
author = {Demner-Fushman, Dina and Shooshan, Sonya E and Rodriguez, Laritza and Antani, Sameer and Thoma, George R},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-24471-6_9},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Demner-Fushman et al. - 2015 - Annotation of chest radiology reports for indexing and retrieval.pdf:pdf},
isbn = {9783319244709},
issn = {16113349},
keywords = {Information storage and retrieval,Radiolog,Vocabulary,controlled},
pages = {99--111},
title = {{Annotation of chest radiology reports for indexing and retrieval}},
volume = {9059},
year = {2015}
}
@book{Robinson2011,
abstract = {Introduction to Bio-Ontologies explores the computational background of ontologies. Emphasizing computational and algorithmic issues surrounding bio-ontologies, this self-contained text helps readers understand ontological algorithms and their applications. The first part of the book defines ontology and bio-ontologies. It also explains the importance of mathematical logic for understanding concepts of inference in bio-ontologies, discusses the probability and statistics topics necessary for understanding ontology algorithms, and describes ontology languages, including OBO (the preeminent langu.},
author = {Robinson, Peter N and Bauer, Sebastian.},
isbn = {978-1-4398-3666-8},
pages = {488},
publisher = {CRC Press},
title = {{Introduction to bio-ontologies}},
url = {http://www.crcnetbase.com/isbn/9781439836668},
year = {2011}
}
@article{Hotho2005,
abstract = {The enormous amount of information stored in unstructured texts cannot sim- ply be used for further processing by computers, which typically handle text as simple sequences of character strings. Therefore, specific (pre-)processing meth- ods and algorithms are required in order to extract useful patterns. Text mining refers generally to the process of extracting interesting information and knowledge from unstructured text. In this article, we discuss text mining as a young and in- terdisciplinary field in the intersection of the related areas information retrieval, machine learning, statistics, computational linguistics and especially data mining. We describe the main analysis tasks preprocessing, classification, clustering, in- formation extraction and visualization. In addition, we briefly discuss a number of successful applications of text mining.},
author = {Hotho, Andreas and N{\"{u}}rnberger, Andreas and Paa{\ss}, Gerhard},
doi = {10.1111/j.1365-2621.1978.tb09773.x},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hotho, N{\"{u}}rnberger, Paa{\ss} - 2005 - A Brief Survey of Text Mining.pdf:pdf},
isbn = {0175-1336},
issn = {01751336},
journal = {LDV Forum - GLDV Journal for Computational Linguistics and Language Technology},
pages = {19--62},
title = {{A Brief Survey of Text Mining}},
url = {http://www.kde.cs.uni-kassel.de/hotho/pub/2005/hotho05TextMining.pdf},
volume = {20},
year = {2005}
}
@incollection{Bretschneider,
abstract = {Ontologies have proven to be useful to enhance NLP-based applications such as information ex-traction. In the biomedical domain rich ontologies are available and used for semantic annotation of texts. However, most of them have either no or only few non-English concept labels and can-not be used to annotate non-English texts. Since translations need expert review, a full translation of large ontologies is often not feasible. For semantic annotation purpose, we propose to use the corpus to be annotated to identify high occurrence terms and their translations to extend respec-tive ontology concepts. Using our approach, the translation of a subset of ontology concepts is sufficient to significantly enhance annotation coverage. For evaluation, we automatically trans-lated RadLex ontology concepts from English into German. We show that by translating a rather small set of concepts (in our case 433), which were identified by corpus analysis, we are able to enhance the amount of annotated words from 27.36 {\%} to 42.65 {\%}.},
author = {Bretschneider, Claudia and Oberkampf, Heiner and Zillner, Sonja and Bauer, Bernhard and Hammon, Matthias},
booktitle = {Proceedings of the Third Workshop on Semantic Web and Information Extraction},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bretschneider et al. - 2014 - Corpus-based Translation of Ontologies for Improved Multilingual Semantic Annotation.pdf:pdf},
pages = {1--8},
title = {{Corpus-based Translation of Ontologies for Improved Multilingual Semantic Annotation}},
year = {2014}
}
@article{Moore1989,
abstract = {A medical expert system is a computer program designed to imitate the behavior of a medical expert. Translation of medical documents may be regarded as an area of medical expertise. Computer translators may also serve as an interface from the user's natural language into the formal language of some medical expert systems. TRANSOFT is a public-domain, table-driven medical document translator, written in MUMPS or GNOSIS (= MUMPS + PROLOG). Translations are obtained by implicit word parity across languages, contextual disambiguation in the source language, and word rearrangement from stereotypic source into stereotypic target language word order. TRANSOFT has properties of a Chomsky language generator and can be modelled as a dialogue between two scholars across an electronic communication medium. TRANSOFT is a mathematical group. Computer translators have potential applications in the international exchange of medical records, medical publications, and electronic mail across language barriers. {\textcopyright} 1989.},
author = {Moore, G. William and Wakai, Ichiro and Satomura, Yoichi and Giere, Wolfgang},
doi = {10.1016/0933-3657(89)90029-8},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Moore et al. - 1989 - TRANSOFT Medical translation expert system.pdf:pdf},
issn = {09333657},
journal = {Artificial Intelligence In Medicine},
keywords = {machine translation,medical expert systems,medical language,parsing},
month = {jan},
number = {4},
pages = {149--157},
publisher = {Elsevier},
title = {{TRANSOFT: Medical translation expert system}},
url = {http://linkinghub.elsevier.com/retrieve/pii/0933365789900298},
volume = {1},
year = {1989}
}
@article{Hersh2001,
abstract = {The recent improvements in capabilities of desktop computers and communications networks give impetus for the development of clinical image repositories that can be used for patient care and medical education. A challenge in the use of these systems is the accurate indexing of images for retrieval performance acceptable to users. This paper describes a series of experiments aiming to adapt the SAPHIRE system, which matches text to concepts in the UMLS Metathesaurus, for the automated indexing of image reports. A series of enhancements to the baseline system resulted in a recall of 63{\%} but a precision of only 30{\%} in detecting concepts. At this level of performance, such a system might be problematic for users in a purely automated indexing environment. However, if the ability to retrieve images in repositories based on content in their reports is desired by clinical users, and no other current systems offer this functionality, then follow-up research questions include whether these imperfect results would be useful in a completely or partially automated indexing environment and/or whether other approaches can improve upon them.},
author = {Hersh, William and Mailhot, Mark and Arnott-Smith, Catherine and Lowe, Henry},
doi = {10.1006/jbin.2001.1025},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hersh et al. - 2001 - Selective automated indexing of findings and diagnoses in radiology reports.pdf:pdf},
isbn = {1532-0464 (Print)$\backslash$r1532-0464 (Linking)},
issn = {1532-0464},
journal = {Journal of biomedical informatics},
keywords = {Abstracting and Indexing as Topic,Algorithms,Computational Biology,Computer Systems,Humans,Natural Language Processing,Radiology Information Systems,Unified Medical Language System},
number = {4},
pages = {262--73},
pmid = {11977808},
publisher = {Academic Press},
title = {{Selective automated indexing of findings and diagnoses in radiology reports.}},
url = {http://europepmc.org/abstract/MED/11977808},
volume = {34},
year = {2001}
}
@article{Meystre2008,
author = {Meystre, S M and Savova, G K and Hurdle, J F},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Meystre, Savova, Hurdle - 2008 - Extracting Information from Textual Documents in the Electronic Health Record A Review of Recent Resea.pdf:pdf},
journal = {IMIA Yearbook of Medical Informatics},
keywords = {linked to concepts,machine learning,rules or based on,statistical methods and,the information ex-,tracted can then be},
pages = {128--144},
title = {{Extracting Information from Textual Documents in the Electronic Health Record : A Review of Recent Research}},
volume = {35},
year = {2008}
}
@article{Koponen2016a,
author = {Koponen, Maarit},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Koponen - 2016 - Is machine translation post-editing worth the effort A survey of research into post-editing and effort.pdf:pdf},
journal = {Journal of Specialised Translation},
keywords = {machine translation,mt post-editing,mt quality,post-editing effort},
number = {25},
pages = {131--148},
title = {{Is machine translation post-editing worth the effort? A survey of research into post-editing and effort}},
year = {2016}
}
@article{Dusek2014,
abstract = {This paper presents the participation of the Charles University team in the WMT 2014 Medical Translation Task. Our sys-tems are developed within the Khresmoi project, a large integrated project aim-ing to deliver a multi-lingual multi-modal search and access system for biomedical information and documents. Being in-volved in the organization of the Medi-cal Translation Task, our primary goal is to set up a baseline for both its subtasks (summary translation and query transla-tion) and for all translation directions. Our systems are based on the phrase-based Moses system and standard meth-ods for domain adaptation. The con-strained/unconstrained systems differ in the training data only.},
author = {Du{\v{s}}ek, Ond$\backslash$vrej and Haji{\v{c}}, Jan and Hlav{\'{a}}{\v{c}}ov{\'{a}}, Jaroslava and Nov{\'{a}}k, Michal and Pecina, Pavel and Rosa, Rudolf and Tamchyna, Ale{\v{s}} and Ure{\v{s}}ov{\'{a}}, Zde$\backslash$vnka and Zeman, Daniel and Hajiˇ, Jan},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Du{\v{s}}ek et al. - 2014 - Machine Translation of Medical Texts in the Khresmoi Project.pdf:pdf},
journal = {Proceedings of the Ninth Workshop on Statistical Machine Translation},
pages = {221--228},
title = {{Machine Translation of Medical Texts in the Khresmoi Project}},
url = {http://www.aclweb.org/anthology/W/W14/W14-3326},
year = {2014}
}
@article{Dreyer2005,
abstract = {PURPOSE: To validate the accuracy of Lexicon Mediated Entropy Reduction (LEXIMER), a new information theory–based computer algorithm developed by the authors for independent analysis and classification of unstructured radiology reports based on the presence of clinically important findings (FT, where T represents “true”) and recommendations for subsequent action (RT). MATERIALS AND METHODS: The study was approved by the Human Research Committee of the institutional review board. Consecutive de-identified radiology reports (n = 1059) comprising results of barium studies (n = 99), computed tomography (n = 107), mammography (n = 90), magnetic resonance imaging (n = 108), nuclear medicine (n = 99), positron emission tomography (n = 106), radiography (n = 212), ultrasonography (n = 131), and vascular procedures (n = 107) were independently analyzed by two radiologists and then with LEXIMER to categorize the reports into FT and FT0 (containing or not containing clinically important findings) categories and RT a...},
author = {Dreyer, Keith J. and Kalra, Mannudeep K. and Maher, Michael M. and Hurier, Autumn M. and Asfaw, Benjamin A. and Schultz, Thomas and Halpern, Elkan F. and Thrall, James H.},
doi = {10.1148/radiol.2341040049},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dreyer et al. - 2005 - Application of Recently Developed Computer Algorithm for Automatic Classification of Unstructured Radiology Repor.pdf:pdf},
issn = {0033-8419},
journal = {Radiology},
month = {feb},
number = {2},
pages = {323--329},
publisher = { Radiological Society of North America },
title = {{Application of Recently Developed Computer Algorithm for Automatic Classification of Unstructured Radiology Reports: Validation Study}},
url = {http://pubs.rsna.org/doi/10.1148/radiol.2341040049},
volume = {234},
year = {2005}
}
@article{Martinez2015,
abstract = {Background: Invasive fungal diseases (IFDs) are associated with considerable health and economic costs. Surveillance of the more diagnostically challenging invasive fungal diseases, specifically of the sino-pulmonary system, is not feasible for many hospitals because case finding is a costly and labour intensive exercise. We developed text classifiers for detecting such IFDs from free-text radiology (CT) reports, using machine-learning techniques. Method: We obtained free-text reports of CT scans performed over a specific hospitalisation period (2003-2011), for 264 IFD and 289 control patients from three tertiary hospitals. We analysed IFD evidence at patient, report, and sentence levels. Three infectious disease experts annotated the reports of 73 IFD-positive patients for language suggestive of IFD at sentence level, and graded the sentences as to whether they suggested or excluded the presence of IFD. Reliable agreement between annotators was obtained and this was used as training data for our classifiers. We tested a variety of Machine Learning (ML), rule based, and hybrid systems, with feature types including bags of words, bags of phrases, and bags of concepts, as well as report-level structured features. Evaluation was carried out over a robust framework with separate Development and Held-Out datasets. Results: The best systems (using Support Vector Machines) achieved very high recall at report- and patient-levels over unseen data: 95{\%} and 100{\%} respectively. Precision at report-level over held-out data was 71{\%}; however, most of the associated false-positive reports (53{\%}) belonged to patients who had a previous positive report appropriately flagged by the classifier, reducing negative impact in practice. Conclusions: Our machine learning application holds the potential for developing systematic IFD surveillance systems for hospital populations.},
author = {Martinez, David and Ananda-Rajah, Michelle R. and Suominen, Hanna and Slavin, Monica A. and Thursky, Karin A. and Cavedon, Lawrence},
doi = {10.1016/j.jbi.2014.11.009},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Martinez et al. - 2015 - Automatic detection of patients with invasive fungal disease from free-text computed tomography (CT) scans.pdf:pdf},
issn = {15320464},
journal = {Journal of Biomedical Informatics},
keywords = {Aspergillosis,Data mining,Invasive fungal disease,Natural language processing,Surveillance},
pages = {251--260},
pmid = {25460203},
title = {{Automatic detection of patients with invasive fungal disease from free-text computed tomography (CT) scans}},
volume = {53},
year = {2015}
}
@article{Laubli2013,
abstract = {In many experimental studies on assessing post-editing efficiency, idiosyncratic user interfaces isolate translators from transla-tion aids that are available to them in their daily work. In contrast, our experimen-tal design allows translators to use a well-known translator workbench for both con-ventional translation and post-editing. We find that post-editing reduces translation time significantly, although considerably less than reported in isolated experiments, and argue that overall assessments of post-editing efficiency should be based on a re-alistic translation environment.},
annote = {Post-editing speeds up translation by 15-20{\%}},
author = {L{\"{a}}ubli, Samuel and Fishel, Mark and Massey, Gary and Ehrensberger-dow, Maureen and Volk, Martin},
doi = {10.5167/uzh-80891},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/L{\"{a}}ubli et al. - 2013 - Assessing post-editing efficiency in a realistic translation environment Assessing Post-Editing Efficiency in a.pdf:pdf},
journal = {MT Summit XIV Workshop on Post-editing Technology and Practice},
number = {September},
pages = {83--91},
title = {{Assessing post-editing efficiency in a realistic translation environment Assessing Post-Editing Efficiency in a Realistic Translation Environment}},
year = {2013}
}
@article{Hong2012,
abstract = {Radiologists are critically interested in promoting best practices in medical imaging, and to that end, they are actively developing tools that will optimize terminology and reporting practices in radiology. The RadLex{\textregistered} vocabulary, developed by the Radiological Society of North America (RSNA), is intended to create a unifying source for the terminology that is used to describe medical imaging. The RSNA Reporting Initiative has developed a library of reporting templates to integrate reusable knowledge, or meaning, into the clinical reporting process. This report presents the initial analysis of the intersection of these two major efforts. From 70 published radiology reporting templates, we extracted the names of 6,489 reporting elements. These terms were reviewed in conjunction with the RadLex vocabulary and classified as an exact match, a partial match, or unmatched. Of 2,509 unique terms, 1,017 terms (41{\%}) matched exactly to RadLex terms, 660 (26{\%}) were partial matches, and 832 reporting terms (33{\%}) were unmatched to RadLex. There is significant overlap between the terms used in the structured reporting templates and RadLex. The unmatched terms were analyzed using the multidimensional scaling (MDS) visualization technique to reveal semantic relationships among them. The co-occurrence analysis with the MDS visualization technique provided a semantic overview of the investigated reporting terms and gave a metric to determine the strength of association among these terms.},
author = {Hong, Yi and Zhang, Jin and Heilbrun, Marta E and Kahn, Charles E},
doi = {10.1007/s10278-011-9423-9},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hong et al. - 2012 - Analysis of RadLex coverage and term co-occurrence in radiology reporting templates.pdf:pdf},
isbn = {1618-727X (Electronic)$\backslash$n0897-1889 (Linking)},
issn = {08971889},
journal = {Journal of Digital Imaging},
keywords = {Mapping,Multidimensional scaling,RadLex,Radiology,Reporting templates,Standardized terminology,Structured reporting,Visualization},
month = {feb},
number = {1},
pages = {56--62},
pmid = {22011936},
publisher = {Springer},
title = {{Analysis of RadLex coverage and term co-occurrence in radiology reporting templates}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22011936 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3264705},
volume = {25},
year = {2012}
}
@article{Womack2010,
abstract = {Chart review is central to health services research. Text processing, which analyzes free-text fields through automated methods, can facilitate this process. We compared precision and accuracy of NegEx and SQLServer 2008 Free-Text Search in identifying acute fractures in radiology reports.The term "fracture" was included in 23,595 radiology reports from the Veterans Aging Cohort Study. Four hundred reports were randomly selected and manually reviewed for acute fractures to establish a gold standard. Reports were then processed by SQLServer and NegEx. Results were compared to the gold standard to determine accuracy, precision, recall, and F-statistic.NegEx and the gold standard identified acute fractures in 13 reports. SQLServer identified 2 in a report-based analysis (precision: 1.00; accuracy: 0.97; recall: 0.15; F-statistic: 0.26), and 12 in a sentence-by-sentence analysis (precision: 1.00; recall: 0.92; accuracy: 0.92; F-statistic: 0.96).Text-processing tools utilizing basic database or programming skills are comparable, precise, and accurate in identifying reports for review.},
author = {Womack, Julie A and Scotch, Matthew and Gibert, Cynthia and Chapman, Wendy and Yin, Michael and Justice, Amy C and Brandt, Cynthia},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Womack et al. - 2010 - A comparison of two approaches to text processing facilitating chart reviews of radiology reports in electronic m.pdf:pdf},
isbn = {1559-4122; 1559-4122},
issn = {1559-4122},
journal = {Perspectives in health information management / AHIMA, American Health Information Management Association},
keywords = {Algorithms,Bone,Bone: radiography,Cohort Studies,Electronic Health Records,False Negative Reactions,Fractures,Health Services Research,Hospital,Humans,Information Storage and Retrieval,Information Storage and Retrieval: standards,Medical Audit,Medical Audit: methods,Natural Language Processing,Radiology Department,Sensitivity and Specificity,United States},
pages = {1a},
pmid = {21063542},
title = {{A comparison of two approaches to text processing: facilitating chart reviews of radiology reports in electronic medical records.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2966352{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {7},
year = {2010}
}
@article{Kirchhoff2011,
abstract = {Accurate, understandable public health information is important for ensuring the health of the nation. The large portion of the US population with Limited English Proficiency is best served by translations of public-health information into other languages. However, a large number of health departments and primary care clinics face significant barriers to fulfilling federal mandates to provide multilingual materials to Limited English Proficiency individuals. This article presents a pilot study on the feasibility of using freely available statistical machine translation technology to translate health promotion materials.},
author = {Kirchhoff, Katrin and Turner, Anne M and Axelrod, Amittai and Saavedra, Francisco},
doi = {10.1136/amiajnl-2011-000176},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kirchhoff et al. - 2011 - Application of statistical machine translation to public health information a feasibility study.pdf:pdf},
issn = {1527-974X},
journal = {Journal of the American Medical Informatics Association : JAMIA},
number = {4},
pages = {473--478},
pmid = {21498805},
title = {{Application of statistical machine translation to public health information: a feasibility study.}},
volume = {18},
year = {2011}
}
@article{Taira1999,
abstract = {Statistical natural language processors have been the focus of much research during the past decade. The main advantage of such an approach over grammatical rule-based approaches is its scalability to new domains. We present a statistical NLP for the domain of radiology and report on methods of knowledge acquisition, parsing, semantic interpretation, and evaluation. Preliminary performance data are given. A discussion of the perceived benefit, limitations and future work is presented.},
author = {Taira, R K and Soderland, S G},
doi = {D005373 [pii]},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Taira, Soderland - 1999 - A Statistical Natural Language Processor for Medical Reports.pdf:pdf},
isbn = {1531-605X (Print)$\backslash$r1531-605X (Linking)},
issn = {1531-605X},
journal = {Proceedings / AMIA ... Annual Symposium. AMIA Symposium},
keywords = {Algorithms,Evaluation Studies as Topic,Humans,Linguistics,Medical Records,Natural Language Processing,Radiography,Semantics,Thoracic},
pages = {970--974},
pmid = {10566505},
publisher = {American Medical Informatics Association},
title = {{A Statistical Natural Language Processor for Medical Reports.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10566505 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC2232848 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2232848{\&}tool=pmcentrez{\&}rendertype=abstract},
year = {1999}
}
@inproceedings{Bouillon2005,
abstract = {We present an overview of MedSLT, an Open Source platform for developing limited-domain medical speech translation systems. We focus in particular on the speech understanding architecture, which uses grammar-based language models derived using corpus-based specialisation methods from a single linguistically motivated grammar, and summarise the results of two evaluations which investigate the appropriateness of these design choices. Other sections describe the interlingua and its relationship with the recognition architecture, and the current demo system.},
annote = {Grammar-based better than statistical translation},
author = {Bouillon, Pierrette and Rayner, Manny and Chatzichrisafis, Nikos and Hockey, Beth Ann and Santaholma, Marianne Elina and Starlander, Marianne and Nakao, Yukie and Kanzaki, Kyoko and Isahara, Hitoshi},
booktitle = {Proceedings of the 10th Conference of the European Association for Machine Translation (EAMT)},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bouillon et al. - 2005 - A Generic Multi-Lingual Open Source Platform for Limited-Domain Medical Speech Translation.pdf:pdf},
pages = {50--58},
title = {{A Generic Multi-Lingual Open Source Platform for Limited-Domain Medical Speech Translation}},
url = {https://archive-ouverte.unige.ch/unige:2293},
year = {2005}
}
@article{Kohler2016,
abstract = {Deep phenotyping has been defined as the precise and comprehensive analysis of phenotypic abnormalities in which the individual components of the phenotype are observed and described. The three components of the Human Phenotype Ontology (HPO; www.human-phenotype-ontology.org) project are the phenotype vocabulary, disease-phenotype annotations and the algorithms that operate on these. These components are being used for computational deep phenotyping and precision medicine as well as integration of clinical data into translational research. The HPO is being increasingly adopted as a standard for phenotypic abnormalities by diverse groups such as international rare disease organizations, registries, clinical labs, biomedical resources, and clinical software tools and will thereby contribute toward nascent efforts at global data exchange for identifying disease etiologies. This update article reviews the progress of the HPO project since the debut Nucleic Acids Research database article in 2014, including specific areas of expansion such as common (complex) disease, new algorithms for phenotype driven genomic discovery and diagnostics, integration of cross-species mapping efforts with the Mammalian Phenotype Ontology, an improved quality control pipeline, and the addition of patient-friendly terminology.},
author = {K{\"{o}}hler, Sebastian and Vasilevsky, Nicole A. and Engelstad, Mark and Foster, Erin and McMurry, Julie and Aym{\'{e}}, S{\'{e}}gol{\`{e}}ne and Baynam, Gareth and Bello, Susan M. and Boerkoel, Cornelius F. and Boycott, Kym M. and Brudno, Michael and Buske, Orion J. and Chinnery, Patrick F. and Cipriani, Valentina and Connell, Laureen E. and Dawkins, Hugh J S and DeMare, Laura E. and Devereau, Andrew D. and de Vries, Bert B A and Firth, Helen V. and Freson, Kathleen and Greene, Daniel and Hamosh, Ada and Helbig, Ingo and Hum, Courtney and J{\"{a}}hn, Johanna A. and James, Roger and Krause, Roland and Laulederkind, Stanley J F and Lochm{\"{u}}ller, Hanns and Lyon, Gholson J. and Ogishima, Soichi and Olry, Annie and Ouwehand, Willem H. and Pontikos, Nikolas and Rath, Ana and Schaefer, Franz and Scott, Richard H. and Segal, Michael and Sergouniotis, Panagiotis I. and Sever, Richard and Smith, Cynthia L. and Straub, Volker and Thompson, Rachel and Turner, Catherine and Turro, Ernest and Veltman, Marijcke W M and Vulliamy, Tom and Yu, Jing and von Ziegenweidt, Julie and Zankl, Andreas and Z{\"{u}}chner, Stephan and Zemojtel, Tomasz and Jacobsen, Julius O B and Groza, Tudor and Smedley, Damian and Mungall, Christopher J. and Haendel, Melissa and Robinson, Peter N.},
doi = {10.1093/nar/gkw1039},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/K{\"{o}}hler et al. - 2017 - The Human Phenotype Ontology in 2017.pdf:pdf},
isbn = {1362-4962 (Electronic)},
issn = {0305-1048},
journal = {Nucleic acids research},
keywords = {diagnosis,phenotype,translational research},
month = {jan},
number = {D1},
pages = {gkw1039},
pmid = {27899602},
publisher = {Oxford University Press},
title = {{The Human Phenotype Ontology in 2017.}},
url = {https://academic.oup.com/nar/article-lookup/doi/10.1093/nar/gkw1039 http://www.ncbi.nlm.nih.gov/pubmed/27899602},
volume = {45},
year = {2016}
}
@article{Hostetter2015,
abstract = {An enormous amount of data exists in unstructured diagnostic and interventional radiology reports. Free text or non-standardized terminologies limit the ability to parse, extract, and analyze these report data elements. Medical lexicons and ontologies contain standardized terms for relevant concepts including disease entities, radiographic technique, and findings. The use of standardized terms offers the potential to improve reporting consistency and facilitate computer analysis. The purpose of this project was to implement an interface to aid in the creation of standards-compliant reporting templates for use in interventional radiology. Non-standardized procedure report text was analyzed and referenced to RadLex, SNOMED-CT, and LOINC. Using JavaScript, a web application was developed which determined whether exact terms or synonyms in reports existed within these three reference resources. The NCBO BioPortal Annotator web service was used to map terms, and output from this application was used to create an interactive annotated version of the original report. The application was successfully used to analyze and modify five distinct reports for the Society of Interventional Radiology's standardized reporting project.},
author = {Hostetter, Jason and Wang, Kenneth and Siegel, Eliot and Durack, Jeremy and Morrison, James J},
doi = {10.1007/s10278-014-9760-6},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hostetter et al. - 2015 - Using Standardized Lexicons for Report Template Validation with LexMap, a Web-based Application.pdf:pdf},
issn = {1618727X},
journal = {Journal of Digital Imaging},
keywords = {Controlled vocabulary,Data display,Education,Internet technology,Medical,Structured reporting},
month = {jun},
number = {3},
pages = {309--314},
pmid = {25561068},
publisher = {Springer},
title = {{Using Standardized Lexicons for Report Template Validation with LexMap, a Web-based Application}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25561068 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4441687},
volume = {28},
year = {2015}
}
@article{Papineni,
abstract = {Human evaluations of machine translation are extensive but expensive. Human eval- uations can take months to finish and in- volve human labor that can not be reused. We propose a method of automatic ma- chine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evalu- ation, and that has little marginal cost per run. We present this method as an auto- mated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.},
author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wj},
doi = {10.3115/1073083.1073135},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Papineni et al. - 2002 - BLEU a method for automatic evaluation of machine translation.pdf:pdf},
isbn = {1-55860-883-4},
issn = {00134686},
journal = {{\ldots} of the 40Th Annual Meeting on {\ldots}},
number = {July},
pages = {311--318},
title = {{BLEU: a method for automatic evaluation of machine translation}},
url = {http://dl.acm.org/citation.cfm?id=1073135},
year = {2002}
}
@article{Friedman1995,
abstract = {This paper describes a natural language text extraction system, called MEDLEE, that has been applied to the medical domain. The system extracts, structures, and encodes clinical information from textual patient reports. It was integrated with the Clinical Information System (CIS), which was developed at Columbia-Presbyterian Medical Center (CPMC) to help improve patient care. MEDLEE is currently used on a daily basis to routinely process radiological reports of patients at CPMC. In order to describe how the natural language system was made compatible with the existing CIS, this paper will also discuss engineering issues which involve performance, robustness, and accessibility of the data from the end users' viewpoint. Also described are the three evaluations that have been performed on the system. The first evaluation was useful primarily for further refinement of the system. The two other evaluations involved an actual clinical application which consisted of retrieving reports that were associated with specified diseases. Automated queries were written by a medical expert based on the structured output forms generated as a result of text processing. The retrievals obtained by the automated system were compared to the retrievals obtained by independent medical experts who read the reports manually to determine whether they were associated with the specified diseases. MEDLEE was shown to perform comparably to the experts. The technique used to perform the last two evaluations was found to be a realistic evaluation technique for a natural language processor.},
author = {Friedman, C. and Hripcsak, G. and DuMouchel, W. and Johnson, S. B. and Clayton, P. D.},
doi = {10.1017/S1351324900000061},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Friedman et al. - 1995 - Natural language processing in an operational clinical information system.pdf:pdf},
issn = {1469-8110},
journal = {Natural Language Engineering},
month = {mar},
number = {01},
pages = {83--108},
publisher = {Cambridge University Press},
title = {{Natural language processing in an operational clinical information system}},
url = {http://journals.cambridge.org/action/displayAbstract?fromPage=online{\&}aid=1313068},
volume = {1},
year = {1995}
}
@inproceedings{Nagata2005,
author = {Nagata, Masaaki and Pedersen, Ted},
booktitle = {Proceedings of the ACL Interactive Poster and Demonstration Sessions},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nagata, Pedersen - 2005 - Proceedings of the ACL Interactive Poster and Demonstration Sessions.pdf:pdf},
title = {{Proceedings of the ACL Interactive Poster and Demonstration Sessions}},
url = {http://ict.usc.edu/pubs/TRANSONICS- A SPEECH TO SPEECH SYSTEM FOR ENGLISH-PERSIAN INTERACTIONS.pdf http://aclweb.org/anthology/P05-3000},
year = {2005}
}
@article{Pecina2014,
abstract = {Objective: We investigate machine translation (MT) of user search queries in the context of cross-lingual information retrieval (IR) in the medical domain. The main focus is on techniques to adapt MT to increase translation quality; however, we also explore MT adaptation to improve effectiveness of cross-lingual IR. Methods and data: Our MT system is Moses, a state-of-the-art phrase-based statistical machine translation system. The IR system is based on the BM25 retrieval model implemented in the Lucene search engine. The MT techniques employed in this work include in-domain training and tuning, intelligent training data selection, optimization of phrase table configuration, compound splitting, and exploiting synonyms as translation variants. The IR methods include morphological normalization and using multiple translation variants for query expansion. The experiments are performed and thoroughly evaluated on three language pairs: Czech-English, German-English, and French-English. MT quality is evaluated on data sets created within the Khresmoi project and IR effectiveness is tested on the CLEF eHealth 2013 data sets. Results: The search query translation results achieved in our experiments are outstanding - our systems outperform not only our strong baselines, but also Google Translate and Microsoft Bing Translator in direct comparison carried out on all the language pairs. The baseline BLEU scores increased from 26.59 to 41.45 for Czech-English, from 23.03 to 40.82 for German-English, and from 32.67 to 40.82 for French-English. This is a 55{\%} improvement on average. In terms of the IR performance on this particular test collection, a significant improvement over the baseline is achieved only for French-English. For Czech-English and German-English, the increased MT quality does not lead to better IR results. Conclusions: Most of the MT techniques employed in our experiments improve MT of medical search queries. Especially the intelligent training data selection proves to be very successful for domain adaptation of MT. Certain improvements are also obtained from German compound splitting on the source language side. Translation quality, however, does not appear to correlate with the IR performance - better translation does not necessarily yield better retrieval. We discuss in detail the contribution of the individual techniques and state-of-the-art features and provide future research directions. {\textcopyright} 2014 Elsevier B.V.},
author = {Pecina, Pavel and Du{\v{s}}ek, Ondřej and Goeuriot, Lorraine and Haji{\v{c}}, Jan and Hlav{\'{a}}{\v{c}}ov{\'{a}}, Jaroslava and Jones, Gareth J F and Kelly, Liadh and Leveling, Johannes and Mare{\v{c}}ek, David and Nov{\'{a}}k, Michal and Popel, Martin and Rosa, Rudolf and Tamchyna, Ale{\v{s}} and Ure{\v{s}}ov{\'{a}}, Zdeňka},
doi = {10.1016/j.artmed.2014.01.004},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pecina et al. - 2014 - Adaptation of machine translation for multilingual information retrieval in the medical domain.pdf:pdf},
isbn = {0933-3657},
issn = {18732860},
journal = {Artificial Intelligence in Medicine},
keywords = {Compound splitting,Cross-language information retrieval,Domain adaptation of statistical machine translati,Intelligent training data selection for machine tr,Medical query translation,Statistical machine translation},
number = {3},
pages = {165--185},
pmid = {24680188},
title = {{Adaptation of machine translation for multilingual information retrieval in the medical domain}},
volume = {61},
year = {2014}
}
@article{Mansouri2008,
abstract = {Recognizing and extracting exact name entities, like Persons, Locations and Organizations are very useful to mining information from text. Learning to extract names in natural language text is called Named Entity Recognition (NER) task. Proper named entity recognition and extraction is important to solve most problems in hot research area such as Question Answering and Summarization Systems, Information Retrieval, Machine Translation, Video Annotation, Semantic Web Search and Bioinformatics. Nowadays more researchers use different methods such as Rule-base NER, Machine Learning-base NER and Hybrid NER, to identify names from text. In this paper, we review these methods and compare them based on precision in recognition and also portability using the Message Understanding Conference (MUC) named entity definition and its standard data set to find their strength and weakness of each these methods. We proposed a robust and novel Machine Learning Based method called Fuzzy support Vector Machine (FSVM) for NER},
author = {Mansouri, Alireza and Affendey, Lilly and Mamayt, Ali},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mansouri, Affendey, Mamayt - 2008 - Named Entity Recognition Approaches.pdf:pdf},
isbn = {9781479970759},
journal = {IJCSNS International Journal of Computer Science and Network Security},
keywords = {conditional random field based,maximum entropy based models,models,named entities,support vector machines},
number = {2},
title = {{Named Entity Recognition Approaches :}},
volume = {8},
year = {2008}
}
@article{G2013,
abstract = {FULL TEXT Author: Randhawa G, Journal: Canadian family physician Medecin de famille canadien[2013/04]},
author = {Randhawa, Gurdeeshpal and Ferreyra, Mariella and Ahmed, Rukhsana and Ezzat, Omar and Pottie, Kevin},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/G et al. - 2013 - Using machine translation in clinical practice.pdf:pdf},
issn = {0008-350X},
journal = {Canadian family physician Medecin de famille canadien},
number = {4},
pages = {382--383},
title = {{Using machine translation in clinical practice.}},
url = {http://europepmc.org/articles/pmc3625087{\#}b6-0590382},
volume = {59},
year = {2013}
}
@misc{Hayrinen2008a,
abstract = {Purpose: This paper reviews the research literature on electronic health record (EHR) systems. The aim is to find out (1) how electronic health records are defined, (2) how the structure of these records is described, (3) in what contexts EHRs are used, (4) who has access to EHRs, (5) which data components of the EHRs are used and studied, (6) what is the purpose of research in this field, (7) what methods of data collection have been used in the studies reviewed and (8) what are the results of these studies. Methods: A systematic review was carried out of the research dealing with the content of EHRs. A literature search was conducted on four electronic databases: Pubmed/Medline, Cinalh, Eval and Cochrane. Results: The concept of EHR comprised a wide range of information systems, from files compiled in single departments to longitudinal collections of patient data. Only very few papers offered descriptions of the structure of EHRs or the terminologies used. EHRs were used in primary, secondary and tertiary care. Data were recorded in EHRs by different groups of health care professionals. Secretarial staff also recorded data from dictation or nurses' or physicians' manual notes. Some information was also recorded by patients themselves; this information is validated by physicians. It is important that the needs and requirements of different users are taken into account in the future development of information systems. Several data components were documented in EHRs: daily charting, medication administration, physical assessment, admission nursing note, nursing care plan, referral, present complaint (e.g. symptoms), past medical history, life style, physical examination, diagnoses, tests, procedures, treatment, medication, discharge, history, diaries, problems, findings and immunization. In the future it will be necessary to incorporate different kinds of standardized instruments, electronic interviews and nursing documentation systems in EHR systems. The aspects of information quality most often explored in the studies reviewed were the completeness and accuracy of different data components. It has been shown in several studies that the use of an information system was conducive to more complete and accurate documentation by health care professionals. The quality of information is particularly important in patient care, but EHRs also provide important information for secondary purposes, such as health policy planning. Conclusion: Studies focusing on the content of EHRs are needed, especially studies of nursing documentation or patient self-documentation. One future research area is to compare the documentation of different health care professionals with the core information about EHRs which has been determined in national health projects. The challenge for ongoing national health record projects around the world is to take into account all the different types of EHRs and the needs and requirements of different health care professionals and consumers in the development of EHRs. A further challenge is the use of international terminologies in order to achieve semantic interoperability. {\textcopyright} 2007 Elsevier Ireland Ltd. All rights reserved.},
author = {H{\"{a}}yrinen, Kristiina and Saranto, Kaija and Nyk{\"{a}}nen, Pirkko},
booktitle = {International Journal of Medical Informatics},
doi = {10.1016/j.ijmedinf.2007.09.001},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/H{\"{a}}yrinen, Saranto, Nyk{\"{a}}nen - 2008 - Definition, structure, content, use and impacts of electronic health records A review of the resea.pdf:pdf},
isbn = {1386-5056 (Print)$\backslash$n1386-5056 (Linking)},
issn = {13865056},
keywords = {Computerized,Medical informatics,Medical records systems,Nursing informatics},
month = {may},
number = {5},
pages = {291--304},
pmid = {17951106},
publisher = {Elsevier},
title = {{Definition, structure, content, use and impacts of electronic health records: A review of the research literature}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17951106},
volume = {77},
year = {2008}
}
@article{Marwede2008,
abstract = {INTRODUCTION: To validate a preliminary version of a radiological lexicon (RadLex) against terms found in thoracic CT reports and to index report content in RadLex term categories.$\backslash$n$\backslash$nMATERIAL AND METHODS: Terms from a random sample of 200 thoracic CT reports were extracted using a text processor and matched against RadLex. Report content was manually indexed by two radiologists in consensus in term categories of Anatomic Location, Finding, Modifier, Relationship, Image Quality, and Uncertainty. Descriptive statistics were used and differences between age groups and report types were tested for significance using Kruskal-Wallis and Mann-Whitney Test (significance level {\textless}0.05).$\backslash$n$\backslash$nRESULTS: From 363 terms extracted, 304 (84{\%}) were found and 59 (16{\%}) were not found in RadLex. Report indexing showed a mean of 16.2 encoded items per report and 3.2 Finding per report. Term categories most frequently encoded were Modifier (1,030 of 3,244, 31.8{\%}), Anatomic Location (813, 25.1{\%}), Relationship (702, 21.6{\%}) and Finding (638, 19.7{\%}). Frequency of indexed items per report was higher in older age groups, but no significant difference was found between first study and follow up study reports. Frequency of distinct findings per report increased with patient age (p {\textless} 0.05).$\backslash$n$\backslash$nCONCLUSION: RadLex already covers most terms present in thoracic CT reports based on a small sample analysis from one institution. Applications for report encoding need to be developed to validate the lexicon against a larger sample of reports and address the issue of automatic relationship encoding.},
author = {Marwede, Dirk and Schulz, Thomas and Kahn, Thomas},
doi = {10.1007/s10278-007-9051-6},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Marwede, Schulz, Kahn - 2008 - Indexing thoracic CT reports using a preliminary version of a standardized radiological lexicon (RadLex).pdf:pdf},
isbn = {1618-727X (Electronic)$\backslash$n0897-1889 (Linking)},
issn = {08971889},
journal = {Journal of Digital Imaging},
keywords = {Chest CT,Classification,Reporting,Terminology},
month = {dec},
number = {4},
pages = {363--370},
pmid = {17661140},
publisher = {Springer},
title = {{Indexing thoracic CT reports using a preliminary version of a standardized radiological lexicon (RadLex)}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17661140 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3043846},
volume = {21},
year = {2008}
}
@incollection{Manning2009a,
author = {Manning, Christopher D. and Raghavan, Prabhakar and Sch{\"{u}}tze, Hinrich},
booktitle = {Introduction to Information Retrieval},
chapter = {2.2.1},
title = {{Tokenization}},
year = {2009}
}
@article{Smith2015,
abstract = {BACKGROUND: Ontology is one strategy for promoting interoperability of heterogeneous data through consistent tagging. An ontology is a controlled structured vocabulary consisting of general terms (such as "cell" or "image" or "tissue" or "microscope") that form the basis for such tagging. These terms are designed to represent the types of entities in the domain of reality that the ontology has been devised to capture; the terms are provided with logical definitions thereby also supporting reasoning over the tagged data. AIM: This paper provides a survey of the biomedical imaging ontologies that have been developed thus far. It outlines the challenges, particularly faced by ontologies in the fields of histopathological imaging and image analysis, and suggests a strategy for addressing these challenges in the example domain of quantitative histopathology imaging. RESULTS AND CONCLUSIONS: The ultimate goal is to support the multiscale understanding of disease that comes from using interoperable ontologies to integrate imaging data with clinical and genomics data.},
author = {Smith, Barry and Arabandi, Sivaram and Brochhausen, Mathias and Calhoun, Michael and Ciccarese, Paolo and Doyle, Scott and Gibaud, Bernard and Goldberg, Ilya and Kahn, Charles E Jr and Overton, James and Tomaszewski, John and Gurcan, Metin},
doi = {10.4103/2153-3539.159214},
issn = {2229-5089 (Print)},
journal = {Journal of pathology informatics},
keywords = {have advanced biomedical research,histopathology image ontology,histopathology imaging,how ontologies,interoperability,introduction,ontology,quantitative},
pages = {37},
pmid = {26167381},
publisher = {Medknow Publications},
title = {{Biomedical imaging ontologies: A survey and proposal for future work.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26167381 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4485195},
volume = {6},
year = {2015}
}
@article{Koehn2007,
abstract = {We describe an open-source toolkit for sta- tistical machine translation whose novel contributions are (a) support for linguisti- cally motivated factors, (b) confusion net- work decoding, and (c) efficient data for- mats for translation models and language models. In addition to the SMT decoder, the toolkit also includes a wide variety of tools for training, tuning and applying the system to many translation tasks.},
author = {Koehn, Philipp and Hoang, Hieu and Birch, Alexandra},
doi = {10.3115/1557769.1557821},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Koehn, Hoang, Birch - 2007 - Moses Open source toolkit for statistical machine translation.pdf:pdf},
isbn = {0736-587X},
journal = {Proceedings of the 45th {\ldots}},
number = {June},
pages = {177--180},
publisher = {Association for Computational Linguistics},
title = {{Moses: Open source toolkit for statistical machine translation}},
url = {http://dl.acm.org/citation.cfm?id=1557821},
year = {2007}
}
@article{Cotik2015,
abstract = {Automatic detection of relevant terms in medical reports is useful for educational purposes and for clinical research. Natural language processing (NLP) techniques can be applied in order to identify them. In this work we present an approach to classify radiology reports written in Spanish into two sets: the ones that indicate pathological findings and the ones that do not. In addition, the entities corresponding to pathological findings are identified in the reports. We use RadLex, a lexicon of English radiology terms, and NLP techniques to identify the occurrence of pathological findings. Reports are classified using a simple algorithm based on the presence of pathological findings, negation and hedge terms. The implemented algorithms were tested with a test set of 248 reports annotated by an expert, obtaining a best result of 0.72 F1 measure. The output of the classification task can be used to look for specific occurrences of pathological findings.},
author = {Cotik, Viviana and Filippo, Dar{\'{i}}o and Casta{\~{n}}o, Jos{\'{e}}},
doi = {10.3233/978-1-61499-564-7-634},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cotik, Filippo, Casta{\~{n}}o - 2015 - An Approach for Automatic Classification of Radiology Reports in Spanish.pdf:pdf},
isbn = {9781614995630},
issn = {18798365},
journal = {Studies in Health Technology and Informatics},
keywords = {Natural language processing,Negation detection,Pathological findings,Radiology reports,Text classification},
pages = {634--638},
pmid = {26262128},
title = {{An Approach for Automatic Classification of Radiology Reports in Spanish}},
volume = {216},
year = {2015}
}
@article{Antony2015,
abstract = {Tamil Siddha medicine, an ancient medicinal system has yielded us a wide range of untapped information about traditional medicines. In this paper, we explore into the various Natural Language Processing techniques that can be implemented to this syntactically rich corpus. As domain information mostly concentrates on the central concepts, we start our work by identifying the Named Entities and categorizing them. An integrated NER classifier is built which comprises of SVM and Decision Tree classifier with an accuracy as high as 95{\%}. These entities play different roles in different context. Hence their roles are labelled along with the predicates surrounding them. These roles and predicates give rise to a rule based sentence tagging system, trained by an MEM model, to tag different contents in this otherwise unstructured text. These two important techniques are then exploited to develop our Information Retrieval System that combines the methods category tagging done by Named Entity Recognition and content tagging done by Semantic Role Labelling. The system takes full advantage of the rich features of the language and hence can be expanded to other domains.},
author = {Antony, Betina J and {Suryanarayanan Mahalakshmi}, G},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Antony, Suryanarayanan Mahalakshmi - Unknown - Content-based Information Retrieval by Named Entity Recognition and Verb Semantic Role(2).pdf:pdf},
journal = {Journal of Universal Computer Science},
keywords = {H31,H33,I27,Information Retrieval,Named Entity Recognition,Semantic Role Labelling Categories,Tamil Siddha medicine},
number = {13},
pages = {1830--1848},
title = {{Content-based Information Retrieval by Named Entity Recognition and Verb Semantic Role Labelling}},
url = {http://www.jucs.org/jucs{\_}21{\_}13/content{\_}based{\_}information{\_}retrieval/jucs{\_}21{\_}13{\_}1830{\_}1848{\_}j.pdf},
volume = {21},
year = {2015}
}
@article{Usami2011,
abstract = {Named Entity Recognition (NER) is an im- portant first step for BioNLP tasks, e.g., gene normalization and event extraction. Employ- ing supervised machine learning techniques for achieving high performance recent NER systems require a manually annotated corpus in which every mention of the desired seman- tic types in a text is annotated. However, great amounts of human effort is necessary to build and maintain an annotated corpus. This study explores a method to build a high-performance NER without a manually annotated corpus, but using a comprehensible lexical database that stores numerous expressions of seman- tic types and with huge amount of unanno- tated texts. We underscore the effectiveness of our approach by comparing the performance of NERs trained on an automatically acquired training data and on a manually annotated cor- pus.},
author = {Usami, Yu and Cho, Han-Cheol and Okazaki, Naoaki and Tsujii, Jun'ichi},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Usami et al. - 2011 - Automatic Acquisition of Huge Training Data for Bio-Medical Named Entity Recognition.pdf:pdf},
isbn = {9781932432916},
journal = {Computational Linguistics},
pages = {65--73},
title = {{Automatic Acquisition of Huge Training Data for Bio-Medical Named Entity Recognition}},
url = {http://portal.acm.org/citation.cfm?id=2002902.2002912},
year = {2011}
}
@article{Kurtz2014,
abstract = {Computer-assisted image retrieval applications can assist radiologists by identifying similar images in archives as a means to providing decision support. In the classical case, images are described using low-level features extracted from their contents, and an appropriate distance is used to find the best matches in the feature space. However, using low-level image features to fully capture the visual appearance of diseases is challenging and the semantic gap between these features and the high-level visual concepts in radiology may impair the system performance. To deal with this issue, the use of semantic terms to provide high-level descriptions of radiological image contents has recently been advocated. Nevertheless, most of the existing semantic image retrieval strategies are limited by two factors: they require manual annotation of the images using semantic terms and they ignore the intrinsic visual and semantic relationships between these annotations during the comparison of the images. Based on these considerations, we propose an image retrieval framework based on semantic features that relies on two main strategies: (1) automatic "soft" prediction of ontological terms that describe the image contents from multi-scale Riesz wavelets and (2) retrieval of similar images by evaluating the similarity between their annotations using a new term dissimilarity measure, which takes into account both image-based and ontological term relations. The combination of these strategies provides a means of accurately retrieving similar images in databases based on image annotations and can be considered as a potential solution to the semantic gap problem. We validated this approach in the context of the retrieval of liver lesions from computed tomographic (CT) images and annotated with semantic terms of the RadLex ontology. The relevance of the retrieval results was assessed using two protocols: evaluation relative to a dissimilarity reference standard defined for pairs of images on a 25-images dataset, and evaluation relative to the diagnoses of the retrieved images on a 72-images dataset. A normalized discounted cumulative gain (NDCG) score of more than 0.92 was obtained with the first protocol, while AUC scores of more than 0.77 were obtained with the second protocol. This automatical approach could provide real-time decision support to radiologists by showing them similar images with associated diagnoses and, where available, responses to therapies. {\textcopyright} 2014 Elsevier B.V.},
author = {Kurtz, Camille and Depeursinge, Adrien and Napel, Sandy and Beaulieu, Christopher F and Rubin, Daniel L},
doi = {10.1016/j.media.2014.06.009},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kurtz et al. - 2014 - On combining image-based and ontological semantic dissimilarities for medical image retrieval applications.pdf:pdf},
isbn = {3318394580},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {Computed tomographic (CT) images,Image annotation,Image retrieval,Riesz wavelets,Semantic dissimilarities},
month = {oct},
number = {7},
pages = {1082--1100},
pmid = {25036769},
publisher = {NIH Public Access},
title = {{On combining image-based and ontological semantic dissimilarities for medical image retrieval applications}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25036769 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4173098},
volume = {18},
year = {2014}
}
@article{Spanier2016,
author = {Spanier, A. B. and Cohen, D. and Joskowicz, L.},
doi = {10.1007/s11548-016-1496-y},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Spanier, Cohen, Joskowicz - 2016 - A new method for the automatic retrieval of medical cases based on the RadLex ontology.pdf:pdf},
issn = {1861-6410},
journal = {International Journal of Computer Assisted Radiology and Surgery},
keywords = {medical content-based image retrieval,radlex ontology,relatedness,similarity metric},
number = {3},
pages = {471--484},
title = {{A new method for the automatic retrieval of medical cases based on the RadLex ontology}},
url = {http://link.springer.com/10.1007/s11548-016-1496-y},
volume = {12},
year = {2016}
}
@article{Vanel2007a,
author = {Vanel, Daniel},
doi = {10.1016/j.ejrad.2006.08.030},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vanel - 2007 - The American College of Radiology (ACR) Breast Imaging and Reporting Data System (BI-RADS) A step towards a universal rad.pdf:pdf},
isbn = {0720-048X (Print)},
issn = {0720048X},
journal = {European Journal of Radiology},
month = {feb},
number = {2},
pages = {183},
pmid = {17164078},
publisher = {Elsevier},
title = {{The American College of Radiology (ACR) Breast Imaging and Reporting Data System (BI-RADS???): A step towards a universal radiological language?}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17164078},
volume = {61},
year = {2007}
}
@incollection{Maisto2016,
author = {Maisto, Alessandro and Guarasci, Raffaele},
doi = {10.1007/978-3-319-42471-2_15},
pages = {172--181},
publisher = {Springer International Publishing},
title = {{Morpheme-Based Recognition and Translation of Medical Terms}},
url = {http://link.springer.com/10.1007/978-3-319-42471-2{\_}15},
year = {2016}
}
@article{Babych2003,
abstract = {Named entities create serious problems$\backslash$nfor state-of-the-art commercial machine$\backslash$ntranslation (MT) systems and often cause$\backslash$ntranslation failures beyond the local$\backslash$ncontext, affecting both the overall$\backslash$nmorphosyntactic well-formedness of$\backslash$nsentences and word sense disambiguation$\backslash$nin the source text. We report on the$\backslash$nresults of an experiment in which MT$\backslash$ninput was processed using output from$\backslash$nthe named entity recognition module of$\backslash$nSheffield's GATE information extraction$\backslash$n(IE) system. The gain in MT quality$\backslash$nindicates that specific components of IE$\backslash$ntechnology could boost the performance$\backslash$nof current MT systems.},
author = {Babych, Bogdan and Hartley, Anthony},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Babych, Hartley - Unknown - Improving Machine Translation Quality with Automatic Named Entity Recognition.pdf:pdf},
journal = {{\ldots} and other Language Technology Tools, Improving {\ldots}},
title = {{Improving machine translation quality with automatic named entity recognition}},
url = {http://www.mt-archive.info/EAMT-2003-Babych.pdf http://dl.acm.org/citation.cfm?id=1609823},
year = {2003}
}
@article{Wei2016,
abstract = {Citation details: Wei,Q., Chen,T., Xu,R. et al. Disease named entity recognition by combining conditional random fields and bidirectional recurrent neural networks. Abstract The recognition of disease and chemical named entities in scientific articles is a very im-portant subtask in information extraction in the biomedical domain. Due to the diversity and complexity of disease names, the recognition of named entities of diseases is rather tougher than those of chemical names. Although there are some remarkable chemical named entity recognition systems available online such as ChemSpot and tmChem, the publicly available recognition systems of disease named entities are rare. This article pre-sents a system for disease named entity recognition (DNER) and normalization. First, two separate DNER models are developed. One is based on conditional random fields model with a rule-based post-processing module. The other one is based on the bidirectional re-current neural networks. Then the named entities recognized by each of the DNER model are fed into a support vector machine classifier for combining results. Finally, each rec-ognized disease named entity is normalized to a medical subject heading disease name by using a vector space model based method. Experimental results show that using 1000 PubMed abstracts for training, our proposed system achieves an F1-measure of 0.8428 at the mention level and 0.7804 at the concept level, respectively, on the testing data of the chemical-disease relation task in BioCreative V.},
author = {Wei, Qikang and Chen, Tao and Xu, Ruifeng and He, Yulan and Gui, Lin},
doi = {10.1093/database/baw140},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wei et al. - 2016 - Disease named entity recognition by combining conditional random fields and bidirectional recurrent neural networks.pdf:pdf},
isbn = {17580463 (Electronic)},
issn = {1758-0463},
journal = {Database (Oxford)},
month = {oct},
pages = {1--8},
pmid = {27777244},
publisher = {Oxford University Press},
title = {{Disease named entity recognition by combining conditional random fields and bidirectional recurrent neural networks}},
url = {https://academic.oup.com/database/article-lookup/doi/10.1093/database/baw140 http://219.223.252.210:8080/SS/cdr.html},
volume = {2016},
year = {2016}
}
@article{Porter1980,
abstract = {The automatic removal of suffixes from words in English is of particular interest in the field of information retrieval. An algorithm for suffix stripping is described, which has been implemented as a short, fast program in BCPL....},
archivePrefix = {arXiv},
arxivId = {http://dx.doi.org/10.1108/BIJ-10-2012-0068},
author = {Porter, M.F.},
doi = {10.1108/eb046814},
eprint = {/dx.doi.org/10.1108/BIJ-10-2012-0068},
isbn = {1558604545},
issn = {0033-0337},
journal = {Program: electronic library and information systems},
number = {3},
pages = {130--137},
pmid = {16143652},
primaryClass = {http:},
title = {{An algorithm for suffix stripping}},
volume = {14},
year = {1980}
}
@inproceedings{Durrani2014,
abstract = {This paper describes the University of Ed-inburgh's (UEDIN) phrase-based submis-sions to the translation and medical trans-lation shared tasks of the 2014 Work-shop on Statistical Machine Translation (WMT). We participated in all language pairs. We have improved upon our 2013 system by i) using generalized represen-tations, specifically automatic word clus-ters for translations out of English, ii) us-ing unsupervised character-based models to translate unknown words in Russian-English and Hindi-English pairs, iii) syn-thesizing Hindi data from closely-related Urdu data, and iv) building huge language models on the common crawl corpus.},
author = {Durrani, Nadir and Haddow, Barry and Koehn, Philipp and Heafield, Kenneth},
booktitle = {Proceedings of the Ninth Workshop on Statistical Machine Translation},
doi = {10.3115/v1/W14-3309},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Durrani et al. - 2014 - Edinburgh's Phrase-based Machine Translation Systems for WMT-14.pdf:pdf},
pages = {97--104},
title = {{Edinburgh's Phrase-based Machine Translation Systems for WMT-14}},
url = {http://aclweb.org/anthology/W14-3309},
year = {2014}
}
@incollection{Toral2005,
abstract = {This paper studies the use of Named Entity Recognition (NER) for the Question Anwering (QA) task in Spanish texts. NER applied as a preprocessing step not only helps to detect the answer to the question but also decreases the amount of data to be considered by QA. Our proposal reduces a 26{\%} the quantity of data and moreover increases a 9{\%} the efficiency of the system.},
author = {Toral, Antonio and Noguera, Elisa and Llopis, Fernando and Munoz, R},
booktitle = {Natural Language Processing and Information Systems, Proceedings},
doi = {10.1007/11428817_17},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Toral et al. - 2005 - Improving question answering using named entity recognition.pdf:pdf},
isbn = {0302-9743$\backslash$r3-540-26031-5},
issn = {3-540-66068-2},
pages = {181--191},
publisher = {Springer, Berlin, Heidelberg},
title = {{Improving question answering using named entity recognition}},
url = {http://link.springer.com/10.1007/11428817{\_}17},
volume = {3513},
year = {2005}
}
@incollection{Cancedda2009,
abstract = {This first chapter is a short introduction to the main aspects of Statistical Machine Translation. In particular, we cover the issues of automatic evaluation of Machine Translation output, language modelling, word-based and phrased-based translation models, and the use of syntax in Machine Translation. We will also do a quick round-up of some more recent directions that we believe may gain importance in the future. We put this in the general context of Machine Learning research, and put the emphasis on similarities and differences with standard Machine Learning problems and practice.},
author = {Cancedda, Nicola and Dymetman, Marc and Foster, George and Goutte, Cyril},
booktitle = {Learning Machine Translation},
doi = {10.7551/mitpress/9780262072977.003.0001},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cancedda et al. - 2009 - A statistical machine translation primer.pdf:pdf},
month = {nov},
pages = {1--36},
publisher = {The MIT Press},
title = {{A statistical machine translation primer}},
url = {http://mitpress.universitypressscholarship.com/view/10.7551/mitpress/9780262072977.001.0001/upso-9780262072977-chapter-1},
year = {2009}
}
@article{Rodrigues2014,
abstract = {Projeto de mestrado em Engenharia Inform{\'{a}}tica, apresentada {\`{a}} Universidade de Lisboa, atrav{\'{e}}s da Faculdade de Ci{\^{e}}ncias, 2013},
author = {Rodrigues, Jo{\~{a}}o A.},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rodrigues - 2014 - Speech-to-speech translation to support medical interviews.pdf:pdf},
keywords = {2013,Dom{\'{i}}nio m{\'{e}}dico,Reconhecimento de fala,Sintetiza{\c{c}}{\~{a}}o de voz,Teses de mestrado,Tradu{\c{c}}{\~{a}}o autom{\'{a}}tica estat{\'{i}}stica,Tradu{\c{c}}{\~{a}}o fala,fala,para},
title = {{Speech-to-speech translation to support medical interviews}},
year = {2014}
}
@inproceedings{Bentivogli2016,
abstract = {Within the field of Statistical Machine Translation (SMT), the neural approach (NMT) has recently emerged as the first technology able to challenge the long-standing dominance of phrase-based approaches (PBMT). In particular, at the IWSLT 2015 evaluation campaign, NMT outperformed well established state-of-the-art PBMT systems on English-German, a language pair known to be particularly hard because of morphology and syntactic differences. To understand in what respects NMT provides better translation quality than PBMT, we perform a detailed analysis of neural versus phrase-based SMT outputs, leveraging high quality post-edits performed by professional translators on the IWSLT data. For the first time, our analysis provides useful insights on what linguistic phenomena are best modeled by neural models -- such as the reordering of verbs -- while pointing out other aspects that remain to be improved.},
archivePrefix = {arXiv},
arxivId = {1608.04631},
author = {Bentivogli, Luisa and Bisazza, Arianna and Cettolo, Mauro and Federico, Marcello},
booktitle = {To appear: EMNLP-2016},
eprint = {1608.04631},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bentivogli et al. - 2016 - Neural versus Phrase-Based Machine Translation Quality a Case Study(2).pdf:pdf},
month = {aug},
title = {{Neural versus Phrase-Based Machine Translation Quality: a Case Study}},
url = {http://arxiv.org/abs/1608.04631},
year = {2016}
}
@article{Taira2001,
abstract = {A natural language processor was developed that automatically structures the important medical information (eg, the existence, properties, location, and diagnostic interpretation of findings) contained in a radiology free-text document as a formal information model that can be interpreted by a computer program. The input to the system is a free-text report from a radiologic study. The system requires no reporting style changes on the part of the radiologist. Statistical and machine learning methods are used extensively throughout the system. A graphical user interface has been developed that allows the creation of hand-tagged training examples. Various aspects of the difficult problem of implementing an automated structured reporting system have been addressed, and the relevant technology is progressing well. Extensible Markup Language is emerging as the preferred syntactic standard for representing and distributing these structured reports within a clinical environment. Early successes hold out hope that...},
author = {Taira, Ricky K. and Soderland, Stephen G. and Jakobovits, Rex M.},
doi = {10.1148/radiographics.21.1.g01ja18237},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Taira, Soderland, Jakobovits - 2001 - Automatic Structuring of Radiology Free-Text Reports.pdf:pdf},
issn = {0271-5333},
journal = {RadioGraphics},
keywords = {Computers,Computers, diagnostic aid,Radiology reporting systems},
month = {jan},
number = {1},
pages = {237--245},
publisher = { Radiological Society of North America },
title = {{Automatic Structuring of Radiology Free-Text Reports}},
url = {http://pubs.rsna.org/doi/10.1148/radiographics.21.1.g01ja18237},
volume = {21},
year = {2001}
}
@article{Barros2016,
abstract = {Introduction: Biomedical research is increasingly becoming a data-intensive science in several areas, where prodigious amounts of data is being generated that has to be stored, integrated, shared and analyzed. In an effort to improve the accessibility of data and knowledge, the Linked Data initiative proposed a well-defined set of recommendations for exposing, sharing and integrating data, information and knowledge, using semantic web technologies. Objective: The main goal of this paper is to identify the current status and future trends of knowledge representation and management in Life and Health Sciences, mostly with regard to linked data technologies. Methods: We selected three prominent linked data studies, namely Bio2RDF, Open PHACTS and EBI RDF platform, and selected 14 studies published after 2014 (inclusive) that cited any of the three studies. We manually analyzed these 14 papers in relation to how they use linked data techniques. Results: The analyses show a tendency to use linked data techniques in Life and Health Sciences, and even if some studies do not follow all of the recommendations, many of them already represent and manage their knowledge using RDF and biomedical ontologies. Conclusion: These insights from RDF and biomedical ontologies are having a strong impact on how knowledge is generated from biomedical data, by making data elements increasingly connected and by providing a better description of their semantics. As health institutes become more data centric, we believe that the adoption of linked data techniques will continue to grow and be an effective solution to knowledge representation and management.},
author = {Barros, M. and Couto, F. M.},
doi = {10.15265/IY-2016-022},
issn = {0943-4747},
journal = {IMIA Yearbook},
keywords = {Information Management,Medical Informatics,Ontologies,RDF,common data elements,information storage and retrieval},
number = {1},
pages = {178--183},
publisher = {Schattauer Publishers},
title = {{Knowledge Representation and Management: a Linked Data Perspective}},
url = {http://www.schattauer.de/index.php?id=1214{\&}doi=10.15265/IY-2016-022},
year = {2016}
}
@article{Friedlin,
abstract = {We have developed a natural language processing system for extracting and coding clinical data from free text reports. The system is designed to be easily modified and adapted to a variety of free text clinical reports such as admission notes, radiology and pathology reports, and discharge summaries. This report presents the results of this system to extract and code clinical concepts related to congestive heart failure from 39,000 chest radiology reports. The system detects the presence or absence of six concepts: congestive heart failure, Kerley B lines, cardiomegaly, prominent pulmonary vasculature, pulmonary edema, and pleural effusion. We compared it's output to a gold standard which consisted of specially trained human coders as well as an experienced physician. Results indicate that the system had high specificity, recall and precision for each of the concepts it is designed to detect.},
author = {Friedlin, Jeff and McDonald, Clement J},
doi = {86418 [pii]},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Friedlin, McDonald - 2006 - A natural language processing system to extract and code concepts relating to congestive heart failure from.pdf:pdf},
isbn = {1942-597X (Electronic)$\backslash$r1559-4076 (Linking)},
issn = {1942-597X},
journal = {AMIA ... Annual Symposium proceedings / AMIA Symposium. AMIA Symposium},
keywords = {Forms and Records Control,Heart Failure,Heart Failure: classification,Heart Failure: radiography,Humans,Natural Language Processing,Predictive Value of Tests,Radiography,Sensitivity and Specificity,Thoracic,Thoracic: classification},
number = {January 2004},
pages = {269--73},
pmid = {17238345},
title = {{A natural language processing system to extract and code concepts relating to congestive heart failure from chest radiology reports.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1839544{\&}tool=pmcentrez{\&}rendertype=abstract{\%}5Cnhttp://www.ncbi.nlm.nih.gov/pubmed/17238345{\%}5Cnhttp://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC1839544},
volume = {2005},
year = {2006}
}
@article{Klein2013,
author = {Klein, Jeffrey S.},
doi = {10.1148/rg.331125217},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Klein - 2013 - A Look Back at 2012 and Plans for 2013.pdf:pdf},
issn = {0271-5333},
journal = {RadioGraphics},
month = {jan},
number = {1},
pages = {1--2},
publisher = {Radiological Society of North America},
title = {{A Look Back at 2012 and Plans for 2013}},
url = {http://pubs.rsna.org/doi/10.1148/rg.331125217},
volume = {33},
year = {2013}
}
@article{Channin2009,
abstract = {The Annotation and Image Mark-up Project is a standardized semantically interoperable information model with storage and communication formats for image annotation and markup.},
author = {Project, Mark-up and Kleper, Vladimir and Rubin, Daniel L.},
doi = {10.1148/radiol.2533090135},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Project, Kleper, Rubin - 2009 - The Annotation and Image.pdf:pdf},
issn = {0033-8419},
journal = {Radiology},
month = {dec},
number = {3},
pages = {590--592},
publisher = {Radiological Society of North America, Inc.},
title = {{The Annotation and Image}},
url = {http://pubs.rsna.org/doi/10.1148/radiol.2533090135},
volume = {253},
year = {2009}
}
@article{Tanenblatt2010,
abstract = {ConceptMapper is an open source tool we created for classifying mentions in an unstructured text document based on concept terminologies and yielding named entities as output. It is implemented as a UIMA1 (Unstructured Information Management Architecture (IBM, 2004)) annotator, and concepts come from standardised or proprietary terminologies. ConceptMapper can be easily configured, for instance, to use different search strategies or syntactic concepts. In this paper we will describe ConceptMapper, its configuration parameters and their trade-offs, in terms of precision and recall in identifying concepts in a collection of clinical reports written in English. ConceptMapper is available from the Apache UIMA Sandbox, using the Apache Open Source license.},
author = {Tanenblatt, Michael and Coden, Anni and Sominsky, Igor},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tanenblatt, Coden, Sominsky - 2010 - The ConceptMapper Approach to Named Entity Recognition.pdf:pdf},
isbn = {2951740867},
journal = {Proceedings of the Seventh conference on International Language Resources and Evaluation LREC10},
number = {January 2010},
pages = {546--551},
title = {{The ConceptMapper Approach to Named Entity Recognition}},
year = {2010}
}
@incollection{Tatsumi2012a,
abstract = {This paper is a partial report of a research effort on evaluating the effect of crowd-sourced post-editing. We first discuss the emerging trend of crowd-sourced post-editing of machine translation output, along with its benefits and drawbacks. Second, we describe the pilot study we have conducted on a platform that facilitates crowd-sourced post-editing. Finally, we provide our plans for further studies to have more insight on how effective crowd-sourced post-editing is.},
address = {San Diego, CA},
author = {Tatsumi, Midori and Aikawa, Takako and Yamamoto, Kentaro and Isahara, Hitoshi},
booktitle = {The Proceedings of Association for Machine Translation in the Americas (AMTA)},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tatsumi et al. - 2012 - How Good Is Crowd Post-Editing Its Potential and Limitations(2).pdf:pdf},
title = {{How Good Is Crowd Post-Editing? Its Potential and Limitations}},
url = {http://www.mt-archive.info/AMTA-2012-Tatsumi.pdf},
year = {2012}
}
@article{Sutton2002,
abstract = {Relational data has two characteristics: first, statistical dependencies exist between the entities we wish to model, and second, each entity often has a rich set of features that can aid classification. For example, when classifying Web documents, the pages text provides much information about the class label, but hyperlinks define a relationship between pages that can improve classification Taskar et al., 2002. Graphical models are a natural formalism for exploiting the dependence structure among entities. Traditionally, graphical models have been used to represent the joint probability distribution p(y, x), where the variables y represent the attributes of the entities that we wish to predict, and the input variables x represent our observed knowledge about the entities. But modeling the joint distribution can lead to difficulties when using the rich local features that can occur in relational data, because it requires modeling the distribution p(x), which can include complex dependencies. Modeling these dependencies among inputs can lead to intractable models, but ignoring them can lead to reduced performance. A solution to this problem is to directly model the conditional distribution p(yx), which is sufficient for classification. This is the approach taken by conditional random fields Lafferty et al., 2001. A conditional random field is simply a conditional distribution p(yx) with an associated graphical structure. Because the model is conditional, dependencies among the input variables x do not need to be explicitly represented, affording the use of rich, global features of the input. For example, in natural language tasks, useful features include neighboring words and word bigrams, prefixes and suffixes, capitalization, membership in domain-specific lexicons, and semantic information from sources such as WordNet. Recently there has been an explosion of interest in CRFs, with successful applications including text processing Taskar et al., 2002, Peng and McCallum, 2004, Settles, 2005, Sha and Pereira, 2003, bioinformatics Sato and Sakakibara, 2005, Liu et al., 2005, and computer vision He et al., 2004, Kumar and Hebert, 2003. This chapter is divided into two parts. First, we present a tutorial on current training and inference techniques for conditional random fields. We discuss the important special case of linear-chain CRFs, and then we generalize these to arbitrary graphical structures. We include a brief discussion of techniques for practical CRF implementations. Second, we present an example of applying a general CRF to a practical relational learning problem. In particular, we discuss the problem of information extraction, that is, automatically building a relational database from information contained in unstructured text. Unlike linear-chain models, general CRFs can capture long distance dependencies between labels. For example, if the same name is mentioned more than once in a document, all mentions probably have the same label, and it is useful to extract them all, because each mention may contain different complementary information about the underlying entity. To represent these long-distance dependencies, we propose a skip-chain CRF, a model that jointly performs segmentation and collective labeling of extracted mentions. On a standard problem of extracting speaker names from seminar announcements, the skip-chain CRF has better performance than a linear-chain CRF.},
archivePrefix = {arXiv},
arxivId = {1011.4088v1},
author = {Sutton, Charles and Mccallum, Andrew},
doi = {10.1677/JME-08-0087},
eprint = {1011.4088v1},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Getoor, Taskar - 2007 - Introduction to statistical relational learning.pdf:pdf},
isbn = {0262072882},
issn = {14796813},
journal = {Graphical Models},
pages = {93},
pmid = {19008334},
title = {{An Introduction to Conditional Random Fields for Relational Learning}},
url = {http://www.cs.umass.edu/∼casutton http://www.cs.umass.edu/∼mccallum http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:An+Introduction+to+Conditional+Random+Fields+for+Relational+Learning{\#}2},
volume = {7},
year = {2002}
}
@article{Eck2004,
address = {Morristown, NJ, USA},
author = {Eck, Matthias and Vogel, Stephan and Waibel, Alex},
doi = {10.3115/1220355.1220469},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Eck, Vogel, Waibel - 2004 - Improving statistical machine translation in the medical domain using the unified medical language system.pdf:pdf},
journal = {Proceedings of the 20th international conference on Computational Linguistics - COLING '04},
pages = {792--es},
publisher = {Association for Computational Linguistics},
title = {{Improving statistical machine translation in the medical domain using the unified medical language system}},
url = {http://portal.acm.org/citation.cfm?doid=1220355.1220469},
year = {2004}
}
@article{Cai2016,
abstract = {Natural language processing technologies are discussed, with emphasis on their use in advancing radiology research and practice by leveraging the large volume of data in electronic medical records.},
author = {Cai, Tianrun and Giannopoulos, Andreas A and Yu, Sheng and Kelil, Tatiana and Ripley, Beth and Kumamaru, Kanako K and Rybicki, Frank J and Mitsouras, Dimitrios},
doi = {10.1148/rg.2016150080},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cai et al. - 2016 - Natural Language Processing Technologies in Radiology Research and Clinical Applications.pdf:pdf},
isbn = {10.1148/rg.2016150080},
issn = {1527-1323},
journal = {RadioGraphics},
number = {1},
pages = {176--191},
pmid = {26761536},
publisher = {Radiological Society of North America},
title = {{Natural Language Processing Technologies in Radiology Research and Clinical Applications}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26761536 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4734053 http://pubs.rsna.org/doi/abs/10.1148/rg.2016150080},
volume = {36},
year = {2016}
}
@inproceedings{Wu2011,
abstract = {In our paper we addressed the research question: "Has machine translation achieved sufficiently high quality to translate PubMed titles for patients?". We analyzed statistical machine translation output for six foreign language - English translation pairs (bi-directionally). We built a high performing in-house system and evaluated its output for each translation pair on large scale both with automated BLEU scores and human judgment. In addition to the in-house system, we also evaluated Google Translate's performance specifically within the biomedical domain. We report high performance for German, French and Spanish -- English bi-directional translation pairs for both Google Translate and our system.},
annote = {Tested Google Translate for translating biomedical text},
author = {Wu, Cuijun and Xia, Fei and Deleger, Louise and Solti, Imre},
booktitle = {AMIA Annu Symp Proc},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu et al. - 2011 - Statistical machine translation for biomedical text are we there yet.pdf:pdf},
issn = {1942-597X},
keywords = {Artificial Intelligence,Natural Language Processing,PubMed,Search Engine,Translating},
pages = {1290--9},
pmid = {22195190},
title = {{Statistical machine translation for biomedical text: are we there yet?}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3243244{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {2011},
year = {2011}
}
@article{McCrae2016,
abstract = {Ontology localization is the task of adapting an ontology to a different cultural context, and has been identified as an important task in the context of the Multilingual Semantic Web vision. The key task in ontology localization is translating the lexical layer of an ontology, i.e., its labels, into some foreign language. For this task, we hypothesize that the translation quality can be improved by adapting a machine translation system to the domain of the ontology. To this end, we build on the success of existing statistical machine translation (SMT) approaches, and investigate the impact of different domain adaptation techniques on the task. In particular, we investigate three techniques: (i) enriching a phrase table by domain-specific translation candidates acquired from existing Web resources, (ii) relying on Explicit Semantic Analysis as an additional technique for scoring a certain translation of a given source phrase, as well as (iii) adaptation of the language model by means of weighting n-grams with scores obtained from topic modelling. We present in detail the impact of each of these three techniques on the task of translating ontology labels. We show that these techniques have a generally positive effect on the quality of translation of the ontology and that, in combination, they provide a significant improvement in quality.},
author = {McCrae, John P. and Arcan, Mihael and Asooja, Kartik and Gracia, Jorge and Buitelaar, Paul and Cimiano, Philipp},
doi = {10.1016/j.websem.2015.12.001},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/McCrae et al. - 2016 - Domain adaptation for ontology localization.pdf:pdf},
issn = {15708268},
journal = {Journal of Web Semantics},
keywords = {Domain adaptation,Ontology localization,Statistical machine translation},
pages = {23--31},
title = {{Domain adaptation for ontology localization}},
volume = {36},
year = {2016}
}
@techreport{Branco2003,
abstract = {This research note addresses the issue of ambiguous strings, strings of non-whitespace characters whose tokenization, depending of the specific occurrence, yields one or more than one token. This sort of strings, typically coinciding with orthographically contracted forms, is shown to raise the problem of undesired circularity between tokenization and tagging, under the standard view that tokenization takes place before tagging. The critical importance of this apparently minor, low-level issue results from the fact that these strings correspond mostly to functional words, that they are quite frequent, covering over 2{\%} of a corpus, and that their careless treatment would introduce unrecoverable degradation of performance at a very early stage of language processing and that this degradation would trigger further and wider loss of accuracy in all subsequent processing stages. We argue for a resolution of this circularity on the basis of a new, two-level approach to tokenization. This approach is shown to be used also to improve the problem of sentence chunking at periods that are ambivalent between marking the end of an abbreviation and the end of a sentence.},
author = {Branco, Ant{\'{o}}nio and Silva, J.},
booktitle = {Universidade de Lisboa},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Branco, Silva, Silva - 2003 - Tokenization of Portuguese resolving the hard cases.pdf:pdf},
institution = {University Of Lisbon},
title = {{Tokenization of Portuguese: resolving the hard cases}},
url = {http://repositorio.ul.pt/bitstream/10451/14199/1/03-4.pdf http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.12.9238{\&}rep=rep1{\&}type=pdf},
year = {2003}
}
@article{Heilbrun2013,
author = {Heilbrun, Marta E.},
doi = {10.1016/j.acra.2013.09.011},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Heilbrun - 2013 - Evaluating radlex and real world radiology reporting. Are we there yet.pdf:pdf},
issn = {10766332},
journal = {Academic Radiology},
number = {11},
pages = {1327--1328},
publisher = {AUR},
title = {{Evaluating radlex and real world radiology reporting. Are we there yet?}},
url = {http://dx.doi.org/10.1016/j.acra.2013.09.011},
volume = {20},
year = {2013}
}
@article{Pons2016,
abstract = {Radiological reporting has generated large quantities of digital content within the electronic health record, which is potentially a valuable source of information for improving clinical care and supporting research. Although radiology reports are stored for communication and documentation of diagnostic imaging, harnessing their potential requires efficient and automated information extraction: they exist mainly as free-text clinical narrative, from which it is a major challenge to obtain structured data. Natural language processing (NLP) provides techniques that aid the conversion of text into a structured representation, and thus enables computers to derive meaning from human (ie, natural language) input. Used on radiology reports, NLP techniques enable automatic identification and extraction of information. By exploring the various purposes for their use, this review examines how radiology benefits from NLP. A systematic literature search identified 67 relevant publications describing NLP methods that support practical applications in radiology. This review takes a close look at the individual studies in terms of tasks (ie, the extracted information), the NLP methodology and tools used, and their application purpose and performance results. Additionally, limitations, future challenges, and requirements for advancing NLP in radiology will be discussed. ({\textcopyright}) RSNA, 2016 Online supplemental material is available for this article.},
author = {Pons, Ewoud and Braun, Loes M. M. and Hunink, M. G. Myriam and Kors, Jan A.},
doi = {10.1148/radiol.16142770},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pons et al. - 2016 - Natural Language Processing in Radiology A Systematic Review.pdf:pdf},
isbn = {0033-8419},
issn = {0033-8419},
journal = {Radiology},
number = {2},
pages = {329--343},
pmid = {27089187},
title = {{Natural Language Processing in Radiology: A Systematic Review}},
url = {http://pubs.rsna.org/doi/10.1148/radiol.16142770},
volume = {279},
year = {2016}
}
@article{Goeuriot,
abstract = {This paper reports on the 3rd CLEFeHealth evaluation lab, which continues our evaluation resource building activities for the med-ical domain. In this edition of the lab, we focus on easing patients and nurses in authoring, understanding, and accessing eHealth information. The 2015 CLEFeHealth evaluation lab was structured into two tasks, fo-cusing on evaluating methods for information extraction (IE) and infor-mation retrieval (IR). The IE task introduced two new challenges. Task 1a focused on clinical speech recognition of nursing handover notes; Task 1b focused on clinical named entity recognition in languages other than English, specifically French. Task 2 focused on the retrieval of health information to answer queries issued by general consumers seeking infor-mation to understand their health symptoms or conditions. The number of teams registering their interest was 47 in Tasks 1 (2 teams in Task 1a and 7 teams in Task 1b) and 53 in Task 2 (12 teams) for a total of 20 unique teams. The best system recognized 4, 984 out of 6, 818 test words correctly and generated 2, 626 incorrect words (i.e., 38.5{\%} error) in Task 1a; had the F-measure of 0.756 for plain entity recognition, 0.711 for normalized entity recognition, and 0.872 for entity normalization in Task 1b; and resulted in P@10 of 0.5394 and nDCG@10 of 0.5086 in Task 2. These results demonstrate the substantial community interest and capabilities of these systems in addressing challenges faced by patients and nurses. As in previous years, the organizers have made data and tools available for future research and development.},
author = {Goeuriot, Lorraine and Kelly, Liadh and Suominen, Hanna and Hanlen, Leif and N{\'{e}}v{\'{e}}ol, Aur{\'{e}}lie and Grouin, Cyril and Palotti, Jo{\~{a}}o and Zuccon, Guido},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Goeuriot et al. - Unknown - Overview of the CLEF eHealth Evaluation Lab 2015.pdf:pdf},
keywords = {Evaluation,Information Extraction,Information Retrieval,Medi-cal Informatics,Nursing Records,Patient Handoff/Handover,Self-Diagnosis,Speech Recogni-tion,Test-set Generation,Text Classification,Text Segmentation},
title = {{Overview of the CLEF eHealth Evaluation Lab 2015}}
}
@incollection{Mohit2014,
author = {Mohit, Behrang},
booktitle = {Natural Language Processing of Semitic Languages},
chapter = {7},
doi = {10.1007/978-3-642-45358-8},
editor = {Zitouni, Imed},
isbn = {978-3-642-45358-8},
pages = {221--246},
publisher = {Springer},
title = {{Named Entity Recognition}},
year = {2014}
}
@book{Koehn2010,
author = {Koehn and Philipp},
booktitle = {Cambridge: Cambridge University Press},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Koehn, Philipp - 2010 - Statistical Machine Translation.pdf:pdf},
isbn = {0521874157, 9780521874151},
pages = {433},
publisher = {Cambridge University Press},
title = {{Statistical Machine Translation}},
year = {2010}
}
@article{A.BergerandS.D.PietraandV.D.Pietra1996,
abstract = {Page 1. A Maximum Approach to Natural Language Processing The concept of maximum can be traced back along multiple threads to Biblical times.},
author = {{A. Berger and S. D. Pietra and V. D. Pietra}},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/A. Berger and S. D. Pietra and V. D. Pietra - 1996 - A Maximum Entropy Approach to Natural Language Processing.pdf:pdf},
isbn = {123337245},
issn = {0891-2017},
journal = {Computational Linguistics},
number = {1},
pages = {39--71},
publisher = {MIT Press},
title = {{A Maximum Entropy Approach to Natural Language Processing}},
url = {http://portal.acm.org/citation.cfm?id=234285.234289{\%}5Cnpapers://e7d065ae-9998-4287-8af0-c9fa85af8e96/Paper/p14890},
volume = {22},
year = {1996}
}
@article{Woods2013,
abstract = {Rationale and Objectives: RadLex was developed to create a unified language for radiologists. Despite the large number of terms, little research has evaluated the degree to which RadLex contains terms frequently used in clinical practice. The purposes of this project are to estimate the completeness of RadLex in the chest radiography domain and to characterize the absent terms. We chose chest radiography because it is a common exam generating a large number of reports, and the terms used represent a relatively well-circumscribed set of terms compared to other anatomic regions and modalities. Materials and Methods: We collected a random sample of 100 chest radiograph reports from 1 month of routine clinical practice of three board-certified radiologists. We parsed each report's findings and impression sections into individual objects. An "object" was defined as any discrete physical object, body part, observation, descriptive modifier, diagnosis, or procedure. Objects were compared to RadLex by entering the object into the RadLex Term Browser. We calculated descriptive statistics and compared the match rate across RadLex categories. Results: We identified 339 unique objects, with an overall match rate of 62{\%}. The match rate for each category was anatomic object, 77{\%}; physiological condition, 73{\%}; physical object, 65{\%}; imaging observation, 47{\%}; procedure, 0{\%}; other, 41{\%} ( P{\textless}.0005). Conclusions: Our study shows that despite the large number of terms in RadLex, terms are still absent and complexities in the definitions of terms exist. However, increasing the completeness and refining the definitions in RadLex is easily surmountable, possibly using manual methods. {\textcopyright} 2013 AUR.},
author = {Woods, Ryan W. and Eng, John},
doi = {10.1016/j.acra.2013.08.011},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Woods, Eng - 2013 - Evaluating the completeness of radlex in the chest radiography domain.pdf:pdf},
issn = {10766332},
journal = {Academic Radiology},
keywords = {Chest radiography,Informatics,RadLex},
number = {11},
pages = {1329--1333},
pmid = {24119344},
publisher = {Elsevier Ltd},
title = {{Evaluating the completeness of radlex in the chest radiography domain}},
url = {http://dx.doi.org/10.1016/j.acra.2013.08.011},
volume = {20},
year = {2013}
}
@article{Ananda-Rajah2014a,
abstract = {PURPOSE Prospective surveillance of invasive mold diseases (IMDs) in haematology patients should be standard of care but is hampered by the absence of a reliable laboratory prompt and the difficulty of manual surveillance. We used a high throughput technology, natural language processing (NLP), to develop a classifier based on machine learning techniques to screen computed tomography (CT) reports supportive for IMDs. PATIENTS AND METHODS We conducted a retrospective case-control study of CT reports from the clinical encounter and up to 12-weeks after, from a random subset of 79 of 270 case patients with 33 probable/proven IMDs by international definitions, and 68 of 257 uninfected-control patients identified from 3 tertiary haematology centres. The classifier was trained and tested on a reference standard of 449 physician annotated reports including a development subset (n = 366), from a total of 1880 reports, using 10-fold cross validation, comparing binary and probabilistic predictions to the reference standard to generate sensitivity, specificity and area under the receiver-operating-curve (ROC). RESULTS For the development subset, sensitivity/specificity was 91{\%} (95{\%}CI 86{\%} to 94{\%})/79{\%} (95{\%}CI 71{\%} to 84{\%}) and ROC area was 0.92 (95{\%}CI 89{\%} to 94{\%}). Of 25 (5.6{\%}) missed notifications, only 4 (0.9{\%}) reports were regarded as clinically significant. CONCLUSION CT reports are a readily available and timely resource that may be exploited by NLP to facilitate continuous prospective IMD surveillance with translational benefits beyond surveillance alone.},
author = {Ananda-Rajah, Michelle R and Martinez, David and Slavin, Monica A and Cavedon, Lawrence and Dooley, Michael and Cheng, Allen and Thursky, Karin A},
doi = {10.1371/journal.pone.0107797},
issn = {1932-6203},
journal = {PLOS ONE},
number = {9},
pages = {1--8},
pmid = {25250675},
publisher = {Public Library of Science},
title = {{Facilitating surveillance of pulmonary invasive mold diseases in patients with haematological malignancies by screening computed tomography reports using natural language processing}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25250675 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4175456},
volume = {9},
year = {2014}
}
@article{Zhang2016,
abstract = {Medicinal chemistry patents contain rich information about chemical compounds. Although much effort has been devoted to extracting chemical entities from scientific literature, limited numbers of patent mining systems are publically available, probably due to the lack of large manually annotated corpora. To accelerate the development of information extraction systems for medicinal chemistry patents, the 2015 BioCreative V challenge organized a track on Chemical and Drug Named Entity Recognition from patent text (CHEMDNER patents). This track included three individual subtasks: (i) Chemical Entity Mention Recognition in Patents (CEMP), (ii) Chemical Passage Detection (CPD) and (iii) Gene and Protein Related Object task (GPRO). We participated in the two subtasks of CEMP and CPD using machine learning-based systems. Our machine learning-based systems employed the algorithms of conditional random fields (CRF) and structured support vector machines (SSVMs), respectively. To improve the performance of the NER systems, two strategies were proposed for feature engineering: (i) domain knowledge features of dictionaries, chemical structural patterns and semantic type information present in the context of the candidate chemical and (ii) unsupervised feature learning algorithms to generate word representation features by Brown clustering and a novel binarized Word embedding to enhance the generalizability of the system. Further, the system output for the CPD task was yielded based on the patent titles and abstracts with chemicals recognized in the CEMP task.The effects of the proposed feature strategies on both the machine learning-based systems were investigated. Our best system achieved the second best performance among 21 participating teams in CEMP with a precision of 87.18{\%}, a recall of 90.78{\%} and aF-measure of 88.94{\%} and was the top performing system among nine participating teams in CPD with a sensitivity of 98.60{\%}, a specificity of 87.21{\%}, an accuracy of 94.75{\%}, a Matthew's correlation coefficient (MCC) of 88.24{\%}, a precision at full recall (P{\_}full{\_}R) of 66.57{\%} and an area under the precision-recall curve (AUC{\_}PR) of 0.9347. The SSVM-based CEMP systems outperformed the CRF-based CEMP systems when using the same features. Features generated from both the domain knowledge and unsupervised learning algorithms significantly improved the chemical NER task on patents.Database URL:http://database.oxfordjournals.org/content/2016/baw049.},
author = {Zhang, Yaoyun and Xu, J and Chen, Hui and Wang, Jingqi and Wu, Yonghui and Prakasam, Manu and Xu, Hua},
doi = {10.1093/database/baw049},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2016 - Chemical named entity recognition in patents by domain knowledge and unsupervised feature learning.pdf:pdf},
isbn = {17580463 (Electronic)},
issn = {17580463},
journal = {Database (Oxford)},
keywords = {Biomedical Informatics University,CEMP systems,CEMP task,CPD task,Capital Medical University,Center Houston Houston,Chemical Entity Mention,China School Biomedical,Entity Recognition patent,Health Science Center,High School Sacramento,Houston Houston TX,Informatics University Texas,NER task patents,Oxford School Biomedical,Passage Detection CPD,Recognition Patents CEMP,Related Object task,School Biomedical Engineering,School Biomedical Informatics,Science Center Houston,Texas Health Science,USA School Biomedical,USA hua xu,University Texas Health,area precision recall,chemical compounds effort,chemical structural patterns,chemistry patents BioCreative,conditional random fields,context candidate chemical,correlation coefficient MCC,effects proposed feature,entity recognition patents,http database oxfordjournals,information extraction systems,knowledge features dictionaries,knowledge unsupervised feature,medicinal chemistry patents,patent mining systems,patents rich information,performance NER systems,semantic type information,subtasks CEMP CPD,support vector machines,teams CEMP precision,teams CPD sensitivity,text CHEMDNER patents,track Chemical Drug,uth tmc edu},
month = {apr},
pages = {1--10},
pmid = {27087307},
publisher = {Oxford University Press},
title = {{Chemical named entity recognition in patents by domain knowledge and unsupervised feature learning}},
url = {https://academic.oup.com/database/article-lookup/doi/10.1093/database/baw049 http://www.ncbi.nlm.nih.gov/pubmed/27087307},
volume = {2016},
year = {2016}
}
@article{Wang2014,
abstract = {This paper explores a number of simple and effective techniques to adapt statisti-cal machine translation (SMT) systems in the medical domain. Comparative exper-iments are conducted on large corpora for six language pairs. We not only compare each adapted system with the baseline, but also combine them to further improve the domain-specific systems. Finally, we attend the WMT2014 medical summary sentence translation constrained task and our systems achieve the best BLEU scores for Czech-English, English-German, French-English language pairs and the second best BLEU scores for re-minding pairs.},
author = {Wang, Longyue and Lu, Yi and Wong, Derek F and Chao, Lidia S and Wang, Yiming and Oliveira, Francisco and Processing, Natural Language and Science, Information},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2014 - Combining Domain Adaptation Approaches for Medical Text Transla- tion.pdf:pdf},
journal = {Acl2014},
pages = {254--259},
title = {{Combining Domain Adaptation Approaches for Medical Text Transla- tion}},
year = {2014}
}
@article{Langlotz2000,
abstract = {We have implemented a structured reporting system for medical imaging that replaces dictation and transcription by allowing radiologists and other imaging professionals to select imaging findings from medical lexicons. The system uses an imaging-specific information model called a Description Set to organize selected terms in a relational database. The system's expressiveness for reporting is enhanced by its ability to codify uncertainty about imaging observations and to represent explicit causal and associational relationships among imaging findings. The system promptly and automatically generates a text report that referring physicians are accustomed to receiving. Because the image report information is stored in a fully coded fashion, it can be used to provide real-time decision support to radiologists, to transmit coded imaging data to electronic patient record systems, to measure and improve radiologists' performance, and to index images by content.},
author = {Langlotz, C P and Meininger, L},
isbn = {1531-605X},
issn = {1531-605X},
journal = {Proceedings / AMIA ... Annual Symposium. AMIA Symposium},
pages = {467--471},
pmid = {11079927},
publisher = {American Medical Informatics Association},
title = {{Enhancing the expressiveness and usability of structured image reporting systems.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/11079927 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC2243902 http://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed{\&}id=11079927{\&}retmode=ref{\&}cmd=prlinks{\%}5Cnpapers2://publication/uuid/79E24F},
year = {2000}
}
@article{Khare2015,
abstract = {Motivated by the high cost of human curation of biological databases, there is an increasing interest in using computational approaches to assist human curators and accelerate the manual curation process. Towards the goal of cataloging drug indications from FDA drug labels, we recently developed LabeledIn, a human-curated drug indication resource for 250 clinical drugs. Its development required over 40 h of human effort across 20 weeks, despite using well-defined annotation guidelines. In this study, we aim to investigate the feasibility of scaling drug indication annotation through a crowdsourcing technique where an unknown network of workers can be recruited through the technical environment of Amazon Mechanical Turk (MTurk). To translate the expert-curation task of cataloging indications into human intelligence tasks (HITs) suitable for the average workers on MTurk, we first simplify the complex task such that each HIT only involves a worker making a binary judgment of whether a highlighted disease, in context of a given drug label, is an indication. In addition, this study is novel in the crowdsourcing interface design where the annotation guidelines are encoded into user options. For evaluation, we assess the ability of our proposed method to achieve high-quality annotations in a time-efficient and cost-effective manner. We posted over 3000 HITs drawn from 706 drug labels on MTurk. Within 8 h of posting, we collected 18 775 judgments from 74 workers, and achieved an aggregated accuracy of 96{\%} on 450 control HITs (where gold-standard answers are known), at a cost of {\$}1.75 per drug label. On the basis of these results, we conclude that our crowdsourcing approach not only results in significant cost and time saving, but also leads to accuracy comparable to that of domain experts.},
author = {Khare, Ritu and Burger, John D and Aberdeen, John S and Tresner-Kirsch, David W and Corrales, Theodore J and Hirchman, Lynette and Lu, Zhiyong},
doi = {10.1093/database/bav016},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Khare et al. - 2015 - Scaling drug indication curation through crowdsourcing.pdf:pdf},
isbn = {17580463 (Electronic)},
issn = {17580463},
journal = {Database (Oxford)},
pmid = {25797061},
publisher = {Oxford University Press},
title = {{Scaling drug indication curation through crowdsourcing}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25797061 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4369375},
volume = {2015},
year = {2015}
}
@article{Hamon2016,
abstract = {An increasing availability of parallel bilingual corpora and of automatic methods and tools makes it possible to build linguistic and terminological resources for low-resourced languages. We propose to exploit corpora available in several languages for building bilingual and trilingual terminologies. Typically, terminology information extracted in better resourced languages is associated with the corresponding units in lower-resourced languages thanks to the multilingual transfer. The method is applied on corpora involving Ukrainian language. According to the experiments, precision of term extraction varies between 0.454 and 0.966, while the quality of the interlingual relations varies between 0.309 and 0.965. The resource built contains 4,588 medical terms in Ukrainian and their 34,267 relations with French and English terms.},
author = {Hamon, Thierry and Grabar, Natalia},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hamon, Grabar - 2016 - Adaptation of Cross-Lingual Transfer Methods for the Building of Medical Terminology in Ukrainian.pdf:pdf},
journal = {CICLING 2016, Apr 2016, Konya, Turkey},
keywords = {Cross-Lingual Transfer,Parallel Corpora,Terminology,Ukrainian},
title = {{Adaptation of Cross-Lingual Transfer Methods for the Building of Medical Terminology in Ukrainian}},
url = {https://hal.archives-ouvertes.fr/hal-01426807/},
year = {2016}
}
@inproceedings{Espinoza2009,
abstract = {This demo proposal aims at providing support for the local- ization of ontologies, and as a result at obtaining multilingual ontologies. We briey present an advanced version of LabelTranslator, our system to localize ontology terms in dierent natural languages. The current ver- sion of the system diers from previous works reported in 1, 2 in that it relies on a modular approach to store the linguistic information asso- ciated to ontology terms. Additionally, it uses a synchronization method to maintain the conceptual and linguistic information updated.},
author = {Espinoza, Mauricio and G{\'{o}}mez-P{\'{e}}rez, Asunci{\'{o}}n and Montiel-Ponsoda, Elena},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-02121-3_63},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Espinoza, G{\'{o}}mez-P{\'{e}}rez, Montiel-Ponsoda - 2009 - Multilingual and localization support for ontologies.pdf:pdf},
isbn = {3642021204},
issn = {03029743},
keywords = {Multilingual ontologies,Ontology localization},
pages = {821--825},
publisher = {Springer Berlin Heidelberg},
title = {{Multilingual and localization support for ontologies}},
url = {http://link.springer.com/10.1007/978-3-642-02121-3{\_}63},
volume = {5554 LNCS},
year = {2009}
}
@article{Yetisgen-Yildiz2013,
abstract = {Communication of follow-up recommendations when abnormalities are identified on imaging studies is prone to error. The absence of an automated system to identify and track radiology recommendations is an important barrier to ensuring timely follow-up of patients especially with non-acute incidental findings on imaging examinations. In this paper, we present a text processing pipeline to automatically identify clinically important recommendation sentences in radiology reports. Our extraction pipeline is based on natural language processing (NLP) and supervised text classification methods. To develop and test the pipeline, we created a corpus of 800 radiology reports double annotated for recommendation sentences by a radiologist and an internist. We ran several experiments to measure the impact of different feature types and the data imbalance between positive and negative recommendation sentences. Our fully statistical approach achieved the best f-score 0.758 in identifying the critical recommendation sentences in radiology reports.},
author = {Yetisgen-Yildiz, Meliha and Gunn, Martin L. and Xia, Fei and Payne, Thomas H.},
doi = {10.1016/j.jbi.2012.12.005},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yetisgen-Yildiz et al. - 2013 - A text processing pipeline to extract recommendations from radiology reports.pdf:pdf},
issn = {15320464},
journal = {Journal of Biomedical Informatics},
number = {2},
pages = {354--362},
title = {{A text processing pipeline to extract recommendations from radiology reports}},
volume = {46},
year = {2013}
}
@article{Patil2014,
abstract = {Communication is the cornerstone of medicine, without which we cannot interact with our patients.1 The General Medical Council's Good Medical Practice states that “Doctors must listen to patients, take account of their views, and respond honestly to their questions.”2 However, we still often interact with patients who do not speak the local language.$\backslash$r$\backslash$n$\backslash$r$\backslash$nIn the United Kingdom most hospitals have access to translation services, but they are expensive and often cumbersome. A complex and nuanced medical, ethical, and treatment discussion with patients whose knowledge of the local language is inadequate remains challenging. Indeed, even in a native language there is an element of translation from medical to lay terminology.$\backslash$r$\backslash$n$\backslash$r$\backslash$nWe recently treated a very sick child in our paediatric intensive care unit. The parents did not speak English, and there were no human translators available. Reluctantly we resorted to a web based translation tool. We were uncertain whether Google Translate was accurately translating our complex medical phrases.3 4 Fortunately our patient recovered, and a human translator later reassured us that we had conveyed information accurately.$\backslash$r$\backslash$n$\backslash$r$\backslash$nWe aimed to evaluate the accuracy and usefulness of Google Translate in translating common English medical statements.},
author = {Patil, Sumant and Davies, Patrick},
doi = {10.1136/bmj.g7392},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Patil, Davies - 2014 - Use of Google Translate in medical communication evaluation of accuracy.pdf:pdf},
isbn = {1756-1833 (Electronic)$\backslash$r0959-535X (Linking)},
issn = {1756-1833},
journal = {The BMJ},
keywords = {Communication,Search Engine,Search Engine: standards,Sensitivity and Specificity,Terminology as Topic,Translating},
number = {December},
pmid = {25512386},
title = {{Use of Google Translate in medical communication: evaluation of accuracy}},
url = {10.1136/bmj.g7392{\%}5Cnhttp://search.ebscohost.com/login.aspx?direct=true{\&}db=a9h{\&}AN=100089083{\&}site=ehost-live http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=4266233{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {349},
year = {2014}
}
@article{Yepes2013,
abstract = {BACKGROUND: Most of the institutional and research information in the biomedical domain is available in the form of English text. Even in countries where English is an official language, such as the United States, language can be a barrier for accessing biomedical information for non-native speakers. Recent progress in machine translation suggests that this technique could help make English texts accessible to speakers of other languages. However, the lack of adequate specialized corpora needed to train statistical models currently limits the quality of automatic translations in the biomedical domain.$\backslash$n$\backslash$nRESULTS: We show how a large-sized parallel corpus can automatically be obtained for the biomedical domain, using the MEDLINE database. The corpus generated in this work comprises article titles obtained from MEDLINE and abstract text automatically retrieved from journal websites, which substantially extends the corpora used in previous work. After assessing the quality of the corpus for two language pairs (English/French and English/Spanish) we use the Moses package to train a statistical machine translation model that outperforms previous models for automatic translation of biomedical text.$\backslash$n$\backslash$nCONCLUSIONS: We have built translation data sets in the biomedical domain that can easily be extended to other languages available in MEDLINE. These sets can successfully be applied to train statistical machine translation models. While further progress should be made by incorporating out-of-domain corpora and domain-specific lexicons, we believe that this work improves the automatic translation of biomedical texts.},
author = {Yepes, Antonio Jimeno and Prieur-Gaston, Elise and N{\'{e}}v{\'{e}}ol, Aur{\'{e}}lie},
doi = {10.1186/1471-2105-14-146},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yepes, Prieur-Gaston, N{\'{e}}v{\'{e}}ol - 2013 - Combining MEDLINE and publisher data to create parallel corpora for the automatic translation of.pdf:pdf},
isbn = {1471-2105 (Linking)},
issn = {1471-2105},
journal = {BMC bioinformatics},
keywords = {Linguistics,Linguistics: methods,MEDLINE,Models,Publishing,Statistical,Translating},
number = {1},
pages = {146},
pmid = {23631733},
publisher = {BioMed Central},
title = {{Combining MEDLINE and publisher data to create parallel corpora for the automatic translation of biomedical text.}},
url = {http://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-146 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3651320{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {14},
year = {2013}
}
@inproceedings{Stewart,
abstract = {Metamap and Mgrep are natural language processing tools for map-ping medical free text to formal medical lexicons, but an indepth comparison of the programs and their application to social media data has never been pur-sued. This project is interested in comparing the programs, in order to determine which program is most appropriate for mapping web 2.0 communication data. The archives of the Pediatric Pain Mailing List (PPML) were mapped with both programs, and each term returned was checked for correctness. The analysis re-sulted in Mgrep having a significantly higher precision (76.1{\%} to 58.8{\%}, differ-ence of 18{\%}, p-value {\textless} 0.0001) while Metamap returned more terms: 2381 to 1350. When considering only perfect or multiple matches, Mgrep still had bet-ter precision (81.2{\%} to 71.3{\%}, difference 10{\%}, p-value {\textless} 0.0001). Ultimately Mgrep's precision may make it the better choice for many applications, but when there is more value in number of correct terms returned over accuracy of those terms, Metamap's larger set and superior scoring function may make it the tool of choice.},
author = {Stewart, Samuel Alan and {Von Maltzahn}, Maia Elizabeth and Abidi, Syed Sibte Raza},
booktitle = {CEUR Workshop Proceedings},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Stewart, Von Maltzahn, Abidi - 2012 - Comparing metamap to mgrep as a tool for mapping free text to formal medical lexicons.pdf:pdf},
issn = {16130073},
keywords = {Knowledge management,Knowledge translation,Mesh,Natural language processing,Semantic mapping,Umls},
pages = {63--77},
title = {{Comparing metamap to mgrep as a tool for mapping free text to formal medical lexicons}},
url = {http://www.cs.dal.ca},
volume = {895},
year = {2012}
}
@article{Elhadad2015,
abstract = {We describe two tasks—named entity recog-nition (Task 1) and template slot filling (Task 2)—for clinical texts. The tasks leverage an-notations from the ShARe corpus, which con-sists of clinical notes with annotated men-tions disorders, along with their normaliza-tion to a medical terminology and eight addi-tional attributes. The purpose of these tasks was to identify advances in clinical named en-tity recognition and establish the state of the art in disorder template slot filling. Task 2 consisted of two subtasks: template slot fill-ing given gold-standard disorder spans (Task 2a) and end-to-end disorder span identifica-tion together with template slot filling (Task 2b). For Task 1 (disorder span detection and normalization), 16 teams participated. The best system yielded a strict F1-score of 75.7, with a precision of 78.3 and recall of 73.2. For Task 2a (template slot filling given gold-standard disorder spans), six teams partici-pated. The best system yielded a combined overall weighted accuracy for slot filling of 88.6. For Task 2b (disorder recognition and template slot filling), nine teams participated. The best system yielded a combined relaxed F (for span detection) and overall weighted ac-curacy of 80.8.},
author = {Elhadad, No{\'{e}}mie and Pradhan, Sameer and Gorman, Sharon Lipsky and Manandhar, Suresh and Chapman, Wendy W. and Savova, Guergana},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Elhadad et al. - 2015 - SemEval-2015 Task 14 Analysis of Clinical Text.pdf:pdf},
journal = {Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015)},
number = {SemEval},
pages = {303--310},
title = {{SemEval-2015 Task 14 : Analysis of Clinical Text}},
url = {http://www.aclweb.org/anthology/S15-2051},
year = {2015}
}
@article{Lacson2012,
abstract = {Radiology reports are permanent legal documents that serve as official interpretation of imaging tests. Manual analysis of textual information contained in these reports requires significant time and effort. This study describes the development and initial evaluation of a toolkit that enables automated identification of relevant information from within these largely unstructured text reports. We developed and made publicly available a natural language processing toolkit, Information from Searching Content with an Ontology-Utilizing Toolkit (iSCOUT). Core functions are included in the following modules: the Data Loader, Header Extractor, Terminology Interface, Reviewer, and Analyzer. The toolkit enables search for specific terms and retrieval of (radiology) reports containing exact term matches as well as similar or synonymous term matches within the text of the report. The Terminology Interface is the main component of the toolkit. It allows query expansion based on synonyms from a controlled terminology (e.g., RadLex or National Cancer Institute Thesaurus [NCIT]). We evaluated iSCOUT document retrieval of radiology reports that contained liver cysts, and compared precision and recall with and without using NCIT synonyms for query expansion. iSCOUT retrieved radiology reports with documented liver cysts with a precision of 0.92 and recall of 0.96, utilizing NCIT. This recall (i.e., utilizing the Terminology Interface) is significantly better than using each of two search terms alone (0.72, p=0.03 for liver cyst and 0.52, p=0.0002 for hepatic cyst). iSCOUT reliably assembled relevant radiology reports for a cohort of patients with liver cysts with significant improvement in document retrieval when utilizing controlled lexicons.},
author = {Lacson, Ronilda and Andriole, Katherine P and Prevedello, Luciano M and Khorasani, Ramin},
doi = {10.1007/s10278-012-9463-9},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lacson et al. - 2012 - Information from searching content with an ontology-utilizing toolkit (ISCOUT).pdf:pdf},
isbn = {1618-727X (Electronic)$\backslash$r0897-1889 (Linking)},
issn = {08971889},
journal = {Journal of Digital Imaging},
keywords = {Controlled vocabulary,Information storage and retrieval,Natural language processing},
month = {aug},
number = {4},
pages = {512--519},
pmid = {22349993},
publisher = {Springer},
title = {{Information from searching content with an ontology-utilizing toolkit (ISCOUT)}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22349993 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3389089},
volume = {25},
year = {2012}
}
@article{Bojar2014,
abstract = {This paper presents the results of the WMT12 shared tasks, which included a translation task, a task for machine translation evaluation metrics, and a task for run-time estimation of machine translation quality. We conducted a large-scale manual evaluation of 103 machine translation systems submitted by 34 teams. We used the ranking of these systems to mea- sure how strongly automatic metrics correlate with human judgments of translation quality for 12 evaluation metrics. We introduced a new quality estimation task this year, and eval- uated submissions from 11 teams.},
author = {Bojar, Ondrej and Buck, Christian and Federmann, Christian and Haddow, Barry and Koehn, Philipp and Leveling, Johannes and Monz, Christof and Pecina, Pavel and Post, Matt and Herve, Saint-Amand and Soricut, Radu and Specia, Lucia and Tamchyna, Ales},
doi = {10.3115/1626431.1626433},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bojar et al. - 2014 - Findings of the 2014 Workshop on Statistical Machine Translation.pdf:pdf},
isbn = {978-1-937284-12-1},
journal = {2014 Workshop on Statistical Machine Translation},
keywords = {Natural Language Processing},
pages = {12--58},
title = {{Findings of the 2014 Workshop on Statistical Machine Translation}},
url = {http://eprints.pascal-network.org/archive/00009584/},
year = {2014}
}
@article{Wu2011a,
abstract = {Radiology reports contain information that can be mined using a search engine for teaching, research, and quality assurance purposes. Current search engines look for exact matches to the search term, but they do not differentiate between reports in which the search term appears in a positive context (i.e., being present) from those in which the search term appears in the context of negation and uncertainty. We describe RadReportMiner, a context-aware search engine, and compare its retrieval performance with a generic search engine, Google Desktop. We created a corpus of 464 radiology reports which described at least one of five findings (appendicitis, hydronephrosis, fracture, optic neuritis, and pneumonia). Each report was classified by a radiologist as positive (finding described to be present) or negative (finding described to be absent or uncertain). The same reports were then classified by RadReportMiner and Google Desktop. RadReportMiner achieved a higher precision (81{\%}), compared with Google Desktop (27{\%}; p {\textless} 0.0001). RadReportMiner had a lower recall (72{\%}) compared with Google Desktop (87{\%}; p = 0.006). We conclude that adding negation and uncertainty identification to a word-based radiology report search engine improves the precision of search results over a search engine that does not take this information into account. Our approach may be useful to adopt into current report retrieval systems to help radiologists to more accurately search for radiology reports.},
author = {Wu, Andrew S and Do, Bao H and Kim, Jinsuh and Rubin, Daniel L},
doi = {10.1007/s10278-009-9250-4},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu et al. - 2011 - Evaluation of negation and uncertainty detection and its impact on precision and recall in search.pdf:pdf},
isbn = {0897-1889},
issn = {08971889},
journal = {Journal of Digital Imaging},
keywords = {Data extraction,Data mining,Databases,Efficiency,Natural language processing,Negation,Reporting},
month = {apr},
number = {2},
pages = {234--242},
pmid = {19902298},
publisher = {Springer},
title = {{Evaluation of negation and uncertainty detection and its impact on precision and recall in search}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19902298 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3056979},
volume = {24},
year = {2011}
}
@article{Kaliyadan2010,
author = {Kaliyadan, Feroze and Pillai, Sreekanth Gopinathan},
doi = {10.14236/jhi.v18i2.764},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kaliyadan, Pillai - 2010 - The use of Google language tools as an interpretation aid in cross-cultural doctor-patient interaction A pilo.pdf:pdf},
isbn = {1476-0320 (Print) 1475-9985},
issn = {14760320},
journal = {Informatics in Primary Care},
keywords = {Cross-cultural doctor-patient interaction,Google language tools,Medical interpretation},
month = {jun},
number = {2},
pages = {141--143},
pmid = {21078237},
title = {{The use of Google language tools as an interpretation aid in cross-cultural doctor-patient interaction: A pilot study}},
url = {http://hijournal.bcs.org/index.php/jhi/article/view/764},
volume = {18},
year = {2010}
}
@article{Aires2016,
abstract = {Our approach to produce translations for the ACL-2016 Biomedical Translation Task on the English-Portuguese language pair, in both directions, is described. Own preliminary tests results and final results, measured by the shared task organizers, are also presented.},
author = {Aires, Jos{\'{e}} and Lopes, Gabriel Pereira and Gomes, Lu{\'{i}}s},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Aires, Lopes, Gomes - 2016 - English-Portuguese Biomedical Translation Task Using a Genuine Phrase-Based Statistical Machine Translation.pdf:pdf},
journal = {Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers},
pages = {456--462},
title = {{English-Portuguese Biomedical Translation Task Using a Genuine Phrase-Based Statistical Machine Translation Approach}},
volume = {2},
year = {2016}
}
@article{Jacobs2004,
abstract = {OBJECTIVES: We assessed the impact of interpreter services on the cost and the utilization of health care services among patients with limited English proficiency.$\backslash$n$\backslash$nMETHODS: We measured the change in delivery and cost of care provided to patients enrolled in a health maintenance organization before and after interpreter services were implemented.$\backslash$n$\backslash$nRESULTS: Compared with English-speaking patients, patients who used the interpreter services received significantly more recommended preventive services, made more office visits, and had more prescriptions written and filled. The estimated cost of providing interpreter services was {\$}279 per person per year.\backslashn\backslashnCONCLUSIONS: Providing interpreter services is a financially viable method for enhancing delivery of health care to patients with limited English proficiency.},
author = {Jacobs, Elizabeth A and Shepard, Donald S and Suaya, Jose A and Stone, Esta-Lee},
doi = {10.2105/AJPH.94.5.866},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jacobs et al. - 2004 - Overcoming language barriers in health care costs and benefits of interpreter services.pdf:pdf},
isbn = {0090-0036 (Print)$\backslash$r0090-0036 (Linking)},
issn = {0090-0036},
journal = {American journal of public health},
keywords = {Adult,Communication Barriers,Female,Health Maintenance Organizations,Health Maintenance Organizations: economics,Health Maintenance Organizations: utilization,Hispanic Americans,Humans,Language,Male,Massachusetts,Middle Aged,Quality of Health Care,Translating},
number = {5},
pages = {866--9},
pmid = {15117713},
title = {{Overcoming language barriers in health care: costs and benefits of interpreter services.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1448350{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {94},
year = {2004}
}
@article{Alex2007,
abstract = {Although recent named entity (NE) annotation efforts involve the markup of nested entities, there has been limited focus on recognising such nested structures. This paper introduces and compares three techniques for modelling and recognising nested entities by means of a conventional sequence tagger. The methods are tested and evaluated on two biomedical data sets that contain entity nesting. All methods yield an improvement over the baseline tagger that is only trained on flat annotation.},
author = {Alex, Beatrice and Haddow, Barry and Grover, Claire},
doi = {10.3115/1572392.1572404},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Alex, Haddow, Grover - 2007 - Recognising nested named entities in biomedical text.pdf:pdf},
journal = {Proceedings of the Workshop on BioNLP 2007 {\ldots}},
number = {June},
pages = {65--72},
title = {{Recognising nested named entities in biomedical text}},
url = {http://dl.acm.org/citation.cfm?id=1572404},
year = {2007}
}
@article{Zhang2013,
abstract = {Named entity recognition is a crucial component of biomedical natural language processing, enabling information extraction and ultimately reasoning over and knowledge discovery from text. Much progress has been made in the design of rule-based and supervised tools, but they are often genre and task dependent. As such, adapting them to different genres of text or identifying new types of entities requires major effort in re-annotation or rule development. In this paper, we propose an unsupervised approach to extracting named entities from biomedical text. We describe a stepwise solution to tackle the challenges of entity boundary detection and entity type classification without relying on any handcrafted rules, heuristics, or annotated data. A noun phrase chunker followed by a filter based on inverse document frequency extracts candidate entities from free text. Classification of candidate entities into categories of interest is carried out by leveraging principles from distributional semantics. Experiments show that our system, especially the entity classification step, yields competitive results on two popular biomedical datasets of clinical notes and biological literature, and outperforms a baseline dictionary match approach. Detailed error analysis provides a road map for future work. {\textcopyright} 2013 The Authors.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Zhang, Shaodian and Elhadad, No{\'{e}}mie},
doi = {10.1016/j.jbi.2013.08.004},
eprint = {NIHMS150003},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Elhadad - 2013 - Unsupervised biomedical named entity recognition Experiments with clinical and biological texts.pdf:pdf},
isbn = {2122633255},
issn = {15320464},
journal = {Journal of Biomedical Informatics},
keywords = {Chunking,Distributional semantics,Named entity recognition,Natural language processing,UMLS},
number = {6},
pages = {1088--1098},
pmid = {23954592},
title = {{Unsupervised biomedical named entity recognition: Experiments with clinical and biological texts}},
volume = {46},
year = {2013}
}
@article{Tseytlin2016,
abstract = {BACKGROUND: Natural language processing (NLP) applications are increasingly important in biomedical data analysis, knowledge engineering, and decision support. Concept recognition is an important component task for NLP pipelines, and can be either general-purpose or domain-specific. We describe a novel, flexible, and general-purpose concept recognition component for NLP pipelines, and compare its speed and accuracy against five commonly used alternatives on both a biological and clinical corpus. NOBLE Coder implements a general algorithm for matching terms to concepts from an arbitrary vocabulary set. The system's matching options can be configured individually or in combination to yield specific system behavior for a variety of NLP tasks. The software is open source, freely available, and easily integrated into UIMA or GATE. We benchmarked speed and accuracy of the system against the CRAFT and ShARe corpora as reference standards and compared it to MMTx, MGrep, Concept Mapper, cTAKES Dictionary Lookup Annotator, and cTAKES Fast Dictionary Lookup Annotator.$\backslash$n$\backslash$nRESULTS: We describe key advantages of the NOBLE Coder system and associated tools, including its greedy algorithm, configurable matching strategies, and multiple terminology input formats. These features provide unique functionality when compared with existing alternatives, including state-of-the-art systems. On two benchmarking tasks, NOBLE's performance exceeded commonly used alternatives, performing almost as well as the most advanced systems. Error analysis revealed differences in error profiles among systems.$\backslash$n$\backslash$nCONCLUSION: NOBLE Coder is comparable to other widely used concept recognition systems in terms of accuracy and speed. Advantages of NOBLE Coder include its interactive terminology builder tool, ease of configuration, and adaptability to various domains and tasks. NOBLE provides a term-to-concept matching system suitable for general concept recognition in biomedical NLP pipelines.},
author = {Tseytlin, Eugene and Mitchell, Kevin and Legowski, Elizabeth and Corrigan, Julia and Chavan, Girish and Jacobson, Rebecca S},
doi = {10.1186/s12859-015-0871-y},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cohen et al. - 2008 - Getting Started in Text Mining.pdf:pdf},
isbn = {14712105 (Electronic)},
issn = {1471-2105},
journal = {BMC bioinformatics},
keywords = {Algorithms,Bioinformatics,Combinatorial Libraries,Computational Biology/Bioinformatics,Computer Appl. in Life Sciences,Microarrays},
month = {jan},
number = {32},
pmid = {26763894},
publisher = {BioMed Central},
title = {{NOBLE - Flexible concept recognition for large-scale biomedical natural language processing.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26763894 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4712516 http://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-015-0871-y},
volume = {17},
year = {2016}
}
@inproceedings{Conference2012,
annote = {Used Google Translate:},
author = {{Marta R. Costa-juss{\`{a}}, Mireia Farr{\'{u}}s}, Jordi Serrano Pons},
booktitle = {ARSA - Proceedings in ARSA - Advanced Research in Scientific Areas},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Marta R. Costa-juss{\`{a}}, Mireia Farr{\'{u}}s - 2012 - Machine Translation in Medicine.pdf:pdf},
isbn = {9788055406060},
keywords = {- medical communication tools,communication,machine translation evaluation,machine translation has become,medical translation,patient-doctor,quite popular,recently,statistical machine,translation},
pages = {1995--1998},
publisher = {EDIS - Publishing Institution of the University of Zilina},
title = {{Machine Translation in Medicine}},
url = {http://www.arsa-conf.com},
year = {2012}
}
@article{Li2016,
abstract = {Relations between chemicals and diseases are one of the most queried biomedical interactions. Although expert manual curation is the standard method for extracting these relations from the literature, it is expensive and impractical to apply to large numbers of documents, and therefore alternative methods are required. We describe here a crowdsourcing workflow for extracting chemical-induced disease relations from free text as part of the BioCreative V Chemical Disease Relation challenge. Five non-expert workers on the CrowdFlower platform were shown each potential chemical-induced disease relation highlighted in the original source text and asked to make binary judgments about whether the text supported the relation. Worker responses were aggregated through voting, and relations receiving four or more votes were predicted as true. On the official evaluation dataset of 500 PubMed abstracts, the crowd attained a 0.505F-score (0.475 precision, 0.540 recall), with a maximum theoretical recall of 0.751 due to errors with named entity recognition. The total crowdsourcing cost was {\$}1290.67 ({\$}2.58 per abstract) and took a total of 7 h. A qualitative error analysis revealed that 46.66{\%} of sampled errors were due to task limitations and gold standard errors, indicating that performance can still be improved. All code and results are publicly available athttps://github.com/SuLab/crowd{\_}cid{\_}relexDatabase URL:https://github.com/SuLab/crowd{\_}cid{\_}relex.},
author = {Li, Tong Shu and Bravo, {\`{A}}lex and Furlong, Laura I. and Good, Benjamin M. and Su, Andrew I.},
doi = {10.1093/database/baw051},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2016 - A crowdsourcing workflow for extracting chemical-induced disease relations from free text.pdf:pdf},
isbn = {17580463 (Electronic)},
issn = {17580463},
journal = {Database (Oxford)},
month = {apr},
pages = {baw051},
pmid = {27087308},
publisher = {Oxford University Press},
title = {{A crowdsourcing workflow for extracting chemical-induced disease relations from free text}},
url = {https://academic.oup.com/database/article-lookup/doi/10.1093/database/baw051},
volume = {2016},
year = {2016}
}
@article{Jensen2012,
abstract = {Clinical data describing the phenotypes and treatment of patients represents an underused data source that has much greater research potential than is currently realized. Mining of electronic health records (EHRs) has the potential for establishing new patient-stratification principles and for revealing unknown disease correlations. Integrating EHR data with genetic data will also give a finer understanding of genotype-phenotype relationships. However, a broad range of ethical, legal and technical reasons currently hinder the systematic deposition of these data in EHRs and their mining. Here, we consider the potential for furthering medical research and clinical care using EHR data and the challenges that must be overcome before this is a reality.},
author = {Jensen, P. B. and Jensen, L. J. and Brunak, S.},
doi = {10.1038/nrg3208},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jensen, Jensen, Brunak - 2012 - Mining electronic health records towards better research applications and clinical care.pdf:pdf},
isbn = {1471-0064 (Electronic)$\backslash$r1471-0056 (Linking)},
issn = {1471-0064},
journal = {Nature reviews. Genetics},
keywords = {Biomedical Research,Biomedical Research: methods,Biomedical Research: standards,Computerized,Computerized: statistics,Computerized: utilization,Electronic Health Records,Electronic Health Records: statistics {\&} numerical,Electronic Health Records: utilization,Genetic Association Studies,Genetic Association Studies: methods,Humans,Information Dissemination,Information Dissemination: methods,Medical Records Systems,Patient Care,Patient Care: methods,Patient Care: standards,Primary Health Care,Primary Health Care: methods,Primary Health Care: standards,Public Health,Public Health: methods,Public Health: standards},
number = {6},
pages = {395--405},
pmid = {22549152},
publisher = {Nature Publishing Group},
title = {{Mining electronic health records: towards better research applications and clinical care.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22549152},
volume = {13},
year = {2012}
}
@article{Wu,
abstract = {Named entities (NEs) are the expressions in human languages that explicitly link notations in languages to the entities in the real world. They play important role in cross-language information retrieval (CLIR) because most user requests have been found to have NEs, and majority of out-of-vocabulary terms are NEs. Therefore, missing their translations has a significant impact to the retrieval effectiveness. In this paper, we examine the effect of high quality translations of NEs in event driven information exploration, where the existence of NEs is even more common. With the focus on the effect of NE translations obtained by using information extraction (IE) techniques, we conducted several experiments using TDT test collections. Our results demonstrate that NEs and their translations play critical roles in improving CLIR effectiveness, and it makes positive impact in CLIR to use high quality translations of NEs obtained by IE techniques.},
author = {Wu, Dan and He, Daqing and Ji, Heng and Grishman, Ralph},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu et al. - Unknown - The Effects of High Quality Translations of Named Entities in Cross-Language Information Exploration.pdf:pdf},
title = {{The Effects of High Quality Translations of Named Entities in Cross-Language Information Exploration}}
}
@inproceedings{Khalid,
abstract = {In the named entity normalization task, a system identifies a canonical unambiguous referent for names like Bush or Alabama. Resolving synonymy and ambiguity of such names can benefit end-to-end information access tasks. We evaluate two entity normalization methods based on Wikipedia in the context of both passage and document retrieval for question anwering. We find that even a simple normalization method leads to improvements of early precision, both for document and passage retrieval. Moreover, better normalization results in better retrieval performance.},
author = {Khalid, Mahboob Alam and Jijkoun, Valentin and {De Rijke}, Maarten},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-540-78646-7_83},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Khalid, Jijkoun, De Rijke - Unknown - The Impact of Named Entity Normalization on Information Retrieval for Question Answering.pdf:pdf},
isbn = {3540786457},
issn = {03029743},
pages = {705--710},
title = {{The impact of named entity normalization on information retrieval for question answering}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.145.8400{\&}rep=rep1{\&}type=pdf},
volume = {4956 LNCS},
year = {2008}
}
@article{Groza2015,
abstract = {Concept recognition tools rely on the availability of textual corpora to assess their performance and enable the identification of areas for improvement. Typically, corpora are developed for specific purposes, such as gene name recognition. Gene and protein name identification are longstanding goals of biomedical text mining, and therefore a number of different corpora exist. However, phenotypes only recently became an entity of interest for specialized concept recognition systems, and hardly any annotated text is available for performance testing and training. Here, we present a unique corpus, capturing text spans from 228 abstracts manually annotated with Human Phenotype Ontology (HPO) concepts and harmonized by three curators, which can be used as a reference standard for free text annotation of human phenotypes. Furthermore, we developed a test suite for standardized concept recognition error analysis, incorporating 32 different types of test cases corresponding to 2164 HPO concepts. Finally, three established phenotype concept recognizers (NCBO Annotator, OBO Annotator and Bio-LarK CR) were comprehensively evaluated, and results are reported against both the text corpus and the test suites. The gold standard and test suites corpora are available from http://bio-lark.org/hpo{\_}res.html. Database URL: http://bio-lark.org/hpo{\_}res.html.},
author = {Groza, Tudor and K{\"{o}}hler, Sebastian and Doelken, Sandra and Collier, Nigel and Oellrich, Anika and Smedley, Damian and Couto, Francisco M. and Baynam, Gareth and Zankl, Andreas and Robinson, Peter N.},
doi = {10.1093/database/bav005},
isbn = {17580463 (Electronic)},
issn = {17580463},
journal = {Database (Oxford)},
month = {feb},
number = {0},
pmid = {25725061},
publisher = {Oxford University Press},
title = {{Automatic concept recognition using the human phenotype ontology reference and test suite corpora}},
url = {https://academic.oup.com/database/article-lookup/doi/10.1093/database/bav005},
volume = {2015},
year = {2015}
}
@article{Neves,
abstract = {The biomedical scientific literature is a rich source of information not only in the English language, for which it is more abundant, but also in other languages, such as Portuguese, Spanish and French. We present the first freely available parallel corpus of scientific publications for the biomedical domain. Documents from the " Biological Sciences " and " Health Sciences " categories were retrieved from the Scielo database and parallel titles and abstracts are available for the following language pairs: Portuguese/English (about 86,000 documents in total), Spanish/English (about 95,000 documents) and French/English (about 2,000 documents). Additionally, monolingual data was also collected for all four languages. Sentences in the parallel corpus were automatically aligned and a manual analysis of 200 documents by native experts found that a minimum of 79{\%} of sentences were correctly aligned in all language pairs. We demonstrate the utility of the corpus by running baseline machine translation experiments. We show that for all language pairs, a statistical machine translation system trained on the parallel corpora achieves performance that rivals or exceeds the state of the art in the biomedical domain. Furthermore, the corpora are currently being used in the biomedical task in the First Conference on Machine Translation (WMT'16).},
author = {Neves, Mariana and Yepes, Antonio Jimeno and N{\'{e}}v{\'{e}}ol, Aur{\'{e}}lie},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Neves, Yepes, N{\'{e}}v{\'{e}}ol - Unknown - The Scielo Corpus a Parallel Corpus of Scientific Publications for Biomedicine.pdf:pdf},
keywords = {biomedicine,machine translation,parallel corpus},
pages = {2942--2948},
title = {{The Scielo Corpus: a Parallel Corpus of Scientific Publications for Biomedicine}}
}
@article{Zyl2016,
author = {Zyl, Izak Van},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zyl - 2016 - The comprehension of medical words Cross- lingual experiments in French and Xhosa.pdf:pdf},
number = {March 2014},
title = {{The comprehension of medical words : Cross- lingual experiments in French and Xhosa}},
year = {2016}
}
@phdthesis{Castilla2007a,
abstract = {Tese de Doutorado},
address = {S{\~{a}}o Paulo},
author = {Castilla, Coutinho},
doi = {10.11606/T.5.2007.tde-16022009-165641},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Castilla - 2007 - Instrumento de Investiga{\c{c}}{\~{a}}o Cl{\'{i}}nico-Epidemiol{\'{o}}gica em Cardiologia Fundamentado no Processamento de Linguagem Natur.pdf:pdf},
keywords = {Armazenamento e recupera{\c{c}}{\~{a}}o da informa{\c{c}}{\~{a}}o,Automatic data processing,Information storage and retrieval,Information systems,Medical records systems computerized,Natural language processing,Processamento automatizado de dados,Processamento de linguagem natural,Radiografia tor{\'{a}}cica,Radiography thoracic,Radiologia,Radiology,Sistemas computadorizados de registros m{\'{e}}dicos,Sistemas de informa{\c{c}}{\~{a}}o},
month = {sep},
pages = {112},
publisher = {Biblioteca Digital de Teses e Disserta{\c{c}}{\~{o}}es da Universidade de S{\~{a}}o Paulo},
school = {Universidade de S{\~{a}}o Paulo},
title = {{Instrumento de Investiga{\c{c}}{\~{a}}o Cl{\'{i}}nico-Epidemiol{\'{o}}gica em Cardiologia Fundamentado no Processamento de Linguagem Natural}},
url = {http://www.teses.usp.br/teses/disponiveis/5/5131/tde-16022009-165641/},
year = {2007}
}
@inproceedings{Bojar2016,
abstract = {This paper presents the results of the WMT16 shared tasks, which included five machine translation (MT) tasks (standard news, IT-domain, biomedical, multimodal, pronoun), three evaluation tasks (metrics, tuning, run-time estimation of MT qual-ity), and an automatic post-editing task and bilingual document alignment task. This year, 102 MT systems from 24 in-stitutions (plus 36 anonymized online sys-tems) were submitted to the 12 translation directions in the news translation task. The IT-domain task received 31 submissions from 12 institutions in 7 directions and the Biomedical task received 15 submissions systems from 5 institutions. Evaluation was both automatic and manual (relative ranking and 100-point scale assessments). The quality estimation task had three sub-tasks, with a total of 14 teams, submitting 39 entries. The automatic post-editing task had a total of 6 teams, submitting 11 en-tries.},
author = {Bojar, Ond$\backslash$vrej and Chatterjee, Rajen and Federmann, Christian and Graham, Yvette and Haddow, Barry and Huck, Matthias and {Jimeno Yepes}, Antonio and Koehn, Philipp and Logacheva, Varvara and Monz, Christof and Negri, Matteo and Neveol, Aurelie and Neves, Mariana and Popel, Martin and Post, Matt and Rubino, Raphael and Scarton, Carolina and Specia, Lucia and Turchi, Marco and Verspoor, Karin and Zampieri, Marcos},
booktitle = {Proceedings of the First Conference on Machine Translation},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bojar et al. - 2016 - Findings of the 2016 Conference on Machine Translation.pdf:pdf},
pages = {131--198},
title = {{Findings of the 2016 Conference on Machine Translation}},
url = {http://www.aclweb.org/anthology/W/W16/W16-2301},
volume = {2},
year = {2016}
}
@inproceedings{Zeng-Treitler2010,
abstract = {With the development of electronic personal health records, more patients are gaining access to their own medical records. However, comprehension of medical record content remains difficult for many patients. Because each record is unique, it is also prohibitively costly to employ human translators to solve this problem. In this study, we investigated whether multilingual machine translation could help make medical record content more comprehensible to patients who lack proficiency in the language of the records. We used a popular general-purpose machine translation tool called Babel Fish to translate 213 medical record sentences from English into Spanish, Chinese, Russian and Korean. We evaluated the comprehensibility and accuracy of the translation. The text characteristics of the incorrectly translated sentences were also analyzed. In each language, the majority of the translations were incomprehensible (76{\%} to 92{\%}) and/or incorrect (77{\%} to 89{\%}). The main causes of the translation are vocabulary difficulty and syntactical complexity. A general-purpose machine translation tool like the Babel Fish is not adequate for the translation of medical records; however, a machine translation tool can potentially be improved significantly, if it is trained to target certain narrow domains in medicine.},
annote = {Use of Babel Fish to translate medical notes},
author = {Zeng-Treitler, Qing and Kim, Hyeoneui and Rosemblat, Graciela and Keselman, Alla},
booktitle = {Studies in Health Technology and Informatics},
doi = {10.3233/978-1-60750-588-4-73},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zeng-Treitler et al. - 2010 - Can multilingual machine translation help make medical record content more comprehensible to patients.pdf:pdf},
isbn = {9781607505877},
issn = {09269630},
keywords = {Comprehension,Consumer health information,Medical records,Translating},
pages = {73--77},
pmid = {20841653},
title = {{Can multilingual machine translation help make medical record content more comprehensible to patients?}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20841653},
volume = {160},
year = {2010}
}
@article{Dang2009,
abstract = {Radiology departments are a rich source of information in the form of digital radiology reports and images obtained in patients with a wide spectrum of clinical conditions. A free text radiology report and image search application known as Render was created to allow users to find pertinent cases for a variety of purposes. Render is a radiology report and image repository that pools researchable information derived from multiple systems in near real time with use of (a) Health Level 7 links for radiology information system data, (b) periodic file transfers from the picture archiving and communication system, and (c) the results of natural language processing (NLP) analysis. Users can perform more structured and detailed searches with this application by combining different imaging and patient characteristics such as examination number; patient age, gender, and medical record number; and imaging modality. Use of NLP analysis allows a more effective search for reports with positive findings, resulting in the retrieval of more cases and terms having greater relevance. From the retrieved results, users can save images, bookmark examinations, and navigate to an external search engine such as Google. Render has applications in the fields of radiology education, research, and clinical decision support.},
author = {Dang, Pragya A. and Kalra, Mannudeep K. and Schultz, Thomas J. and Graham, Steven A. and Dreyer, Keith J.},
doi = {10.1148/rg.295085036},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dang et al. - 2009 - Informatics in radiology Render an online searchable radiology study repository.pdf:pdf},
isbn = {1527-1323 (Electronic)$\backslash$r0271-5333 (Linking)},
issn = {1527-1323},
journal = {RadioGraphics},
keywords = {Computerized,Databases,Factual,Internet,Medical Informatics,Medical Informatics: methods,Medical Records Systems,Online Systems,Radiology,Radiology Information Systems,Radiology: methods,United States},
month = {sep},
number = {5},
pages = {1233--46},
pmid = {19564253},
publisher = {Radiological Society of North America},
title = {{Informatics in radiology: Render: an online searchable radiology study repository.}},
url = {http://pubs.rsna.org/doi/10.1148/rg.295085036 http://pubs.rsna.org/doi/abs/10.1148/rg.295085036},
volume = {29},
year = {2009}
}
@incollection{AcunaPartal2000,
abstract = {This book breaks new ground in translation theory and practice. The central question is: In what ways are translations affected by text types? The two main areas of investigation are: A. What are the advantages of focusing on text types when trying to understand the process of translation? How do translators tackle different text types in their daily practice? B. To what extent and in what areas are text types identical across languages and cultures? What similarities and dissimilarities can be observed in text types of original and translated texts?Part I deals with methodological aspects and offers a typology of translations both as product and as process. Part II is devoted to domain-specific texts in a cross-cultural perspective, while Part III is concerned with terminology and lexicon as well as the constraints of mode and medium involving dubbing and subtitling as translation methods. Sonnets, sagas, fairy tales, novels and feature films, sermons, political speeches, international treaties, instruction leaflets, business letters, academic lectures, academic articles, medical research articles, technical brochures and legal documents are but some of the texts under investigation.In sum, this volume provides a theoretical overview of major problems and possibilities as well as investigations into a variety of text types with practical suggestions that deserve to be weighted by anyone considering the relation between text typology and translation. The volume is indispensable for the translator in his/her efforts to become a "competent text-aware professional."},
annote = {Chapter: TRANSLATION OF MEDICAL RESEARCH ARTICLES},
author = {{Acuna Partal}, Carmen},
booktitle = {TRANS. Revista de Traductologia},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Acuna Partal - 2000 - Text Typology and Translation.pdf:pdf},
isbn = {9027216290},
issn = {11372311},
pages = {161--163},
title = {{Text Typology and Translation}},
volume = {4},
year = {2000}
}
@article{Cimiano2010a,
abstract = {We revisit the notion of ontology localization, propose a new definition and clearly specify the layers of an ontology that can be affected by the process of localizing it. We also work out a number of dimensions that allow to characterize the type of ontology localization performed and to predict the layers that will be affected. Overall our aim is to contribute to a better understanding of the task of localizing an ontology.},
author = {Cimiano, Philipp and Montiel-ponsoda, Elena and Buitelaar, Paul and Espinoza, Mauricio},
doi = {10.3233/AO-2010-0075},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cimiano et al. - 2010 - A note on ontology localization.pdf:pdf},
issn = {00318116},
journal = {Applied Ontology},
keywords = {multilinguality,ontology engineering,ontology localization},
pages = {127--137},
title = {{A Note on Ontology Localization}},
volume = {5},
year = {2010}
}
@inproceedings{Wok2015,
abstract = {The quality of machine translation is rapidly evolving. Today one can find several machine translation systems on the web that provide reasonable translations, although the systems are not perfect. In some specific domains, the quality may decrease. A recently proposed approach to this domain is neural machine translation. It aims at building a jointly-tuned single neural network that maximizes translation performance, a very different approach from traditional statistical machine translation. Recently proposed neural machine translation models often belong to the encoder-decoder family in which a source sentence is encoded into a fixed length vector that is, in turn, decoded to generate a translation. The present research examines the effects of different training methods on a Polish-English Machine Translation system used for medical data. The European Medicines Agency parallel text corpus was used as the basis for training of neural and statistical network-based translation systems. The main machine translation evaluation metrics have also been used in analysis of the systems. A comparison and implementation of a real-time medical translator is the main focus of our experiments.},
author = {Wo{\l}k, Krzysztof and Marasek, Krzysztof},
booktitle = {Procedia Computer Science},
doi = {10.1016/j.procs.2015.08.456},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wo{\l}k, Marasek - 2015 - Neural-based Machine Translation for Medical Text Domain. Based on European Medicines Agency Leaflet Texts.pdf:pdf},
issn = {18770509},
keywords = {machine translation,nlp,statistical machine translation,text processing},
pages = {2--9},
title = {{Neural-based Machine Translation for Medical Text Domain. Based on European Medicines Agency Leaflet Texts}},
volume = {64},
year = {2015}
}
@article{Jonquet2009,
abstract = {The range of publicly available biomedical data is enormous and is expanding fast. This expansion means that researchers now face a hurdle to extracting the data they need from the large numbers of data that are available. Biomedical researchers have turned to ontologies and terminologies to structure and annotate their data with ontology concepts for better search and retrieval. However, this annotation process cannot be easily automated and often requires expert curators. Plus, there is a lack of easy-to-use systems that facilitate the use of ontologies for annotation. This paper presents the Open Biomedical Annotator (OBA), an ontology-based Web service that annotates public datasets with biomedical ontology concepts based on their textual metadata (www.bioontology.org). The biomedical community can use the annotator service to tag datasets automatically with ontology terms (from UMLS and NCBO BioPortal ontologies). Such annotations facilitate translational discoveries by integrating annotated data.[1].},
author = {Jonquet, Clement and Shah, Nigam H and Musen, Mark A},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jonquet, Shah, Musen - 2009 - The open biomedical annotator.pdf:pdf},
isbn = {2153-6430 (Electronic)$\backslash$r2153-6430 (Linking)},
issn = {2153-6430},
journal = {Summit on translational bioinformatics},
pages = {56--60},
pmid = {21347171},
title = {{The open biomedical annotator.}},
url = {https://hal.archives-ouvertes.fr/hal-00492024 http://www.ncbi.nlm.nih.gov/pubmed/21347171{\%}5Cnhttp://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3041576},
volume = {2009},
year = {2009}
}
@article{Moore1986,
abstract = {The feasibility of automated translation of scientific and medical documents remains controversial. This report describes a minicomputer-based German-to-English translation system (TRANSOFT) that employs word order rearrangement followed by word-for-word translation and disambiguation based on context. This translation system was applied to a computer-readable version of Adler's Knochenkrankheiten (Bone Diseases), which contains 118,604 words, with 10,216 distinct words in 7,211 sentences averaging 16.4 words each. The translation required 2,791 word rearrangement formulas, 78 percent of which were first used in the first half of the document. There were 2,392 occurrences of 12 potentially ambiguous terms, of which only 18 (0.8 percent) were not resolvable from the immediate context. As foreign language medical documents become increasingly available in computer-readable form through computerized typesetting, electronic publishing, and improved optical character recognition equipment, automated translation systems may provide a rapid and inexpensive means of obtaining draft translations. ?? 1986.},
author = {Moore, G. William and Riede, U. N. and Polacsek, Richard A. and Miller, Robert E. and Hutchins, Grover M.},
doi = {10.1016/0002-9343(86)90190-7},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Moore et al. - 1986 - Automated translation of German to English medical text.pdf:pdf},
isbn = {0002-9343 (Print)$\backslash$r0002-9343 (Linking)},
issn = {00029343},
journal = {The American Journal of Medicine},
number = {1},
pages = {103--111},
pmid = {3755289},
publisher = {Elsevier},
title = {{Automated translation of German to English medical text}},
volume = {81},
year = {1986}
}
@article{Langlotz2006,
author = {Langlotz, Curtis P.},
doi = {10.1148/rg.266065168},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Langlotz - 2006 - RadLex a new method for indexing online educational materials.pdf:pdf},
isbn = {0271-5333},
issn = {0271-5333},
journal = {RadioGraphics},
month = {nov},
number = {6},
pages = {1595--1597},
pmid = {17102038},
publisher = {Radiological Society of North America},
title = {{RadLex: a new method for indexing online educational materials}},
url = {http://pubs.rsna.org/doi/10.1148/rg.266065168 papers3://publication/doi/10.1148/rg.266065168},
volume = {26},
year = {2006}
}
@article{Sokolova2009,
abstract = {This paper presents a systematic analysis of twenty four performance measures used in the complete spectrum of Machine Learning classification tasks, i.e., binary, multi-class, multi-labelled, and hierarchical. For each classification task, the study relates a set of changes in a confusion matrix to specific characteristics of data. Then the analysis concentrates on the type of changes to a confusion matrix that do not change a measure, therefore, preserve a classifier's evaluation (measure invariance). The result is the measure invariance taxonomy with respect to all relevant label distribution changes in a classification problem. This formal analysis is supported by examples of applications where invariance properties of measures lead to a more reliable evaluation of classifiers. Text classification supplements the discussion with several case studies. ?? 2009 Elsevier Ltd. All rights reserved.},
author = {Sokolova, Marina and Lapalme, Guy},
doi = {10.1016/j.ipm.2009.03.002},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sokolova, Lapalme - 2009 - A systematic analysis of performance measures for classification tasks.pdf:pdf},
isbn = {0306-4573},
issn = {03064573},
journal = {Information Processing and Management},
keywords = {Machine Learning,Performance evaluation,Text classification},
number = {4},
pages = {427--437},
title = {{A systematic analysis of performance measures for classification tasks}},
volume = {45},
year = {2009}
}
@article{Lehman2011,
author = {Lehman, Li-wei and Moody, George and Heldt, Thomas and Kyaw, Tin H},
doi = {10.1097/CCM.0b013e31820a92c6.Multiparameter},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lehman et al. - 2011 - Multiparameter Intelligent Monitoring in Intensive Care II (MIMICII) A public-access intensive care unit database.pdf:pdf},
journal = {Critical Care},
number = {February 2010},
pages = {952--960},
title = {{Multiparameter Intelligent Monitoring in Intensive Care II (MIMICII): A public-access intensive care unit database}},
volume = {39},
year = {2011}
}
@article{Lamurias,
abstract = {Electronic health records have been adopted by many institutions and constitute an important source of biomedical information. Text mining methods can be applied to this type of information to automati-cally extract useful knowledge. We propose a crowd-sourcing pipeline to improve the precision of extraction and normalization of biomedi-cal terms. Although crowd-sourcing has been applied in other fields, it has not been applied yet to the annotation of health records. We expect this pipeline to improve the precision of supervised machine learning classifiers, by letting the users suggest the boundaries of the terms, as well as the respective ontology concept. We intend to apply this pipeline to the recognition and normalization of disorder menti-ons (i.e., references to a disease or other health related conditions in a text) in electronic health records, as well as drug, gene and protein mentions.},
author = {Lamurias, Andre and Pedro, Vasco and Clarke, Luka and Couto, Francisco M},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lamurias et al. - Unknown - Annotating biomedical ontology terms in electronic health records using crowd-sourcing.pdf:pdf},
title = {{Annotating biomedical ontology terms in electronic health records using crowd-sourcing}}
}
@article{Kathol2005,
abstract = {We present a number of challenges and solutions that have arisen in the development of a speech translation system for American English and Pashto, highlighting those specific to a very low resource language. In particular, we address issues posed by Pashto in the areas of written representation, corpus creation, speech recognition, speech synthesis, and grammar development for translation.},
author = {Kathol, Andreas and Precoda, Kristin and Vergyri, Dimitra and Wang, Wen and Riehemann, Susanne and International, S R I and Park, Menlo},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kathol et al. - 2005 - Speech Translation for Low-Resource Languages The Case of Pashto.pdf:pdf},
journal = {Syntax},
pages = {2273--2276},
title = {{Speech Translation for Low-Resource Languages : The Case of Pashto}},
year = {2005}
}
@inproceedings{Atdag2013,
abstract = {Named entity recognition (NER) is a popular domain of natural language processing. For this reason, many tools exist to perform this task. Amongst other points, they differ in the processing method they rely upon, the entity types they can detect, the nature of the text they can handle, and their input/output formats. This makes it difficult for a user to select an appropriate NER tool for a specific situation. In this article, we try to answer this question in the context of biographic texts. For this matter, we first constitute a new corpus by annotating Wikipedia articles. We then select publicly available, well known and free for research NER tools for comparison: Stanford NER, Illinois NET, OpenCalais NER WS and Alias-i LingPipe. We apply them to our corpus, assess their performances and compare them. When considering overall performances, a clear hierarchy emerges: Stanford has the best results, followed by LingPipe, Illionois and OpenCalais. However, a more detailed evaluation performed relatively to entity types and article categories highlights the fact their performances are diversely influenced by those factors. This complementarity opens an interesting perspective regarding the combination of these individual tools in order to improve performance.$\backslash$n$\backslash$nPublished in: 2nd International Conference on Systems and Computer Science,$\backslash$nVilleneuve d'Ascq (FR), 228-233, 2013},
archivePrefix = {arXiv},
arxivId = {1308.0661},
author = {Atdaǧ, Samet and Labatut, Vincent},
booktitle = {2013 2nd International Conference on Systems and Computer Science, ICSCS 2013},
doi = {10.1109/IcConSCS.2013.6632052},
eprint = {1308.0661},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Atdaǧ, Labatut - 2013 - A comparison of named entity recognition tools applied to biographical texts.pdf:pdf},
isbn = {9781479920204},
issn = {978-1-4799-2020-4},
month = {aug},
pages = {228--233},
title = {{A comparison of named entity recognition tools applied to biographical texts}},
url = {http://arxiv.org/abs/1308.0661 http://dx.doi.org/10.1109/IcConSCS.2013.6632052},
year = {2013}
}
@article{Wang2015,
abstract = {Disorders of the peripheral nervous system have traditionally been evaluated using clinical history, physical examination, and electrodiagnostic testing. In selected cases, imaging modalities such as magnetic resonance (MR) neurography may help further localize or characterize abnormalities associated with peripheral neuropathies, and the clinical importance of such techniques is increasing. However, MR image interpretation with respect to peripheral nerve anatomy and disease often presents a diagnostic challenge because the relevant knowledge base remains relatively specialized. Using the radiology knowledge resource RadLex{\textregistered}, a series of RadLex queries, the Annotation and Image Markup standard for image annotation, and a Web services–based software architecture, the authors developed an application that allows ontology-assisted image navigation. The application provides an image browsing interface, allowing users to visually inspect the imaging appearance of anatomic structures. By interacting directly with the images, users can access additional structure-related information that is derived from RadLex (eg, muscle innervation, muscle attachment sites). These data also serve as conceptual links to navigate from one portion of the imaging atlas to another. With 3.0-T MR neurography of the brachial plexus as the initial area of interest, the resulting application provides support to radiologists in the image interpretation process by allowing efficient exploration of the MR imaging appearance of relevant nerve segments, muscles, bone structures, vascular landmarks, anatomic spaces, and entrapment sites, and the investigation of neuromuscular relationships. {\textcopyright}RSNA, 2015},
author = {Wang, K.C.a b and Salunkhe, A.R.c and Morriso, J.J.d and Lee, P.P.b and Mejino, J.L.V.e and Detwiler, L.T.e f and Brinkley, J.F.e F and Siegel, E.L.a d and Rubin, D.L.g and Carrino, J.A.h},
doi = {10.1148/rg.351130072},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2015 - Ontology-based image navigation Exploring 3.0-T MR neurography of the brachial plexus using AIM and radlex.pdf:pdf},
isbn = {10.1148/rg.351130072},
issn = {1527-1323 (Electronic)},
journal = {RadioGraphics},
number = {1},
pages = {142--151},
pmid = {25590394},
publisher = {Radiological Society of North America},
title = {{Ontology-based image navigation: Exploring 3.0-T MR neurography of the brachial plexus using AIM and radlex}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25590394 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4319494 https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922443890{\&}partnerID=40{\&}md5=f244c110f036e962b138296a1fa82598},
volume = {35},
year = {2015}
}
@inproceedings{Buitelaar2008,
abstract = {For enabling advanced access to clinical imaging$\backslash$nand text data, it is relevant to know what kind of$\backslash$nknowledge the clinician wants to know or the queries$\backslash$nthat clinicians are interested in. Through intensive$\backslash$ninterviews and discussions with radiologists$\backslash$nand clinicians, we have learned that medical imaging$\backslash$ndata is analyzed - and hence queried from$\backslash$nthree different perspectives, i.e. the anatomic perspective$\backslash$naddressing the involved body parts, the$\backslash$nradiology-specific spatial perspective describing$\backslash$nthe relationships of located anatomical regions to$\backslash$nother anatomical parts, and the disease perspective$\backslash$ndistinguishing between normal and abnormal imaging$\backslash$nfeatures. Our aim is to establish query patterns$\backslash$nreflecting those three perspectives that would$\backslash$ntypically be used by clinicians and radiologists to$\backslash$nfind patient-specific sets of relevant images.},
author = {Buitelaar, Paul and Wennerberg, Pinar and Zillner, Sonja},
booktitle = {Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing. Workshop on Current Trends in Biomedical Natural Language Processing (BioNLP-2008), located at 46th International Meeting of the Association for Computational Linguis},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Buitelaar, Wennerberg, Zillner - 2008 - Statistical Term Profiling for Query Pattern Mining.pdf:pdf},
pages = {114--115},
title = {{Statistical Term Profiling for Query Pattern Mining}},
url = {http://www.aclweb.org/anthology-new/},
year = {2008}
}
@article{Monteiro2016,
abstract = {The tremendous quantity of data stored daily in healthcare institutions demands the development of new methods to summarize and reuse available information in clinical practice. In order to leverage modern healthcare information systems, new strategies must be developed that address challenges such as extraction of relevant information, data redundancy, and the lack of associations within the data. This article proposes a pipeline to overcome these challenges in the context of medical imaging reports, by automatically extracting and linking information, and summarizing natural language reports into an ontology model. Using data from the Physionet MIMIC II database, we created a semantic knowledge base with more than 6.5 millions of triples obtained from a collection of 16,000 radiology reports.},
author = {Monteiro, Eriksson and Sernadela, Pedro and Matos, S{\'{e}}rgio and Costa, Carlos and Oliveira, Jos{\'{e}} Lu{\'{i}}s},
doi = {10.5220/0005709503450352},
file = {:home/lcampos/Desktop/HEALTHINF{\_}2016{\_}81.pdf:pdf},
isbn = {978-989-758-170-0},
journal = {Proceedings of the 9th International Joint Conference on Biomedical Engineering Systems and Technologies},
keywords = {Clinical reports,Healthcare information management,Radiology,Semantic web,Text-mining},
number = {Biostec},
pages = {345--352},
title = {{Semantic Knowledge Base Construction from Radiology Reports}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84969263661{\&}partnerID=tZOtx3y1{\%}5Cnhttp://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0005709503450352},
volume = {5},
year = {2016}
}
@article{O.B2015,
abstract = {Machine translation (MT) is defined as a subfield of computational linguistics that investigates the use of computer software to translate text or speech from one natural language to another. It is a key application in the field of natural language processing. At its basic level, MT performs simple substitution of words in one natural language for words in another. Effort to access documents from one language to another leads to the development of machine translation system which involves lots of heterogeneous features and its implementations. Approaches to machine translations are different and each of this approach has its own benefits and drawbacks. This study looks at the various approaches to machine translations and future needs in order to provide more robust and sensible system in the area of natural language processing which will be resistant and impervious to failure regardless of users' inputs. It is hopeful that researchers in the area of language processing can make use of our valuable improvement and suggestions.},
author = {O.B, Abiola and A.O, Adetunmbi and A, Oguntimilehin.},
doi = {10.5120/21325-4278},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/O.B, A.O, A - 2015 - A Review of the Various Approaches for Text to Text Machine Translations.pdf:pdf},
issn = {09758887},
journal = {International Journal of Computer Applications},
keywords = {Direct-Based,Hybrid Approach,Natural Language Processing,Rule-Based,Statistical-Based},
number = {18},
pages = {7--12},
title = {{A Review of the Various Approaches for Text to Text Machine Translations}},
url = {http://research.ijcaonline.org/volume120/number18/pxc3904278.pdf},
volume = {120},
year = {2015}
}
@article{Kahn2014,
abstract = {Rationale and Objectives: RadLex is a standardized vocabulary developed for clinical practice, research, and education in radiology. This report sought to analyze the use of RadLex to annotate and index the captions of images from the peer-reviewed biomedical literature and to compare the number of annotations per term for RadLex and five other biomedical ontologies in a large corpus of figure captions from biomedical imaging publications. Materials and Methods: RadLex and five other biomedical vocabularies were evaluated. A fully automated web service was used to discover the vocabularies' terms in a collection of 385,018 figure captions from 613 peer-reviewed biomedical journals. Annotations (i.e., figure-term pairs) were analyzed by vocabulary. RadLex annotations were analyzed by journal and RadLex term class. Results: RadLex had the greatest number of annotations per term of the six vocabularies. On average, there were 10.1 RadLex annotations per figure; 380,338 figures (98.8{\%}) were annotated with at least one RadLex term and 288,163 figures (74.8{\%}) were annotated with six or more RadLex terms. Of 39,218 RadLex terms, 8504 (21.7{\%}) were mapped to images in the collection, which was the highest percentage of any of the vocabularies. Conclusions: Although comprising four to 10 times fewer terms than other vocabularies, RadLex showed excellent performance in indexing radiology-centric content. Almost all of the images in a large collection of figures from peer-reviewed biomedical journals were annotated with at least one RadLex term, and almost 75{\%} of the images were annotated with six or more terms. ?? 2014.},
author = {Kahn, Charles E.},
doi = {10.1016/j.acra.2013.11.007},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kahn - 2014 - Annotation of figures from the biomedical imaging literature A comparative analysis of radlex and other standardized vocab.pdf:pdf},
isbn = {1878-4046 (Electronic)$\backslash$r1076-6332 (Linking)},
issn = {10766332},
journal = {Academic Radiology},
keywords = {ARRS GoldMiner,Annotation,Figure captions,Indexing,Lexicon,Ontology,RadLex,Vocabulary},
number = {3},
pages = {384--392},
pmid = {24507425},
title = {{Annotation of figures from the biomedical imaging literature: A comparative analysis of radlex and other standardized vocabularies}},
volume = {21},
year = {2014}
}
@article{Nothman2013,
abstract = {We automatically create enormous, free and multilingual silver-standard training annotations for named entity recognition (ner) by exploiting the text and structure of Wikipedia. Most ner systems rely on statistical models of annotated data to identify and classify names of people, locations and organisations in text. This dependence on expensive annotation is the knowledge bottleneck our work overcomes. We first classify each Wikipedia article into named entity (ne) types, training and evaluating on 7200 manually-labelled Wikipedia articles across nine languages. Our cross-lingual approach achieves up to 95{\%} accuracy. We transform the links between articles into ne annotations by projecting the target articles classifications onto the anchor text. This approach yields reasonable annotations, but does not immediately compete with existing gold-standard data. By inferring additional links and heuristically tweaking the Wikipedia corpora, we better align our automatic annotations to gold standards. We annotate millions of words in nine languages, evaluating English, German, Spanish, Dutch and Russian Wikipedia-trained models against conll shared task data and other gold-standard corpora. Our approach outperforms other approaches to automatic ne annotation (Richman and Schone, 2008 [61], Mika et al., 2008 [46]) competes with gold-standard training when tested on an evaluation corpus from a different source; and performs 10{\%} better than newswire-trained models on manually-annotated Wikipedia text. {\textcopyright} 2012 Elsevier B.V. All rights reserved.},
author = {Nothman, Joel and Ringland, Nicky and Radford, Will and Murphy, Tara and Curran, James R.},
doi = {10.1016/j.artint.2012.03.006},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nothman et al. - 2013 - Learning multilingual named entity recognition from Wikipedia.pdf:pdf},
isbn = {0004-3702},
issn = {00043702},
journal = {Artificial Intelligence},
keywords = {Annotated corpora,Information extraction,Named entity recognition,Semi-structured resources,Semi-supervised learning,Wikipedia},
pages = {151--175},
title = {{Learning multilingual named entity recognition from Wikipedia}},
volume = {194},
year = {2013}
}
@article{Johnson2016,
abstract = {MIMIC-III ('Medical Information Mart for Intensive Care') is a large, single-center database comprising information relating to patients admitted to critical care units at a large tertiary care hospital. Data includes vital signs, medications, laboratory measurements, observations and notes charted by care providers, fluid balance, procedure codes, diagnostic codes, imaging reports, hospital length of stay, survival data, and more. The database supports applications including academic and industrial research, quality improvement initiatives, and higher education coursework.},
author = {Johnson, Alistair E W and Pollard, Tom J and Shen, Lu and Lehman, Li-Wei H and Feng, Mengling and Ghassemi, Mohammad and Moody, Benjamin and Szolovits, Peter and Celi, Leo Anthony and Mark, Roger G},
doi = {10.1038/sdata.2016.35},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Johnson et al. - 2016 - MIMIC-III, a freely accessible critical care database.pdf:pdf},
issn = {2052-4463},
journal = {Scientific data},
pages = {160035},
pmid = {27219127},
title = {{MIMIC-III, a freely accessible critical care database.}},
url = {http://www.nature.com/articles/sdata201635{\%}5Cnpapers3://publication/doi/10.1038/sdata.2016.35},
volume = {3},
year = {2016}
}
@incollection{Manning2009b,
author = {Manning, Christopher D. and Raghavan, Prabhakar and Sch{\"{u}}tze, Hinrich},
booktitle = {Introduction to Information Retrieval},
chapter = {8},
title = {{Evaluation in information retrieval}},
year = {2009}
}
@article{Do2010,
abstract = {The article describes an open-source, RadLex-compatible system that uses natural language processing methods to automatically create a searchable teaching file from the RIS and PACS.},
author = {Do, Bao H. and Wu, Andrew and Biswal, Sandip and Kamaya, Aya and Rubin, Daniel L.},
doi = {10.1148/rg.307105083},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Do et al. - 2010 - Informatics in Radiology RADTF A Semantic Search–enabled, Natural Language Processor–generated Radiology Teaching.pdf:pdf},
isbn = {0271-5333},
issn = {0271-5333},
journal = {RadioGraphics},
month = {nov},
number = {7},
pages = {2039--2048},
pmid = {20801868},
publisher = {Radiological Society of North America},
title = {{Informatics in Radiology: RADTF: A Semantic Search–enabled, Natural Language Processor–generated Radiology Teaching File}},
url = {http://pubs.rsna.org/doi/10.1148/rg.307105083},
volume = {30},
year = {2010}
}
@article{Ratinov2009,
abstract = {We analyze some of the fundamental design challenges and misconceptions that underlie the development of an efficient and robust NER system. In particular, we address issues such as the representation of text chunks, the inference approach needed to combine local NER decisions, the sources of prior knowledge and how to use them within an NER system. In the process of comparing several solutions to these challenges we reach some surprising conclusions, as well as develop an NER system that achieves 90.8 F1 score on the CoNLL-2003 NER shared task, the best reported result for this dataset.},
author = {Ratinov, Lev and Roth, Dan},
doi = {10.3115/1596374.1596399},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ratinov, Roth - 2009 - Design challenges and misconceptions in named entity recognition.pdf:pdf},
isbn = {978-1-932432-29-9},
issn = {1932432299},
journal = {CoNLL: Conference on Computational Natural Language Learning},
number = {June},
pages = {147--155},
title = {{Design challenges and misconceptions in named entity recognition}},
url = {http://dl.acm.org/citation.cfm?id=1596399},
volume = {30},
year = {2009}
}
@article{Langlotz2002,
abstract = {Although most medical lexicons contain up to 80{\%} of clinical terms used in an ambulatory patient medical records archive, preliminary research suggests that they may be far less complete for radiology terms. We therefore compared the likelihood that several existing medical lexicons would contain terms found in a radiology report to the likelihood they would contain terms found in an ambulatory care medical record. We used three samples of imaging terms to assess the completeness of existing lexicons for medical imaging: (1) a random sample of imaging terms from the Unified Medical Language System Large Scale Vocabulary Test (UMLS-LSVT; n = 218), (2) terms actually used in the first 80 clinical knee magnetic resonance imaging reports generated by the routine clinical use of a structured reporting system (eDictation, Marlton, NJ; n = 76), and (3) terms listed in a glossary of thoracic imaging prepared by the Fleischner Society (n = 173). Using the UMLS Web-based Knowledge Source Server (http://umlsks.nlm.nih. gov/), we measured the rate at which terms in each of the above three sources were found in the UMLS and two of its major constituent terminologies: ICD-9-CM and SNOMED International. ICD-9-CM contained matches for 3{\%}, 8{\%}, and 11{\%} of terms from the Fleischner Society Glossary, eDictation, and NLM-LSVT, respectively. SNOMED International contained matches for 32{\%}, 46{\%}, and 36{\%} of terms from the Fleischner Society Glossary, eDictation, and NLM-LSVT, respectively. The UMLS contained matches for 36{\%}, 50{\%}, and 45{\%} of terms from the Fleischner Society Glossary, eDictation, and NLM-LSVT, respectively. The assessed vocabularies were least likely to contain a term from the Fleischner Society Glossary and most likely to contain a term from the eDictation lexicon. The UMLS was the most complete, and ICD-9 was the least complete of the three systems evaluated. No lexicon achieved greater than 50{\%} completeness for any test set of imaging terms. Our results show that no single lexicon is sufficiently complete to allow comprehensive indexing, search, and retrieval of radiology report information. These results confirm the few results available from the medical literature indicating that existing controlled vocabularies are insufficiently complete to represent the contents of radiology reports. A subjective analysis of these results may identify particular imaging sub-areas for which new terms should be developed.},
author = {Langlotz, Curtis P and Caldwell, Susan A},
doi = {10.1007/s10278-002-5046-5},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Langlotz, Caldwell - 2002 - The completeness of existing lexicons for representing radiology report information.pdf:pdf},
isbn = {1027800250465},
issn = {0897-1889},
journal = {Journal of Digital Imaging},
keywords = {3 structured data capture,completeness evaluation,radiology reporting,terminology},
number = {March},
pages = {201--205},
pmid = {12105728},
title = {{The completeness of existing lexicons for representing radiology report information.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/12105728},
volume = {15 Suppl 1},
year = {2002}
}
@phdthesis{Leal2015,
abstract = {Clinical notes in textual form occur frequently in Electronic Health Records (EHRs).They are mainly used to describe treatment plans, symptoms, diagnostics, etc. Clinicalnotes are recorded in narrative language without any structured form and, since each medicalprofessional uses different types of terminologies according to context and to theirspecialization, the interpretation of these notes is very challenging for their complexity,heterogeneity, ambiguity and contextual sensitivity.Forcing medical professionals to introduce the information in a predefined structuresimplifies the interpretation. However, the imposition of such a rigid structure increasesnot only the time needed to record data, but it also introduces barriers at recording unusualcases. Thus, medical professionals are already encouraged to record the information in adigital form, but mostyl as narrative text. This will increase the amount of clinical notes toprocess, and doing it manually requires a huge human effort to accomplish it in a feasible time. This work presents a system for automatic recognition and normalization of biomedical concepts within clinical notes, by applying text mining techniques and using domain knowledge from the SNOMED CT ontology. The system is composed by two modules.The first one is responsible for the recognition and it is based on the Stanford NER Softwareto generate CRF models. The models were generated by using a rich set of features and employing a novel classification system, SBIEON. The second module is responsible for the normalization, where a pipeline framework was created. This modular framework leverages on a set of techniques such as (i) direct match dictionary lookup, (ii) pattern matching, (iii) information content and (iv) semantic similarity. The system was evaluated in the SemEval 2015 international competition, achieving the second best F-score (74{\%}) and the second best precision (77.9{\%}), among 38 submissions. After the competition, this system was improved, increasing the overall performance and reducing the running time by 60{\%}.},
author = {Leal, Andr{\'{e}}},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Leal - 2015 - RECOGNITION AND NORMALIZATION OF BIOMEDICAL ENTITIES WITHIN CLINICAL NOTES.pdf:pdf},
pages = {123},
school = {University of Lisbon},
title = {{RECOGNITION AND NORMALIZATION OF BIOMEDICAL ENTITIES WITHIN CLINICAL NOTES}},
year = {2015}
}
@article{Schyve2007,
abstract = {Effective communication with patients is critical to the safety and quality of care. Barriers to this communication include differences in language, cultural differences, and low health literacy. Evidence-based practices that reduce these barriers must be integrated into, rather than just added to, health care work processes.},
author = {Schyve, Paul M},
doi = {10.1007/s11606-007-0365-3},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schyve - 2007 - Language differences as a barrier to quality and safety in health care The joint commission perspective.pdf:pdf},
isbn = {1160600703653},
issn = {08848734},
journal = {Journal of General Internal Medicine},
keywords = {Cultural differences,Health care,Information management,Language differences,Low health literacy,accreditation},
number = {SUPPL. 2},
pages = {360--361},
pmid = {17957426},
title = {{Language differences as a barrier to quality and safety in health care: The joint commission perspective}},
volume = {22},
year = {2007}
}
@misc{Friedman1998,
abstract = {Evaluating natural language processing (NLP) systems in the clinical domain is a difficult task which is important for advancement of the field. A number of NLP systems have been reported that extract information from free-text clinical reports, but not many of the systems have been evaluated. Those that were evaluated noted good performance measures but the results were often weakened by ineffective evaluation methods. In this paper we describe a set of criteria aimed at improving the quality of NLP evaluation studies. We present an overview of NLP evaluations in the clinical domain and also discuss the Message Understanding Conferences (MUC) [1-4]. Although these conferences constitute a series of NLP evaluation studies performed outside of the clinical domain, some of the results are relevant within medicine. In addition, we discuss a number of factors which contribute to the complexity that is inherent in the task of evaluating natural language systems.},
author = {Friedman, C and Hripcsak, G},
booktitle = {Methods of Information in Medicine},
doi = {98040334 [pii]},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Friedman, Hripcsak - 1998 - Evaluating natural language processors in the clinical domain.pdf:pdf},
isbn = {0026-1270},
issn = {00261270},
keywords = {Evaluation of Clinical Information Resources,Medical Language processing,Natural Language Processing},
month = {nov},
number = {4-5},
pages = {334--344},
pmid = {9865031},
title = {{Evaluating natural language processors in the clinical domain}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/9865031},
volume = {37},
year = {1998}
}
@article{Powell2015,
abstract = {Rationale and Objectives: To survey North American radiologists on current practices in structured reporting and language. Materials and Methods: An e-mail invitation was sent to the Association of University Radiologists membership (comprising 910 members) to participate in an online survey that addressed development, use, and experience of structured reporting, language, and imaging classification or reporting systems and personal dictation styles. Results: Of the 910 members e-mailed, 265 (29.1{\%}) responded, 90.6{\%} of whom were from academic teaching hospitals. There were no significant differences in responses based on group size or region of practice. Of all the respondents, 51.3{\%} come from groups that developed structured reporting for at least half of their reports and only 10.9{\%} for none. A significantly fewer 13{\%} of respondents used rigid unmodifiable structures or checklists rather than adaptable outlines; 59.5{\%} respondents report being satisfied or very satisfied with their structured reports, whereas a significantly fewer 13{\%} report being dissatisfied or very dissatisfied. Structured reports were reportedly significantly more likely to be required, appreciated, and to decrease errors in departments using many structured reports compared to groups with less widespread use. Conclusions: Most academic radiology departments are using or experimenting with structured reports. Although radiologist satisfaction with standardization is significant, there are strong opinions about their limitations and value. Our survey suggests that North American radiologists are invested in exploring structured reporting and will hopefully inform future study on how we define a standard report and how much we can centralize this process.},
author = {Powell, Daniel K. and Silberzweig, James E.},
doi = {10.1016/j.acra.2014.08.014},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Powell, Silberzweig - 2015 - State of Structured Reporting in Radiology, a Survey.pdf:pdf},
isbn = {1076-6332},
issn = {18784046},
journal = {Academic Radiology},
keywords = {National standards,Reporting systems,Reporting templates,Standardized language,Structured reporting},
number = {2},
pages = {226--233},
pmid = {25442793},
title = {{State of Structured Reporting in Radiology, a Survey}},
volume = {22},
year = {2015}
}
@article{Nadeau2007,
abstract = {David Nadeau and Satoshi Sekine. 2007. A survey of named entity recognition and classification. Linguisticae Investigationes, 30:3–26.},
author = {Nadeau, David},
doi = {10.1075/li.30.1.03nad},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nadeau - 2007 - A survey of named entity recognition and classification.pdf:pdf},
isbn = {0378-4169},
issn = {03784169},
journal = {Linguisticae Investigationes},
number = {30},
pages = {3--26.},
pmid = {23847424},
title = {{A survey of named entity recognition and classification}},
url = {http://nlp.cs.nyu.edu/sekine/papers/li07.pdf},
year = {2007}
}
@article{Hong2013,
abstract = {The Radiological Society of North America (RSNA) has developed a set of templates for structured reporting of radiology results. To measure how much of the content of conventional narrative ("free-text") reports is covered by the concepts included in the RSNA reporting templates, we selected five reporting templates that represented a variety of imaging modalities and organ systems. From a sample of 8,275 consecutive, de-identified radiology reports from an academic medical center, we identified one corresponding imaging procedure code for each reporting template. The reports were annotated with RadLex and SNOMED CT terms using the BioPortal Annotator web service. The reporting templates we examined accounted for 17 to 49 {\%} of the concepts that actually appeared in a sample of corresponding radiology reports. The findings suggest that the concepts that appear in the reporting templates occur frequently within free-text clinical reports; thus, the templates provide useful coverage of the "domain of discourse" in radiology reports. The techniques used in this study may be helpful to guide the development of reporting templates by identifying concepts that occur frequently in radiology reports, to evaluate the coverage of existing templates, and to establish global benchmarks for reporting templates.},
author = {Hong, Yi and Kahn, Charles E},
doi = {10.1007/s10278-013-9597-4},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hong, Kahn - 2013 - Content analysis of reporting templates and free-text radiology reports.pdf:pdf},
isbn = {1027801395974},
issn = {08971889},
journal = {Journal of Digital Imaging},
keywords = {BioPortal Annotator,Biomedical ontologies,Narrative (free-text) reports,RadLex,Radiology,Reporting,Reporting templates,SNOMED CT,Structured reports},
month = {oct},
number = {5},
pages = {843--849},
pmid = {23553231},
publisher = {Springer},
title = {{Content analysis of reporting templates and free-text radiology reports}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23553231 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3782601},
volume = {26},
year = {2013}
}
@incollection{Manning2009c,
author = {Manning, Christopher D. and Raghavan, Prabhakar and Sch{\"{u}}tze, Hinrich},
booktitle = {Introduction to Information Retrieval},
chapter = {2.2.4},
title = {{Stemming and lemmatization}},
year = {2009}
}
@article{Gerstmair2012,
abstract = {OBJECTIVES: To create an advanced image retrieval and data-mining system based on in-house radiology reports.$\backslash$n$\backslash$nMETHODS: Radiology reports are semantically analysed using natural language processing (NLP) techniques and stored in a state-of-the-art search engine. Images referenced by sequence and image number in the reports are retrieved from the picture archiving and communication system (PACS) and stored for later viewing. A web-based front end is used as an interface to query for images and show the results with the retrieved images and report text. Using a comprehensive radiological lexicon for the underlying terminology, the search algorithm also finds results for synonyms, abbreviations and related topics.$\backslash$n$\backslash$nRESULTS: The test set was 108 manually annotated reports analysed by different system configurations. Best results were achieved using full syntactic and semantic analysis with a precision of 0.929 and recall of 0.952. Operating successfully since October 2010, 258,824 reports have been indexed and a total of 405,146 preview images are stored in the database.$\backslash$n$\backslash$nCONCLUSIONS: Data-mining and NLP techniques provide quick access to a vast repository of images and radiology reports with both high precision and recall values. Consequently, the system has become a valuable tool in daily clinical routine, education and research.$\backslash$n$\backslash$nKEY POINTS: Radiology reports can now be analysed using sophisticated natural language-processing techniques. Semantic text analysis is backed by terminology of a radiological lexicon. The search engine includes results for synonyms, abbreviations and compositions. Key images are automatically extracted from radiology reports and fetched from PACS. Such systems help to find diagnoses, improve report quality and save time.},
author = {Gerstmair, Axel and Daumke, Philipp and Simon, Kai and Langer, Mathias and Kotter, Elmar},
doi = {10.1007/s00330-012-2608-x},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gerstmair et al. - 2012 - Intelligent image retrieval based on radiology reports.pdf:pdf},
isbn = {1432-1084 (Electronic)$\backslash$r0938-7994 (Linking)},
issn = {09387994},
journal = {European Radiology},
keywords = {Data mining,Image retrieval,Natural language processing,Radiology reports,Search engine},
number = {12},
pages = {2750--2758},
pmid = {22865274},
title = {{Intelligent image retrieval based on radiology reports}},
volume = {22},
year = {2012}
}
@article{Finkel2005,
abstract = {Most current statistical natural language process- ing models use only local features so as to permit dynamic programming in inference, but this makes them unable to fully account for the long distance structure that is prevalent in language use. We show how to solve this dilemma with Gibbs sam- pling, a simple Monte Carlo method used to per- form approximate inference in factored probabilis- tic models. By using simulated annealing in place of Viterbi decoding in sequence models such as HMMs, CMMs, andCRFs, it is possible to incorpo- rate non-local structure while preserving tractable inference. We use this technique to augment an existing CRF-based information extraction system with long-distance dependency models, enforcing label consistency and extraction template consis- tency constraints. This technique results in an error reduction of up to 9{\%} over state-of-the-art systems on two established information extraction tasks.},
author = {Finkel, Jenny Rose and Grenager, Trond and Manning, Christopher},
doi = {10.3115/1219840.1219885},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Finkel, Grenager, Manning - 2005 - Incorporating non-local information into information extraction systems by gibbs sampling.pdf:pdf},
issn = {02773791},
journal = {in Acl},
number = {1995},
pages = {363 -- 370},
title = {{Incorporating non-local information into information extraction systems by gibbs sampling}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.131.8904},
year = {2005}
}
@misc{Sistrom2005,
abstract = {The interpretative reports rendered by radiologists are the only tangible manifestation of their expertise, training, and experience. These documents are very often the primary means by which radiologists provide patient care. Radiology reports are extremely variable in form, content, and quality. The authors propose a framework for conceptualizing the reporting process and how it might be improved. This consists of standard language, a structured format, and consistent content. These attributes will be realized by modifying the clinical reporting process, including the creation, storage, transmission, and review of interpretative documents. The authors also point out that changes in training and evaluation must be a part of the process, because they are complementary to purely technical solutions. Copyright ?? 2005 American College of Radiology.},
author = {Sistrom, Chris L and Langlotz, Curtis P},
booktitle = {Journal of the American College of Radiology},
doi = {10.1016/j.jacr.2004.06.015},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sistrom, Langlotz - 2005 - A framework for improving radiology reporting.pdf:pdf},
isbn = {1558-349X (Electronic) 1546-1440 (Linking)},
issn = {15461440},
keywords = {RadLex,Radiology lexicon,Radiology reporting,Reporting and communication,Speech recognition,Structured reporting},
month = {feb},
number = {2},
pages = {159--267},
pmid = {17411786},
title = {{A framework for improving radiology reporting}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17411786},
volume = {2},
year = {2005}
}
@article{Turner2015,
abstract = {BACKGROUND: Chinese is the second most common language spoken by limited English proficiency individuals in the United States, yet there are few public health materials available in Chinese. Previous studies have indicated that use of machine translation plus postediting by bilingual translators generated quality translations in a lower time and at a lower cost than human translations., OBJECTIVE: The purpose of this study was to investigate the feasibility of using machine translation (MT) tools (eg, Google Translate) followed by human postediting (PE) to produce quality Chinese translations of public health materials., METHODS: From state and national public health websites, we collected 60 health promotion documents that had been translated from English to Chinese through human translation. The English version of the documents were then translated to Chinese using Google Translate. The MTs were analyzed for translation errors. A subset of the MT documents was postedited by native Chinese speakers with health backgrounds. Postediting time was measured. Postedited versions were then blindly compared against human translations by bilingual native Chinese quality raters., RESULTS: The most common machine translation errors were errors of word sense (40{\{}{\%}{\}}) and word order (22{\{}{\%}{\}}). Posteditors corrected the MTs at a rate of approximately 41 characters per minute. Raters, blinded to the source of translation, consistently selected the human translation over the MT+PE. Initial investigation to determine the reasons for the lower quality of MT+PE indicate that poor MT quality, lack of posteditor expertise, and insufficient posteditor instructions can be barriers to producing quality Chinese translations., CONCLUSIONS: Our results revealed problems with using MT tools plus human postediting for translating public health materials from English to Chinese. Additional work is needed to improve MT and to carefully design postediting processes before the MT+PE approach can be used routinely in public health practice for a variety of language pairs.},
annote = {MT+PE system was good for English -{\textgreater} Spanish translation but not so good for English -{\textgreater} Chinese as this study show (compared to just HT). This could be due to the different phrase structure of this languages.},
author = {Turner, Anne M and Dew, Kristin N and Desai, Loma and Martin, Nathalie and Kirchhoff, Katrin},
doi = {10.2196/publichealth.4779},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Turner et al. - 2015 - Machine Translation of Public Health Materials From English to Chinese A Feasibility Study.pdf:pdf},
issn = {2369-2960},
journal = {JMIR public health and surveillance},
keywords = {Chinese language,Consumer health,Health Promotion,Health literacy,Public Health,Public Health Informatics,limited English proficiency,machine translation,natural language processing,public health departments},
number = {2},
pages = {e17},
title = {{Machine Translation of Public Health Materials From English to Chinese: A Feasibility Study.}},
url = {http://ovidsp.ovid.com/ovidweb.cgi?T=JS{\%}7B{\&}{\%}7DPAGE=reference{\%}7B{\&}{\%}7DD=prem{\%}7B{\&}{\%}7DNEWS=N{\%}7B{\&}{\%}7DAN=27227135},
volume = {1},
year = {2015}
}
@article{Hassanpour2016,
abstract = {Objectives: The radiology report is the most important source of clinical imaging information. It documents critical information about the patient's health and the radiologist's interpretation of medical findings. It also communicates information to the referring physicians and records that information for future clinical and research use. Although efforts to structure some radiology report information through predefined templates are beginning to bear fruit, a large portion of radiology report information is entered in free text. The free text format is a major obstacle for rapid extraction and subsequent use of information by clinicians, researchers, and healthcare information systems. This difficulty is due to the ambiguity and subtlety of natural language, complexity of described images, and variations among different radiologists and healthcare organizations. As a result, radiology reports are used only once by the clinician who ordered the study and rarely are used again for research and data mining. In this work, machine learning techniques and a large multi-institutional radiology report repository are used to extract the semantics of the radiology report and overcome the barriers to the re-use of radiology report information in clinical research and other healthcare applications. Material and methods: We describe a machine learning system to annotate radiology reports and extract report contents according to an information model. This information model covers the majority of clinically significant contents in radiology reports and is applicable to a wide variety of radiology study types. Our automated approach uses discriminative sequence classifiers for named-entity recognition to extract and organize clinically significant terms and phrases consistent with the information model. We evaluated our information extraction system on 150 radiology reports from three major healthcare organizations and compared its results to a commonly used non-machine learning information extraction method. We also evaluated the generalizability of our approach across different organizations by training and testing our system on data from different organizations. Results: Our results show the efficacy of our machine learning approach in extracting the information model's elements (10-fold cross-validation average performance: precision: 87{\%}, recall: 84{\%}, F1 score: 85{\%}) and its superiority and generalizability compared to the common non-machine learning approach (p-value {\textless} 0.05). Conclusions: Our machine learning information extraction approach provides an effective automatic method to annotate and extract clinically significant information from a large collection of free text radiology reports. This information extraction system can help clinicians better understand the radiology reports and prioritize their review process. In addition, the extracted information can be used by researchers to link radiology reports to information from other data sources such as electronic health records and the patient's genome. Extracted information also can facilitate disease surveillance, real-time clinical decision support for the radiologist, and content-based image retrieval.},
author = {Hassanpour, Saeed and Langlotz, Curtis P.},
doi = {10.1016/j.artmed.2015.09.007},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hassanpour, Langlotz - 2016 - Information extraction from multi-institutional radiology reports.pdf:pdf},
issn = {18732860},
journal = {Artificial Intelligence in Medicine},
keywords = {Discriminative sequence classifier,Information extraction,Natural language processing,Radiology report narrative},
pages = {29--39},
pmid = {26481140},
title = {{Information extraction from multi-institutional radiology reports}},
volume = {66},
year = {2016}
}
@book{Manning2009,
author = {Manning, Christopher D. and Raghavan, Prabhakar and Sch{\"{u}}tze, Hinrich},
isbn = {978-0521865715},
pages = {581},
publisher = {Cambridge University Press Cambridge, England},
title = {{Introduction to Information Retrieval}},
year = {2009}
}
@article{Musleh2016,
abstract = {We present research towards bridging the language gap be-tween migrant workers in Qatar and medical staff. In particular, we present the first steps towards the development of a real-world Hindi-English machine translation system for doctor-patient communication. As this is a low-resource language pair, especially for speech and for the medical domain, our initial focus has been on gathering suitable training data from various sources. We applied a variety of methods ranging from fully automatic extraction from the Web to manual annotation of test data. Moreover, we developed a method for automatically augmenting the training data with synthetically generated variants, which yielded a very sizable improvement of more than 3 BLEU points absolute.},
author = {Musleh, Ahmad and Durrani, Nadir and Temnikova, Irina and Nakov, Preslav and Vogel, Stephan and Alsaad, Osama},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Musleh et al. - 2016 - Enabling Medical Translation for Low-Resource Languages.pdf:pdf},
journal = {Proceedings of the 16th Conference on Intelligent Text Processing and Computational Linguistics (CICLING)},
keywords = {Hindi,Machine Translation,doctor-patient communication,medical translation,resource-poor languages},
title = {{Enabling Medical Translation for Low-Resource Languages}},
year = {2016}
}
@article{Beitia2013,
abstract = {We evaluated the performance of LOINC{\textregistered} and RadLex standard terminologies for covering CT test names from three sites in a health information exchange (HIE) with the eventual goal of building an HIE-based clinical decision support system to alert providers of prior duplicate CTs. Given the goal, the most important parameter to assess was coverage for high frequency exams that were most likely to be repeated. We showed that both LOINC{\textregistered} and RadLex provided sufficient coverage for our use case through calculations of (a) high coverage of 90{\%} and 94{\%}, respectively for the subset of CTs accounting for 99{\%} of exams performed and (b) high concept token coverage (total percentage of exams performed that map to terminologies) of 92{\%} and 95{\%}, respectively. With trends toward greater interoperability, this work may provide a framework for those wishing to map radiology site codes to a standard nomenclature for purposes of tracking resource utilization.},
author = {Beitia, Anton Oscar and Kuperman, Gilad and Delman, Bradley N and Shapiro, Jason S},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Beitia et al. - 2013 - Assessing the performance of LOINC{\textregistered} and RadLex for coverage of CT scans across three sites in a health informati.pdf:pdf},
issn = {1942597X},
journal = {AMIA ... Annual Symposium proceedings / AMIA Symposium. AMIA Symposium},
pages = {94--102},
pmid = {24551324},
publisher = {American Medical Informatics Association},
title = {{Assessing the performance of LOINC{\textregistered} and RadLex for coverage of CT scans across three sites in a health information exchange.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24551324 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3900124 http://www.scopus.com/inward/record.url?eid=2-s2.0-84901268758{\&}partnerID=tZOtx3y1},
volume = {2013},
year = {2013}
}
@book{Somers2003,
author = {Somers, Harold},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Somers - 2003 - Computers and translation a translator{\&}{\#}39s guide.pdf:pdf},
pages = {366},
publisher = {John Benjamins Publishing Company},
title = {{Computers and translation: a translator's guide}},
url = {http://www.benjamins.nl/cgi-bin/t{\_}bookview.cgi?bookid=BTL 35},
year = {2003}
}
@article{Campos2017,
abstract = {MRA (Multilingual Report Annotator) is a web application that translates Radiology text and annotates it with RadLex terms. Its goal is to explore the solution of translating non-English Radiology reports as a way to solve the problem of most of the Text Mining tools being developed for English. In this brief paper we explain the language barrier problem and shortly describe the application. MRA can be found at https://github.com/lasigeBioTM/MRA.},
archivePrefix = {arXiv},
arxivId = {1704.01748},
author = {Campos, Lu{\'{i}}s and Couto, Francisco},
eprint = {1704.01748},
file = {:home/lcampos/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Campos, Couto - 2017 - MRA - Proof of Concept of a Multilingual Report Annotator Web Application.pdf:pdf},
month = {apr},
title = {{MRA - Proof of Concept of a Multilingual Report Annotator Web Application}},
url = {http://arxiv.org/abs/1704.01748},
year = {2017}
}
