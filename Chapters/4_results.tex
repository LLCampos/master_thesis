\label{chap4}

%TODO: intro to this chapter


% You may title this section "Methods" or "Models". 
% "Models" is not a valid title for PLoS ONE authors. However, PLoS ONE
% authors may use "Analysis" 
%\section*{Materials and Methods}
%\section{Methods}


% Results and Discussion can be combined.
\section{NER Gazetteer-based approach}

Let's check the number of terms found by document.


\begin{table}[ht]
\centering
\begin{tabular}{lrrr}
\toprule
\textbf{Translation}   &   \textbf{Direct Match} &   \textbf{All Match} &   \textbf{Best Match} \\
\midrule
 \textbf{Human}         &         119.98 &      178.08 &       145.11 \\
 
 \textbf{Yandex}        &         115.34 &      172.66 &       144.19 \\
 
 \textbf{Google}        &         120.15 &      177.89 &       146.70 \\
 
 \textbf{Unbabel}       &         121.19 &      178.79 &       148.09 \\
 
\bottomrule
\end{tabular} 
\caption{Number of RadLex terms found by document}
\label{table:terms_by_document}
\end{table}

One of the highlights here is that the All Match approach consistently found more terms than the Best Match approach, which itself found more terms than the Direct Match approach. This makes sense since the All Match approach its the most liberal one in what it considers to be a mention of a RadLex term. The Best Match approach is more conservative than the All Match approach but less than the Direct Match approach, considering lexical variations and word reordering, for example. Let's now look at the differences between translations.

The highlights are that there were less RadLex terms in the Yandex translations than in any of the others. On average, there were less between 0.92 and 5.42 RadLex terms in each document, compared with the Human translations. On the other hand, on average there are more between 0.71 and 2.98 RadLex terms in each Unbabel translation than in each Human translation. The Unbabel's translations were the ones with the most RadLex terms. 

% Best Match Micro
\begin{table}[!htp]
\centering
\begin{tabular}{@{}lccc@{}}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{\textbf{Translation}}} & \multicolumn{3}{c}{\textbf{Best Match}}                            \\ \cmidrule(l){2-4}
\multicolumn{1}{c}{}          & \textbf{Micro P}     & \textbf{Micro R}      & \textbf{Micro F}          \\ \midrule
\textbf{Yandex}   & 0.829                & 0.824                 & 0.826                   \\ 

\textbf{Google}   & 0.851                & 0.861                 & 0.856                   \\ 

\textbf{Unbabel}  & 0.853                & 0.87                 & 0.862         \\ \bottomrule 

\end{tabular}%
\caption{Micro-Evaluation of how close the Best Match annotations of MT and MT+PE translation are to the annotations of HT}
\label{table: Results Best Match Micro}
\end{table}


% All Match Micro
\begin{table}[!htp]
\centering
\begin{tabular}{@{}lccc@{}}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{\textbf{Translation}}} & \multicolumn{3}{c}{\textbf{All Match}}                            \\ \cmidrule(l){2-4}
\multicolumn{1}{c}{}          & \textbf{Micro P}     & \textbf{Micro R}      & \textbf{Micro F}          \\ \midrule
\textbf{Yandex}   & 0.839                & 0.814                 & 0.826                   \\ 

\textbf{Google}   & 0.862                & 0.861                 & 0.862                   \\ 

\textbf{Unbabel}  & 0.864                & 0.868                 & 0.866         \\ \bottomrule 

\end{tabular}%
\caption{Micro-Evaluation of how close the All Match annotations of MT and MT+PE translation are to the annotations of HT}
\label{table: Results All Match Micro}
\end{table}


% Direct Match Micro
\begin{table}[!htp]
\centering
\begin{tabular}{@{}lccc@{}}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{\textbf{Translation}}} & \multicolumn{3}{c}{\textbf{Direct Match}}                            \\ \cmidrule(l){2-4}
\multicolumn{1}{c}{}          & \textbf{Micro P}     & \textbf{Micro R}      & \textbf{Micro F}          \\ \midrule
\textbf{Yandex}   & 0.841                & 0.809                 & 0.825                   \\ 

\textbf{Google}   & 0.864                & 0.865                 & 0.864                   \\ 

\textbf{Unbabel}  & 0.867                & 0.876                 & 0.872         \\ \bottomrule 

\end{tabular}%
\caption{Micro-Evaluation of how close the Direct Match annotations of MT and MT+PE translation are to the annotations of HT}
\label{table: Results Direct Match Micro}
\end{table}


% Best Match Macro
\begin{table}[!htp]
\centering
\begin{tabular}{@{}lccc@{}}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{\textbf{Translation}}} & \multicolumn{3}{c}{\textbf{Best Match}}                            \\ \cmidrule(l){2-4}
\multicolumn{1}{c}{}          & \textbf{Macro P}     & \textbf{Macro R}      & \textbf{Macro F}          \\ \midrule
\textbf{Yandex}   & 0.823                & 0.823                 & 0.823                   \\ 

\textbf{Google}   & 0.843                & 0.855                 & 0.848                   \\ 

\textbf{Unbabel}  & 0.845                & 0.865                 & 0.854         \\ \bottomrule 

\end{tabular}%
\caption{Macro-Evaluation of how close the Best Match annotations of MT and MT+PE translation are to the annotations of HT}
\label{table: Results Best Match Macro}
\end{table}


% All Match Macro
\begin{table}[!htp]
\centering
\begin{tabular}{@{}lccc@{}}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{\textbf{Translation}}} & \multicolumn{3}{c}{\textbf{All Match}}                            \\ \cmidrule(l){2-4}
\multicolumn{1}{c}{}          & \textbf{Macro P}     & \textbf{Macro R}      & \textbf{Macro F}          \\ \midrule
\textbf{Yandex}   & 0.833                & 0.817                 & 0.823                   \\ 

\textbf{Google}   & 0.856                & 0.855                 & 0.854                   \\ 

\textbf{Unbabel}  & 0.858                & 0.862                 & 0.859         \\ \bottomrule 

\end{tabular}%
\caption{Macro-Evaluation of how close the All Match annotations of MT and MT+PE translation are to the annotations of HT}
\label{table: Results All Match Macro}
\end{table}


% Direct Match Macro
\begin{table}[!htp]
\centering
\begin{tabular}{@{}lccc@{}}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{\textbf{Translation}}} & \multicolumn{3}{c}{\textbf{Direct Match}}                            \\ \cmidrule(l){2-4}
\multicolumn{1}{c}{}          & \textbf{Macro P}     & \textbf{Macro R}      & \textbf{Macro F}          \\ \midrule
\textbf{Yandex}   & 0.833                & 0.811                 & 0.820                   \\ 

\textbf{Google}   & 0.853                & 0.859                 & 0.855                   \\ 

\textbf{Unbabel}  & 0.857                & 0.871                 & 0.863         \\ \bottomrule 

\end{tabular}%
\caption{Macro-Evaluation of how close the Direct Match annotations of MT and MT+PE translation are to the annotations of HT}
\label{table: Results Direct Match Macro}
\end{table}


The annotations of the Google MT translation are closer to the ones of the HT translation than the ones of the Yandex MT translation. This could be just because the human translators used Google Translator to help them in their translation process, and so the translation is more similar than compared with the Yandex translation. This argument loses strength is we assume Google Translate translation outputs changed since the articles were human translated (publication years of the articles used range from 2003 to 2013), but I could not found data on this. 

The annotations of Unbabel MT+PE translations are by itself closer to the ones of Human translations than the ones of Google translations. This is not surprising since the Post-Editing at Unbabel is done on the Google MT translation so we would expect the Unbabel results to the Human results than the Google results. The question is how much closer. 

In the Introduction to thesis I've proposed the following hypothesis:

\begin{description}
	\item[Hypothesis:] MT+PE is a good trade-off between quality and cost, compared with MT and HT, for translating radiology reports for the purpose of identifying RadLex terms. 
\end{description}


I've written that for this to be true, "MT+PE quality for the task at hand has to be close enough to HT quality". The MT+PE translation is indeed the one closest to HT quality at the task at hand. This is not surprising, since that, from the types of translation being compared against the Human Translation, MT+PE is the only one which also involves humans in the translation process. But is it close enough to HT? This really would depend on the application of the annotations, but the results show that MT+PE translation results are far from being identical to the HT ones.  

I also add the condition that for the hypothesis to be true, "MT+PE quality for the task at hand has to be better than MT quality, enough to compensate its higher cost". It is better indeed but if it is enough would also depend on the practical application of the annotations. Having said this, the results show that results from MT+PE translations are not very much closer to the results of HT translation comparing with Google MT translation. 

\includegraphics{SupportFiles/plots/All_Match_Macro_plot.pdf}


\section{Discussion}





\section{Conclusions}




  
