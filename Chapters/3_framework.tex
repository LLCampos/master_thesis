\label{chap3}

\section{MRRAD (Multilingual Radiology Research Articles Dataset) Corpus}

To the best of my knowledge there is no parallel corpus of Radiology reports. So I created a Portuguese-English parallel corpus of research articles related to Radiology, assuming that the writing style and content of these research articles are similar to Radiology reports. For each research article the MRRAD corpus contains:

\begin{enumerate}
\item Original Portuguese text
\item Human Translated English text
\item Machine Translated English text (Yandex) 
\item Machine Translated English text (Google) 
\item Machine Translation + Post-Editing English text (Google + Unbabel) 
\end{enumerate}

\noindent In the next few lines I will explain how I have constructed the corpus. 

\subsection{Web Crawl of the articles (1,2)}

To obtain a list of research articles related to Radiology, that were available both in English and in Portuguese, I used used the  NCBO Entrez Programming Utilities (E-utilities)\footnote{\url{https://www.ncbi.nlm.nih.gov/books/NBK25501/}} to query the PubMed database with the search query “portuguese[Language] AND english[Language] AND radiography[MeSH Major Topic] AND hasabstract[text]” (search done on Dec 11, 2016). The last filter is used to avoid getting texts for which only the title is available. 

Then I programmatically crawled each article PubMed page to get the URL where the full article could be found. Most of the articles were hosted in SciELO\footnote{\url{http://www.scielo.br/}} and only articles hosted in there were included in the corpus. More, only articles for which the original language is Portuguese are included in the corpus. 

Finally, I programmatically crawled the SciELO pages for each article to get both the original Portuguese texts and the corresponding English translations. From the HTML of each page I extracted everything from the abstract until, but not including, the references/bibliography.

Three of the articles were surveys, not containing much vocabulary about Radiology (PMIDs: 19936506, 22002140, 23515770). They were excluded from the corpus. Other two contained encoding problems and were also excluded (PMIDs: 21793046 and 24263777).

The final result is a parallel corpus of 51 articles, distributed by journal as shown in Table \ref{table:articles_by_journal}.

\begin{table}[ht]
\centering
\caption{Number of articles by journal in parallel corpus}
\label{table:articles_by_journal}
\begin{tabular}{@{}ll@{}}
\toprule
\multicolumn{1}{c}{\textbf{Journal}}                 & \textbf{Number Of Articles} \\ \midrule
Arquivos Brasileiros de Cardiologia         & 24                          \\
Jornal Brasileiro de Pneumologia            & 14                          \\
Revista do Colégio Brasileiro de Cirurgiões & 4                           \\
Brazilian Journal of Otorhinolaryngology    & 2                           \\
Arquivos Brasileiros de Cirurgia Digestiva  & 2                           \\
Revista Brasileira de Cirurgia Cardiovascular        & 2                           \\
Jornal da Sociedade Brasileira de Fonoaudiologia     & 1                           \\
Einstein (São Paulo)                                 & 1                           \\
Revista Brasileira de Reumatologia                   & 1                           \\ \bottomrule
\end{tabular}
\end{table}

To give a sense of the corpus size, the human English translations have a total of 163,423 words\footnote{Tokenization done by NLTK's word\_tokenize function (\url{http://www.nltk.org/})} the longer article having 12,451 and the smaller 848. The articles have an average of 3,204 words each. 

\subsection{Note On Human Translations}

It is not known for sure how exactly the original human translations were performed, since some of the articles are not recent and some of the journals did not answer my request for more information about the translation, but all the answers received mentioned the use of specialized translation services. Having said this, it is being assumed that the translations are of high quality since they are published by scientific magazines. 

\subsection{Yandex Translation (3)}

I used Yandex's free Translate API\footnote{\url{https://tech.yandex.com/translate/}} to machine translate the Portuguese version of the articles. Yandex is a Russian company which, among other things, sells automatic translation services, but it has a limited free service. It currently uses a Statistical approach to Machine Translation. Each translation request had a limit of 10,000 characters so I developed software to break the text to various pieces, without breaking the text in the middle of sentences, send the translation request for each piece and then join everything back. 

\subsection{Google and Unbabel Translation (4,5)}

Both MT with Google and MT+PE with Unbabel were obtained using Unbabel's API\footnote{\url{http://developers.unbabel.com/}}. I obtained Google’s Statistical Machine translation using the \textit{mt\_translation} endpoint of the API and Unbabel’s Machine Translation + Post-Editing using the \textit{translation} API’s endpoint.  The requests for Unbabel Translations have a limit of words, so I used a software similar to the one utilized for the Yandex Translations. 

\section{Annotation}

All the English versions of the articles in the corpus were annotated three times with RadLex terms, one time using a direct matching approach and two using two of the built-in matching strategies provided by NOBLE Coder. I am calling the three approaches Direct Match\footnote{See \ref{Named-entity Recognition}}, All Match and Best Match\footnote{See \ref{NOBLE Coder}}. Three different kinds of approaches were used to check what effect the annotation strategy have on the results, if any.

\subsection{Direct Match - Annotation with Open Biomedical Annotator}

The articles were annotated with OBA using the REST API\footnote{\url{http://data.bioontology.org/documentation\#nav_annotator}}. The default parameters were used, namely the ones shown in Table \ref{table-ncbo-parameters}.

\begin{table}[h]
\centering
\caption{OBA parameters used}
\label{table-ncbo-parameters}
\begin{tabular}{|l|l|}
\hline
\multicolumn{1}{|c|}{\textbf{Parameter}}       & \multicolumn{1}{c|}{\textbf{Value}} \\ \hline
\multicolumn{1}{|c|}{expand\_class\_hierarchy} & false                               \\ \hline
expand\_mappings                               & false                               \\ \hline
minimum\_match\_length                         & 3                                   \\ \hline
exclude\_numbers                               & false                               \\ \hline
whole\_word\_only                              & true                                \\ \hline
exclude\_synonyms                              & false                               \\ \hline
longest\_only                                  & false                               \\ \hline
\end{tabular}
\end{table}

\subsection{All Match and Best Match - Annotation with NOBLE Coder}

NOBLE Coder was chosen against others similar tools because of its comparable quality and higher ease of use. Each of the articles was annotated twice with this tool, using two different matching strategies, Best match and All match (these strategies are described in section \ref{NOBLE Coder}).

More information on how NOBLE Coder was used can be found at the MRRAD GitHub repository\footnote{\url{https://github.com/lasigeBioTM/MRRAD/blob/master/notes_on_dataset_creation/using_noble_coder.md}}.

\subsection{Other Work on Annotations}

\subsubsection{Participation in BioCreative V.5}
\label{biocreative}

While working on the main question of this thesis, I also participated in the CEMP (Chemical Entity Mention recognition) and TIPS (Technical interoperability and performance of annotation servers) tasks of BioCreative V.5\footnote{\url{http://www.biocreative.org/news/biocreative-v5/biocreative-v5-cfp/}}, as member of a team composed by other members of the LaSIGE group\footnote{\url{http://lasige.di.fc.ul.pt/}}.

The goal of the CEMP task was to identify and localize mentions of chemical entities (a NER task) in titles and abstracts of patents. For this task the team used two systems, IBEnt\footnote{\url{https://github.com/lasigeBioTM/IBEnt}} (Identifying Biomedical Entities) and MER\footnote{\url{https://github.com/lasigeBioTM/MER}} (Minimal Named-Entity Recognizer). IBEnt uses a Machine Learning approach and MER uses a lexicon-based approach. 

The TIPS Task, on the other hand, had has its objective the development of annotation servers. For this we used the MER tool mention before. 

%TODO [Write about results, and cite BioCreative proceedings papers]

The participation in this competitions was relevant since it made me more familiar with annotation systems.

\subsubsection{Crowd Annotations}

One alternative to use automated NER tools is to crowdsource the annotation process. That is, instead of having automatic tools or medical experts annotating documents, its possible to have members of the general public doing it. In theory, this will be cheaper and faster than using medical experts and the annotations will have a better quality than just using automatic tools.

If you think about it, this is analogue to use crowdsourced MT+PE instead of MT or HT. One idea is even to use automatic annotations as a basis for the crowd members annotations. This is the strategy used on recent work on crowd NER: \citep{Li2016} studied the use of the crowd to validate the automatic identification of chemical-induced disease relations in PubMed abstract while \citep{Burger2014} did something similar for gene–mutation relations.

I helped in the organization and presentation of a Workshop on Biomedical Text Mining, in which we experimented with having participants of the Workshop annotating Radiology reports with RadLex terms, using annotations by OBA as a basis: in the annotation interface, the users could see the annotations by OBA and could accept or reject them or add new annotations. Currently, we are working on another experiment of the same kind, but this time using annotation of HPO (Human Phenotype Ontology) terms and having a gold-standard by experts. This will allow us compare the annotations of the crowd with the expert's annotations.


\section{Evaluation}

For each document and annotation approach I created the set of the RadLex terms (identified by their RIDs) that were found in that document with that annotation approach. This is the data used in the assessment of translation solutions that follows.

The RadLex terms identified in each MT or MT+PE translated article were compared against the ones identified in the corresponding HT translated article, which was considered the gold standard. Both Micro- and Macro- Precision, Recall and F-scores were calculated. This was done for each matching approach. 

To facilitate the understanding of the results, I will now walk trough a short example. Consider that we have one Portuguese document and corresponding HT English translation and MT English translation. Four terms of interest were identified in the HT translation, \emph{bone, cell, finger, colon}\footnote{I use here human understandable names instead of RIDs so that the example is easier to follow}. This is going to be our gold standard. In the MT translation, two terms of interest were found, {brain, bone}. One of these terms is also in the gold standard, which means TP = 1, but the other term is not, FP = 1. In the gold standard there are three terms that were not found in the MT translation, which means FN = 3. After calculations (see \ref{Evaluation Metrics}), this gives us a Precision score of 0.5, a Recall score of 0.25 and F-Score of 0.33. These methods measure how similar are the terms annotated on the MT or MT+PE texts to the terms annotated on the HT texts.


\section{MRA - Proof of Concept of a Multilingual Report Annotator Web Application}

As an example of what could be done when integrating translation with medical applications I have built a web application called MRA \citep{Campos2017}\footnote{\url{https://github.com/lasigeBioTM/MRA}} which translates text from six languages to English, followed by annotation with RadLex terms. 

The flow of the application goes like this. The doctor or researcher uploads a text file containing a medical report to the application, and the text of this report is sent to Unbabel's translation services. In this prototype only machine translation is being used, for demonstration purposes. In a real-life scenario, human translation could also be used for more reliable results. So, the text is sent to translation and after a while (approximately 2 minutes, to simulate a real human and machine translation) the translation is ready.

Then, the translated text is sent to OBA services. After
this is done it is possible to explore the annotations in the translated text. The interface of the annotations was partly inspired by a similar project called LexMap \citep{Hostetter2015}.

The idea is that this application can be used to bootstrap other, more useful applications. It was inclusively demonstrated to a clinical facility during talks to create a partnership between the facility and Faculty of Sciences.

 