\label{chap3}

\section{Portuguese-English Parallel Corpus}

For the purpose of this work, I've created a Portuguese-English parallel corpus of research articles related to radiology. For each research article there is:

\begin{enumerate}
\item Original Portuguese text
\item Human Translated English text
\item Machine Translated English text (Yandex) 
\item Machine Translated English text (Google) 
\item Machine Translation + Post-Editing English text (Google + Unbabel) 
\end{enumerate}

\noindent In the next few lines I will explain how I've constructed the corpus. 

\subsection{Web Crawl of the articles (1,2)}

First, I needed a list of articles related to radiography that were available both in English and in Portuguese. To get this list I’ve used the  NCBO Entrez Programming Utilities (E-utilities)\footnote{\url{https://www.ncbi.nlm.nih.gov/books/NBK25501/}} to query the PubMed database with the search query “portuguese[Language] AND english[Language] AND radiography[MeSH Major Topic] AND hasabstract[text]” (search done on 11/12/2016). The last filter is used to avoid getting texts for which only the title is available. 

Then I programmatically crawled each article PubMed page to get the URL where the full article could be found. Most of the articles were hosted in SciELO\footnote{\url{http://www.scielo.br/}} so for the sake of consistency I've only included in the corpus articles hosted in there. 

For the purposes of this work, it made sense to only include articles for which the original language is Portuguese, so I've also filtered the corpus by this parameter. 

Finally, I've programmatically crawled the articles SciELO pages to get both language versions of articles text. I've extracted from the HTML everything from the abstract until, but not including, the references/bibliography.

Three of the article contained were about surveys, not containing much vocabulary about radiology. They were excluded from the corpus. Other two contained encoding problems and were also excluded.

What is left is a parallel corpus of 51 articles, distributed by journal in the following way:

\begin{table}[ht]
\centering
\caption{Number of articles by journal in parallel corpus}
\label{table:articles_by_journal}
\begin{tabular}{@{}ll@{}}
\toprule
\multicolumn{1}{c}{\textbf{Journal}}                 & \textbf{Number Of Articles} \\ \midrule
Arquivos Brasileiros de Cardiologia         & 24                          \\
Jornal Brasileiro de Pneumologia            & 14                          \\
Revista do Colégio Brasileiro de Cirurgiões & 4                           \\
Brazilian Journal of Otorhinolaryngology    & 2                           \\
Arquivos Brasileiros de Cirurgia Digestiva  & 2                           \\
Revista Brasileira de Cirurgia Cardiovascular        & 2                           \\
Jornal da Sociedade Brasileira de Fonoaudiologia     & 1                           \\
Einstein (São Paulo)                                 & 1                           \\
Revista Brasileira de Reumatologia                   & 1                           \\ \bottomrule
\end{tabular}
\end{table}

The corpus has a total of 163423 words\footnote{Tokenization done by NLTK's word\_tokenize function (\url{http://www.nltk.org/})} the longer article having 12451 and the smaller 848. The articles have an average of 3204 words each. 

\subsection{Note On Human Translations}

It is not known for sure how exactly the original human translations were done, since some of the articles are not recent and some of the journals did not answer my emails questioning about this, but all the answers received mentioned the use of specialized translation services. Having said this, we assume that the translations are of high quality since they were published by scientific magazines. 

\subsection{Yandex Translation (3)}

The Portuguese version of the articles were machine translated using Yandex's free Translate API\footnote{\url{https://tech.yandex.com/translate/}}. Each translation request had a limit of 10000 characters so an algorithm was used to break the text to various pieces, without breaking the text in the middle of sentences, send the translation request for each piece and then join everything back.

\subsection{Google and Unbabel Translation (4,5)}

Both MT with Google and MT+PE with Unbabel were obtained using Unbabel's API\footnote{\url{http://developers.unbabel.com/}}. The requests for Unbabel Translations have a limit of words, so an algorithm similar used for the Yandex Translations was used. 

\section{Annotation}

All the English versions of the articles in the corpus were annotated thrice with RadLex terms, one time using a direct matching approach and two using two of the built-in matching strategies provided by NOBLE Coder. I'm calling the three approaches Direct Match\footnote{See \ref{Named-entity Recognition}}, All Match and Best Match\footnote{See \ref{NOBLE Coder}}. Three different kinds of approaches were used to check what effect the annotation strategy have on the results.

Each class of the RadLex ontology has a \textit{preferred name} and a list of synonyms. For all the approaches the output of each annotation process consists in the set of the preferred names of the RadLex terms that are mentioned in the corresponding article. I normalize all the mentions to the preferred name so that a use of the preferred name in one translation and the use of one of the synonyms in another translation are considered mentions of the same term. 

\subsection{Direct Match - Annotation with NCBO Annotator}

The articles were annotated with the NCBO Annotator using the REST API\footnote{\url{http://data.bioontology.org/documentation\#nav_annotator}}. The default parameters were used, namely:

\begin{table}[h]
\centering
\caption{NCBO Annotator parameters used}
\label{table-ncbo-parameters}
\begin{tabular}{|l|l|}
\hline
\multicolumn{1}{|c|}{\textbf{Parameter}}       & \multicolumn{1}{c|}{\textbf{Value}} \\ \hline
\multicolumn{1}{|c|}{expand\_class\_hierarchy} & false                               \\ \hline
expand\_mappings                               & false                               \\ \hline
minimum\_match\_length                         & 3                                   \\ \hline
exclude\_numbers                               & false                               \\ \hline
whole\_word\_only                              & true                                \\ \hline
exclude\_synonyms                              & false                               \\ \hline
longest\_only                                  & false                               \\ \hline
\end{tabular}
\end{table}

\subsection{All Match and Best Match - Annotation with NOBLE Coder}

NOBLE Coder was chosen against others similar tools because of it's comparable quality and higher ease of use. Each of the articles was annotated twice with this tool, using two different matching strategies, Best match and All match.

The commands used to annotate the reports were these:


\begin{lstlisting}[language=bash]
$ java -jar NobleCoder-1.0.jar -terminology radlex \
-input [portuguese reports path] -output [output path] \
-search all-match\textit{

$ java -jar NobleCoder-1.0.jar -terminology radlex \
-input [portuguese reports path] -output [output path] \
-search best-match
\end{lstlisting}


The RadLex ontology .owl file had to be edited before it could be correctly processed and uploaded to NOBLE Coder. In the original .owl file the properties  "Preferred\_name" and "Synonym" are considered to be \textit{DatatypeProperty} but I had to change both to \textit{AnnotationProperty}. That is, where in the file was


\begin{lstlisting}[language=xml]
<owl:DatatypeProperty rdf:ID="Preferred_name">
</owl:DatatypeProperty>
\end{lstlisting}


I've had to change it to:


\begin{lstlisting}[language=xml]
<owl:AnnotationProperty rdf:ID="Preferred_name">
</owl:AnnotationProperty>
\end{lstlisting}


And the analogous thing for the "Synonym" property.

\section{Evaluation}

The annotations of each MT or MT+PE translated article were compared against the annotations of corresponding HT translated article, which was considered a gold standard. Both Micro- and Macro- Precision, Recall and F1-scores were calculated. This was done for each matching approach. 

To facilitate the understanding of the results, I will now walk trough a short example for one document. Consider that we have two Portuguese documents and for each one we have a HT English translation and a MT English translation. There were found 4 terms of interest in the HT translation, {bone, cell, finger, colon}. This is going to be our gold standard. In the MT translation, 2 terms of interest were found, {brain, bone}. One of these terms is also in the gold standard so there is 1 True Positive, but the other term is  not, so that is 1 False Positive. In the gold standard there are 3 terms that were not found in the MT translation. That is 3 False Negatives. After calculations (see \ref{Evaluation Metrics}), this gives us a Precision score of 0.5, a Recall score of 0.25 and F-Score of 0.33. 

These methods measure how similar are the terms annotated on the MT or MT+PE texts to the terms annotated on the HT texts. They don't say nothing about the quality of the annotations, however is that measured. 





 