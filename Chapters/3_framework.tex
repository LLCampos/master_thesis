\label{chap3}

\section{Portuguese-English Parallel Corpus}

For the purpose of this work, I've created a Portuguese-English parallel corpus of research articles related to radiology. For each research article there is:

\begin{enumerate}
\item Original Portuguese text
\item Human Translated English text
\item Machine Translated English text (Yandex) 
\item Machine Translated English text (Google) 
\item Machine Translation + Post-Editing English text (Google + Unbabel) 
\end{enumerate}

\noindent In the next few lines I will explain how I've constructed the corpus. 

\subsection{Web Crawl of the articles (1,2)}

First, I needed a list of articles related to radiography that were available both in English and in Portuguese. To get this list I’ve used the  NCBO Entrez Programming Utilities (E-utilities)\footnote{https://www.ncbi.nlm.nih.gov/books/NBK25501/} to query the PubMed database with the search query “portuguese[Language] AND english[Language] AND radiography[MeSH Major Topic] AND hasabstract[text]” (search done on 11/12/2016). The last filter is used to avoid getting texts for which only the title is available. 

Then I programmatically crawled each article PubMed page to get the URL where the full article could be found. Most of the articles were hosted in SciELO\footnote{http://www.scielo.br/} so for the sake of consistency I've only included in the corpus articles hosted in there. 

For the purposes of this work, it made sense to only include articles for which the original language is Portuguese, so I've also filtered the corpus by this parameter. 

Finally, I've programmatically crawled the articles SciELO pages to get both language versions of articles text. I've extracted from the HTML everything from the abstract until, but not including, the references/bibliography.

Three of the article contained were about surveys, containing to much vocabulary about radiology. They were excluded from the corpus.

What is left is a parallel corpus of 53 articles, distributed by journal in the following way:

\begin{table}[ht]
\centering
\caption{Number of articles by journal in parallel corpus}
\label{table:articles_by_journal}
\begin{tabular}{@{}ll@{}}
\toprule
\multicolumn{1}{c}{\textbf{Journal}}                 & \textbf{Number Of Articles} \\ \midrule
Arquivos Brasileiros de Cardiologia         & 26                          \\
Jornal Brasileiro de Pneumologia            & 14                          \\
Revista do Colégio Brasileiro de Cirurgiões & 4                           \\
Brazilian Journal of Otorhinolaryngology    & 2                           \\
Arquivos Brasileiros de Cirurgia Digestiva  & 2                           \\
Revista Brasileira de Cirurgia Cardiovascular        & 2                           \\
Jornal da Sociedade Brasileira de Fonoaudiologia     & 1                           \\
Einstein (São Paulo)                                 & 1                           \\
Revista Brasileira de Reumatologia                   & 1                           \\ \bottomrule
\end{tabular}
\end{table}

\subsection{Note On Human Translations}

It is not known for sure how exactly the original human translations were done, since some of the articles are not recent and some of the journals did not answer my emails questioning about this, but all the answers received mentioned the use of specialized translation services. Having said this, we assume that the translations are of high quality since they were published by scientific magazines. 

\subsection{Yandex Translation (3)}

The Portuguese version of the articles were machine translated using Yandex's free Translate API\footnote{https://tech.yandex.com/translate/}. Each translation request had a limit of 10000 characters so an algorithm was used to break the text to various pieces, without breaking the text in the middle of sentences, send the translation request for each piece and then join everything back.

\subsection{Google and Unbabel Translation (4,5)}

Both MT with Google and MT+PE with Unbabel were obtained using Unbabel's API\footnote{http://developers.unbabel.com/}. The requests for Unbabel Translations have a limit of words, so an algorithm similar used for the Yandex Translations was used. 

\section{Annotation}

All the English versions of the articles in the corpus were annotated thrice, one time using a direct matching approach and two using two of the built-in matching strategies provided by NOBLE Coder. 

Each class of the RadLex ontology has a \textit{preferred name} and a list of synonyms. For all the cases the output of each annotation consists in the set of the preferred names of the terms of RadLex that are mentioned in the corresponding article. I normalize all the mentions to the preferred name so that a use of the preferred name in one translation and the use of one of the synonyms in another translation are considered mentions of the same term. 

\subsection{Annotation with Radlex Annotator}

The articles were annotated with terms from RadLex using a direct match strategy with an alternative to NCBO Annotator\footnote{http://bioportal.bioontology.org/annotator} that I've developed. This tool has the advantage of doing away with the dependence on a external service like NCBO Annotator. Although it is possible to have an instance of the Annotator on your machine, it has computationally heavy requirements, too much for the simple task of annotating terms on a text. The local system has other advantages. First, it annotates terms that the NCBO system doesn’t. For example, the local system annotates “benign” in “•Benign” (note the little black point) but NCBO's doesn’t. More, NCBO’s system annotates terms that makes no sense to annotate, like “Class”, which is a metaclass and not really a radiology-related term. Having said this, the local system has a “annotate whole words only” using a regex expression, so it doesn’t annotate the term “artery” in “(…)\_artery\_(…)", for example,  something that the NCBO’s system does. The local system is also way slower than NCBO's one, even though it is local. This is not too surprising since the local system was not developed having speed performance in mind. 

The local system also annotates some terms in duplicate: consider the RadLex term “minimum intensity projection”, which has as a synonym the expression “Minimum Intensity Projection”, which is the same as the preferred name, but with a different case. If this expression is found on the text, the local system will annotate it twice (it is case insensitive), one for the preferred name, other for the synonym. NCBO’s system only annotates it once. 

Other than this, from the tests I’ve made, the results are equivalent to the NCBO’s system. Even the output is similar, so that the processing is easier for the ones already familiar with the NCBO’s system. This tool is available on GitHub\footnote{https://github.com/LLCampos/radlex\_annotator} and I'm going to mention it as RadLex Annotator from now on.

\subsection{Annotation with NOBLE Coder}

NOBLE Coder was chosen against others similar tools because of it's comparable quality and higher ease of use. Each of the articles was annotated twice with this tool, using two different matching strategies, \textit{Best match} and \textit{All match}.

The commands used to annotate the reports were these:


\begin{lstlisting}[language=bash]
$ java -jar NobleCoder-1.0.jar -terminology radlex \
-input [portuguese reports path] -output [output path] \
-search all-match

$ java -jar NobleCoder-1.0.jar -terminology radlex \
-input [portuguese reports path] -output [output path] \
-search best-match
\end{lstlisting}


The RadLex ontology .owl file had to be edited before it could be correctly processed and uploaded to NOBLE Coder. In the original .owl file the properties  "Preferred\_name" and "Synonym" are considered to be \textit{DatatypeProperty} but I had to change both to \textit{AnnotationProperty}. That is, where in the file was


\begin{lstlisting}[language=xml]
<owl:DatatypeProperty rdf:ID="Preferred_name">
</owl:DatatypeProperty>
\end{lstlisting}


I've had to change it to:


\begin{lstlisting}[language=xml]
<owl:AnnotationProperty rdf:ID="Preferred_name">
</owl:AnnotationProperty>
\end{lstlisting}


And the analogous thing for the "Synonym" property.

\section{Evaluation}

The annotations of each MT or MT+PE translated article were compared against the annotations of corresponding HT translated article, which was considered a gold standard. Both Micro- and Macro- Precision, Recall and F1-scores were calculated. This was done for each matching approach. 

These methods measure how similar are the terms annotated on the MT or MT+PE texts to the terms annotated on the HT texts. They don't say nothing about the quality of the annotations, however is that measured. 





 