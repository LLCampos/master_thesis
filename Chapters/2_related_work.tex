\label{chap2}

% Write about NLP
% write about Machine Learning?
% Two systems I've built
% What is an ontology
% Radiology Reports - text mining research on this

\section{Electronic Health Records}



\section{Text Mining}

Text Mining can mean different things for different people \citep{Hotho2005}, but in this dissertation I will assume that it consists on the automatic extraction of useful information from unstructured text documents. It can be used, for example, to help researchers cope with information overload \citep{Cohen2005a} due to the big volume of scientific data in the form of unstructured literature. More related to this thesis, it can also be used to extract information from free-text radiology reports \citep{Pons2016}.

Because Text Mining has to manipulate text, it is not too surprising that it borrows tools from Natural Language Processing (NLP), a research fields that seeks to improve computational understanding of natural language. 

(present next sections)

\subsection{Name-entity Recognition}

Named-entity recognition (NER) is a task of NLP that has the goal of locate and classify all the named-entities in a certain document. Named-entities are elements of the text that belong to one of certain predefined classes. For example, the phrase \textit{atrial fibrillation} is a named-entity that belongs to the class \textit{Disease}. The task of NER can be further divided in two subtasks: identify where in the text are the named-entities and classify the named-entities.

The approaches of NER can be divided into three categories \citep{Mansouri2008}: Rule-based approaches, Machine Learning based approaches and hybrid approaches. 

\begin{itemize}
\item In rule-based approaches the identification and classification subtasks are based on rules crafted by humans. Usually domain specific. 
\item In ML based approaches the subtasks are turned into classification problems and machine learning algorithms are used to identify and classify named-entities. These approaches are easily ported to different domains other than the ones they were originally developed to be applied on.
\item Hybrids approaches combines the two last approaches.
\end{itemize}

Dictionary based-approaches are a subset of the rule-based approaches. In this approach we already have a list of the named-entities that we want to identify in the text. This list of terms can be called "dictionary", "vocabulary", "lexicon" or "terminology", for example. The goal of the dictionary based-approaches is then to identify, in text, mentions of terms presented in the dictionary. This could be done by direct matching, as implemented by the Open Biomedical Annotator \footnote{http://bioportal.bioontology.org/annotator} \citep{Jonquet2009}. In this strategy, the system only tries to find in text terms that are also in the dictionary, not considering, for example, lexical variations. The recall can be lower than expected because lexical variants (like plurals), abbreviations and partial matchings of dictionary terms are not recognized in the text. For this purpose, more complex tools like NOBLE Coder\footnote{http://noble-tools.dbmi.pitt.edu/} \citep{Tseytlin2016} or Concept Mapper \citep{Stewart} can be used.

\subsection{Application of Text Mining on Radiology Reports}

Text Mining tools can be used for automatic detection of important findings in Radiology Reports. For example, \citep{Dreyer2005} used an algorithm based on information theory to classify reports as having/not having important clinical findings and as having/not having recommendations for subsequent action. \citep{Cotik2015} did something similar for Spanish reports, using a translation of RadLex terms. These tools can also be used to detect the presence of more specific findings, as the presence of invasive mold diseases  \citep{Ananda-Rajah2014} or invasive fungal diseases \citep{Martinez2015}, both using a classifier based on a Support Vector Machine. Also possible is to extract general information about reports \citep{Hassanpour2016} and the data obtained can be used as input to other tools.

In literature it's possible to find some examples of Radiology reports/images search applications, that use NLP tools. The goals of these search tools include search for educational, research and clinical decision support purposes. One example of such a system is Render \citep{Dang2009}, which even applies one of the information extraction system mentioned above \citep{Dreyer2005} to improve relevance of information retrieved.

Other applications include studying the appropriateness of existing Radiology reports templates, as done by \citep{Hong2013}

\section{Translation}

\subsection{Machine Translation}

Machine Translation (MT) is the use of computers to automatically translate natural language text. Currently, Statistical Machine Translation (SMT) is the most popular approach to MT. I will briefly review word-based and phrase-based which are both covered by the SMT approach. More recently, there as being a growth in the use of neural-networks to translation, so called neural-machine-translation \citep{Bentivogli2016}. 

This is mostly based on \citep{Koehn2010}.

\subsubsection{Word-Based Models}

These kind of models are not the state of the art anymore, but many of the principles and techniques of this approach are still in use today. The idea here is to translate the sentences word by word. Here is an example, translating English to Portuguese:

\begin{center}
\textbf{English}    - The bone    is      broken

\textbf{Portuguese} -  O  osso   está     partido
\end{center}
	
This is easy for a human to translate, but how would a computer know that \textit{partido} is the translation of \textit{broken} when \textit{broken} has other potential translations? For example, the word \textit{broken} could be interpreted as being financially ruined, as in “I’ve spent all the money in the casino, I'm completely \textit{broken}". In that case, \textit{broken} would be translated to \textit{falido}. Of course, this doesn't make sense because bones don't have a financial life but the computer doesn't know that. 

One way to teach the computer which translation to use would be to pick a large collection of English texts paired with the corresponding Portuguese translation and check how many times \textit{broken} is translated to \textit{partido} and how many times it is translated to \textit{falido}. Lets assume that in our collection of texts the word \textit{broken} is translated to \textit{partido} 80\% of the times and to \textit{falido} 20\% of the time. With this we could create a lexical translation probability table for the word \textit{broken}. We could have a table like this one for every word in the source texts.

\begin{table}[h]
\centering
\begin{tabular}{ll}
\multicolumn{2}{c}{\textbf{broken}} \\ \hline
t                     & p(t|s)      \\ \hline
\textit{partido}      & 0.8         \\
\textit{falido}       & 0.2         \\ \hline
\end{tabular}
\caption{Lexical translation probability table for the word broken}
\label{table:translation-prob-table}
\end{table}

Here \textit{t} stands for target, \textit{s} stands for source and \textit{p(t|s)} is the probability that the target word is the translation of the source word. So, when the computer is translating the sentence above and arrives to the word \textit{broken}, it checks the table and chooses \textit{partido} as the translation because it has the higher probability of being the real translation. This type of estimation is called maximum likelihood estimation. What we are doing here is estimating lexical translation probability distributions.

The example above was easy because the sentences were aligned word by word. This is not always the case. For example, the English expression \textit{red swelling} should be translated to \textit{inchaço vermelho}, not \textit{vermelho inchaço}\footnote{red -> vermelho, swelling -> inchaço}. Meaning, sometimes we must do some word reordering so that the translation is correct. This is accommodated by using an alignment model. But how can we generate an alignment model from a pair of collection of texts if we don't know which word is aligned with which word? This is done by using the expectation maximization algorithm, which, in this case, iteratively applies the alignment model to the texts (expectation step) and learns the alignment model from the texts (maximization step) until convergence of the parameters in the algorithm. 

With the lexical translation probability distributions and an alignment model we have a translation model. But this is not enough. A translation could be syntactically and semantically right but still not sounding right. For example, two possible translations of \textit{chá forte} are \textit{strong tea} and \textit{powerful tea}. However, the second option doesn't sound right, it is not fluent. This problem is solved by using a language model. With an English language model, for example, we could calculate the probability that a certain sentence is correct English, considering all the data that was used to train the model. A language model would probably give a low probability to the phrase \textit{powerful tea} because normally the word powerful is not used with the word \textit{tea}.

We combine the language model and the translation model this way:

\begin{equation}
\argmax{t} \Pr(t|s) = \argmax{t} \Pr(s|t) \Pr(t)
\end{equation}


We want to find the target word (\textit{t}) with the higher probability of being the translation of the source word (\textit{s}). $\Pr(t|s)$ represents the translation model and the the $\Pr(t)$ represents the language model. This way of combining the translation and the language models is called noisy-channel model.  

\subsubsection{Phrase-Based Models}

In this approach, instead of translating a sentence word by word we translate small words sequences at a time, sequences that we call phrases. These models have a better performance than the word-based models and this is not too surprising. Sometimes words are not the best unit of translation: there are cases when two words in the source sentence are translated into one word in the target sentence, for example. Another advantage is that translating phrases instead of words can help to solve ambiguities, as in the problem of deciding how to translate the text \textit{chá forte} (see last section). We would check a parallel collection of texts and realize that most of the times \textit{chá forte} is translated to \textit{strong tea}. So, the idea here is to divide the sentence in phrases, translate the phrases and do some reordering if necessary.  

\subsection{Machine Translation Services}

\subsubsection{Yandex}

Yandex\footnote{https://yandex.com/} is a Russian search-engine company. At the time the work for this thesis was being done, Yandex.Translate (the name given to Yandex's MT system) uses a statistical approach. From their website\footnote{https://tech.yandex.com/translate/doc/intro/concepts/how-works-machine-translation-docpage/}, the system is composed by three components, a translation model, a language model and a decoder which is the part that actually does the translation. 

I couldn't find any research paper evaluating the translation's quality of Yandex.Translate in the language pair Portuguese-English.

\subsubsection{Google}

Google\footnote{https://www.google.com} is a company from the United States that offers a lot of technological services, including machine translation. For the language pair Portuguese-English, their translation services now use Neural Machine Translation\footnote{https://blog.google/products/translate/found-translation-more-accurate-fluent-sentences-google-translate/}.


\subsection{Post-editing}

Post-editing (PE) is the task of editing, modifying and/or correcting a text that was pre-translated by use of MT, in order to improve the translation. \citep{Somers2003} refers to the lower cost of MT+PE compared with HT to explain the growth of PE: companies want to become global but can't afford the cost of HT to translate from native language to the many languages they want to operate on.

\citep{Koponen2016a} tried to understand if MT+PE is really worth, compared with just HT, concluding that yes, most of the times it is worth it, but it depends on the quality of the MT, which in turn depends on, for example, the quality of the MT system and on the language pair. 

Most of the research regarding PE refers to work done by professional translators. One approach that has been gaining traction is the use of the crowd to do the PE \citep{Tatsumi2012a}. The advantages of this strategy include lower per-word cost and sometimes an higher speed. One big disadvantage is less assurance of quality. 

\subsection{Translation of Medical Text}

As I've written previously, as far as I know, there is no research studying the effect of translation on NLP techniques. But there is a lot of work on translation of medical text. In this section I briefly review this work.

\subsubsection{Machine Translation of Doctor-Patient Communication}

Most of the work done on medical translation focuses on translation of doctor-patient communication. This has the objective of breaking language barriers that sometimes exist between a doctor and a patient who don't speak the same language, with health-related consequences to the patient \citep{Schyve2007}. This could be done with trained medical interpreters but that option is costly compared with using MT and raises problems regarding patient confidentially. 

Several MT speech-to-speech translation systems for doctor-patient communication exist, but for most of them, evaluations are not found in the literature. One exception is \citep{Bouillon2005} which studies MedSLT, a multilingual spoken language translation system tailored for headache, chest pain and abdominal pain domains. However, \citep{Bouillon2005} only studies the appropriateness of the design choices within the system, not comparing its performance with anything else. Others example of systems of thys type are Jibbigo\footnote{http://jibbigo-translator-2-0.soft112.com/}, Universal Doctor\footnote{http://www.universaldoctor.com/} and Transonics \citep{Nagata2005}.

\citep{Kaliyadan2010} did a small study on the use of Google Translate to translate between English and French during doctor-patient interaction in India medical offices, with promising results regarding patient satisfaction. Work was also done on non-European languages, which have less resources \citep{Musleh2016, Kathol2005}.

Some researchers \citep{G2013, Conference2012} suggest that MT should be used very cautiously in this situations, because of imperfect performance in a domain where accuracy is really important. One way to improve the systems could involve the use of existing public medical terms database \citep{Eck2004}.

\subsubsection{Machine Translation of Public-Health Information}

In the USA, most of the public health information is written in English, although a substantial percentage of the population have limited English proficiency. One of the barriers for more widespread translation is the cost of translation services and a way of streamlining the process would be using MT+PE. \citep{Kirchhoff2011, Turner2015} studied the feasibility of this system for translation from English to Spanish, with some promising results, and to Chinese, which was more problematic. 

\subsubsection{Machine Translation for Information Retrieval}

The ACL 2014 Ninth Workshop on Statistical Machine Translation had a Medical Translation Task \citep{Bojar2014}, which consisted in two subtasks: translation of sentences from summaries of medical articles and translation of queries entered by users of medical information search engines. This task was supported by the Khresmoi \footnote{http://khresmoi.eu/} project which develops a multilingual search and access system for biomedical information and documents, allowing the user to make search queries and read summaries of the results in their own language. The task had 8 participants, the winner being the UEDIN team \citep{Durrani2014} which used the Moses phrase-based system. 

\subsubsection{Machine Translation of Other Types of Medical Text}

Studies of the translation of other types of documents are also present in the literature. For example, \citep{Wok2015} compares neural based with statistical machine translation of descriptions of medical products in the language pair Polish-English, obtaining mixed results.

More related to the work done on this thesis, in the first recorded study of translation of medical records \citep{Zeng-Treitler2010} tested if a general-purpose machine translation tool like the Babel Fish is adequate to translate sentences of discharge summaries, surgical notes, admission notes, and radiology reports from English to Spanish, Chinese, Russian and Korean. They found that most of the times the translation is incomprehensible and inaccurate.

More recently, there was a Biomedical Translation Task during the ACL 2016 First Conference on Machine Translation (WMT16) in which the participants were asked to submit systems to translate titles and abstracts from scientific publications \citep{Bojar2016}. The evaluators note that the quality of the machine translation is still poor in comparison to the reference translations. The only submission to the English-Portuguese  and Portuguese-English translation tasks \citep{Aires2016} were the ones with the worse results relative to the baseline system. 

\section{External Tools and Terminologies}

Some of the work done during the thesis used and was inspired by some external tools and terminologies that I now briefly review.

\subsection{RadLex}

RadLex\footnote{http://www.rsna.org/RadLex.aspx} is a domain-ontology which focuses on radiology-related terms. It was developed to standardize annotation, indexation, and retrieval of radiology information resources in the digital world \citep{Langlotz2006} and it helped to fill a gap in radiology terminology \citep{Langlotz2002, Woods2013}. The RadLex terms were originally gathered from existing ontologies at the time, including the American College of Radiology (ACR) Index, SNOMED-CT, and the Foundational Model Anatomy and it is a highly dynamic ontology: its number of terms grew from around 8000 to around 75000 in just ten years. 

\subsection{Open Biomedical Annotator}

The Open Biomedical Annotator (OBA)\footnote{http://bioportal.bioontology.org/annotator} is an open-source tool made available by the North-American National Center for Biomedical Ontology \citep{Jonquet2009}, which can be used to annotate text with concepts from ontologies. For example, if you go to the website, input a radiology report and choose the ontology RadLex, the tool will return all the mentions in the text of terms belonging to the RadLex terminology. It can easily be used as a web-service and it is relatively fast. 

\subsection{NOBLE Coder}

NOBLE Coder\footnote{http://noble-tools.dbmi.pitt.edu/} \citep{Tseytlin2016} is a software for NER using a dictionary-based approach. The dictionary is set by the user (it  has to be in UMLS (RRF)\footnote{https://www.ncbi.nlm.nih.gov/books/NBK9685/}, OWL\footnote{https://www.w3.org/OWL/} or OBO\footnote{http://www.geneontology.org/faq/what-obo-file-format} formats or be present in BioPortal\footnote{http://bioportal.bioontology.org/ontologies}) and NOBLE finds, in an arbitrary text, mentions of terms found in the dictionary.

Unlike the system used by the Open Biomedical Annotator (OBA)\footnote{http://bioportal.bioontology.org/annotator}\citep{Jonquet2009}, NOBLE can find mentions of lexical variations of the terms present in the dictionary because it applies word stemming. For example, \textit{lobe} is a term present in the RadLex terminology, but it's plural, \textit{lobes}, isn't. However, NOBLE considers that \textit{lobes} is a mention of the term \textit{lobe}, which is right. But this can sometimes go wrong; for example, NOBLE considers that \textit{headings} is a mention of the RadLex term \textit{head}, which is wrong. So although this strategy can improve recall in does so at the cost of precision. 

The NOBLE tool is also flexible in what is considered a mention of a dictionary word giving the user the power to adapt the tool for her purposes. This can be done by choosing to use or not a certain \textit{matching options}. These include:

\begin{itemize}

\item \textbf{Subsumption} - Only matches the longest mention. For example, \textit{toe}, \textit{toe skin} and \textit{skin} are all RadLex terms. If the "Subsumption" option is set, in the text \textit{toe skin}, only the term \textit{toe skin} will be recognized. Otherwise, the terms \textit{toe} and \textit{skin} are also recognized.

\item \textbf{Overlap} - ...

\item \textbf{Contiguity} - Terms must be contiguous to be matched. For example, if set, in the text \textit{multiple ducts lesions} both \textit{multiple ducts} and \textit{multiple lesions} are considered matches, although \textit{multiple} and \textit{lesions} are not adjacent to each other. It's possible to set how many irrelevant words can be between words belonging to a term (in \ref{table:matching-strategies}, this is called \textit{gap}. 

\item \textbf{Order} - Terms must be in the same order as in the dictionary to be considered mentions. If set, \textit{lesions multiple} is considered a mention of the Radlex term \textit{multiple lesions}.

\item \textbf{Partial} - Partial match with terms in dictionary are considered a dictionary term mention. If set, \textit{multiple} is considered a mention of \textit{multiple lesions}. 

\end{itemize}

The user can also choose to, for example:

\begin{itemize}
\item Skip single letter words
\item Skip stop words
\item Use heuristics to filter out potential false positives
\item When a term can be considered a mention of more than one concept in the dictionary,  select only the highest scoring one
\end{itemize}

Different combinations of these options are useful for different purposes. NOBLE already offers some built-in matching strategies, listed in \ref{table:matching-strategies}. 

\begin{table}[h]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llllll@{}}
\toprule
\multicolumn{1}{c}{} & \multicolumn{5}{l}{\textbf{Combination of matching options}} \\ \midrule
\textbf{Task} & \textbf{Subsumption} & \textbf{Overlap} & \textbf{Contiguity} & \textbf{Order} & \textbf{Partial} \\
\textit{Best match} & Yes & Yes & Yes (gap=1) & No & No \\
\textit{All match} & No & Yes & No & No & No \\
\textit{Precise match} & Yes & Yes & Yes (gap=0) & Yes & No \\
\textit{Sloppy/Partial match} & No & Yes & No & No & Yes \\ \bottomrule
\end{tabular}%
}
\caption{NOBLE matching strategies present in the GUI interface. Adapted from \citep{Tseytlin2016}. This correspond to the options used in the GUI tool.}
\label{table:matching-strategies}
\end{table}

The authors of the tool provide suggestion for what kind of task each strategy is more appropriate. For example, they suggest that the \textit{best match} strategy is best for concept coding and information extraction and that the \textit{all match} strategy is more suitable for information retrieval and text mining.

\citep{Tseytlin2016} compares the NOBLE tool with other dictionary-based NER tools, finding that its performance in recognizing terms from dictionaries its comparable with other similar software like Concept Mapper \citep{Stewart} or cTAKES \footnote{https://cwiki.apache.org/confluence/display/CTAKES/cTAKES+3.0+-+Dictionary+Lookup} \footnote{https://cwiki.apache.org/confluence/display/CTAKES/cTAKES+3.2+-+Fast+Dictionary+Lookup}, although it probably depends a lot on the corpus used.

One big advantage of NOBLE is its ease of use compared with other similar systems. Little or no programming skills are needed to use the software since it includes a GUI (Graphical User Interface) which allows an user to upload dictionaries in a number of formats and easily annotate texts. 

\section{Evaluation Metrics}

For a certain task (for example, annotation of terms that represent diseases from a corpus) it is useful to have standard evaluation metrics so that we can compare many systems and know which one is the best. In information retrieval and information extraction systems precision (P), recall (R) and F-score (F) are the measures that are mostly used. For example they were the measures used in a competition which involved a task similar to the example I gave above \citep{Elhadad2015}.

To use this measures we need to have a reference, a gold-standard, which we assume represents the perfect performance in a certain task, the ground truth. In the example of extraction of disorder mentions, it could be an annotation done by an human expert. To calculate this measures we also need the number of true positives, true negatives, false positives and false negatives. I will illustrate each one of these with the example of the annotation of diseases mentions.

\begin{itemize}
\item True positive (TP) – The system being tested annotated a term also annotated in the reference. 
\item True negative (TN) – The system didn't annotate a term that is also not annotated in the reference.
\item False positive (FP) – The system annotated a term that is not annotated in the reference. 
\item False negative (FN) – The system didn't annotate a term that is annotated in the reference. 
\end{itemize}

Precision corresponds to the fraction of the terms annotated by the system that are also annotated in the reference. 

\begin{equation}
P = \frac{TP}{TP+FP}
\end{equation}

If of the 10 terms annotated by the system, only 6 are annotated by the reference, then the system has a precision of 0.6. If every term extracted by the system is also extracted by the reference, then the system has a precision of 1, the best score possible. But the system can have a score of 1 if only annotates one right term, even though there are a lot of other terms annotated in the reference. This system, although having a score of 1, would not be very useful. Recall is a measure that helps to solve this issue.

Recall calculates what fraction of all terms annotated in the reference are annotated by the system.

\begin{equation}
R = \frac{TP}{TP+FN}
\end{equation}

If the system annotates 8 terms of the 10 that are annotated in the reference, then it has a recall of 0.8. If it annotates all of them, it has a recall of 1, the perfect score. But, as is the case with precision, this measure also has problems. If the system annotates all the terms in a corpus, it will have a perfect score in the recall measure, because it is sure to have annotated all the terms annotated in the reference, although it also annotated a lot of wrong terms. 

As you can see, both measures have problems when used in isolation. One way to combine them is by using the F-score measure, that corresponds to the harmonic mean of precision and recall. 

\begin{equation}
F−score = 2 * \frac{P*R}{P*R}
\end{equation}


